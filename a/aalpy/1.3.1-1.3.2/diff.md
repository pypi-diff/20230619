# Comparing `tmp/aalpy-1.3.1-py3-none-any.whl.zip` & `tmp/aalpy-1.3.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,76 +1,76 @@
-Zip file size: 112932 bytes, number of entries: 74
--rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-14 17:40 aalpy/__init__.py
--rw-rw-rw-  2.0 fat      460 b- defN 21-Sep-14 17:40 aalpy/paths.py
--rw-rw-rw-  2.0 fat     3561 b- defN 22-Jan-25 20:10 aalpy/SULs/AutomataSUL.py
--rw-rw-rw-  2.0 fat     1745 b- defN 22-Apr-26 12:31 aalpy/SULs/PyMethodSUL.py
--rw-rw-rw-  2.0 fat      943 b- defN 21-Oct-18 11:17 aalpy/SULs/RegexSUL.py
--rw-rw-rw-  2.0 fat     1546 b- defN 21-Sep-14 17:40 aalpy/SULs/TomitaSUL.py
--rw-rw-rw-  2.0 fat      218 b- defN 21-Sep-14 17:40 aalpy/SULs/__init__.py
--rw-rw-rw-  2.0 fat     2024 b- defN 23-Mar-28 16:04 aalpy/automata/Dfa.py
--rw-rw-rw-  2.0 fat     1698 b- defN 21-Sep-14 17:40 aalpy/automata/MarkovChain.py
--rw-rw-rw-  2.0 fat     1766 b- defN 22-May-27 17:45 aalpy/automata/Mdp.py
--rw-rw-rw-  2.0 fat     1392 b- defN 23-Mar-28 16:04 aalpy/automata/MealyMachine.py
--rw-rw-rw-  2.0 fat     2049 b- defN 23-Mar-28 16:04 aalpy/automata/MooreMachine.py
--rw-rw-rw-  2.0 fat     2527 b- defN 22-Feb-15 18:15 aalpy/automata/Onfsm.py
--rw-rw-rw-  2.0 fat     3674 b- defN 22-May-10 15:47 aalpy/automata/StochasticMealyMachine.py
--rw-rw-rw-  2.0 fat      335 b- defN 21-Sep-14 17:40 aalpy/automata/__init__.py
--rw-rw-rw-  2.0 fat    16276 b- defN 23-Mar-28 16:09 aalpy/base/Automaton.py
--rw-rw-rw-  2.0 fat     5736 b- defN 22-Nov-26 15:50 aalpy/base/CacheTree.py
--rw-rw-rw-  2.0 fat     1229 b- defN 21-Sep-14 17:40 aalpy/base/Oracle.py
--rw-rw-rw-  2.0 fat     4120 b- defN 22-Nov-26 15:50 aalpy/base/SUL.py
--rw-rw-rw-  2.0 fat      124 b- defN 22-Jan-09 13:33 aalpy/base/__init__.py
--rw-rw-rw-  2.0 fat      521 b- defN 23-Mar-13 18:38 aalpy/learning_algs/__init__.py
--rw-rw-rw-  2.0 fat    16383 b- defN 23-Mar-13 18:39 aalpy/learning_algs/deterministic/ClassificationTree.py
--rw-rw-rw-  2.0 fat     3261 b- defN 22-Nov-26 15:50 aalpy/learning_algs/deterministic/CounterExampleProcessing.py
--rw-rw-rw-  2.0 fat     5797 b- defN 23-Mar-13 18:34 aalpy/learning_algs/deterministic/KV.py
--rw-rw-rw-  2.0 fat     7555 b- defN 22-Nov-26 15:50 aalpy/learning_algs/deterministic/LStar.py
--rw-rw-rw-  2.0 fat     8316 b- defN 22-Nov-26 15:50 aalpy/learning_algs/deterministic/ObservationTable.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-14 17:40 aalpy/learning_algs/deterministic/__init__.py
--rw-rw-rw-  2.0 fat     5859 b- defN 23-Mar-28 16:04 aalpy/learning_algs/deterministic_passive/GeneralizedStateMerging.py
--rw-rw-rw-  2.0 fat     7843 b- defN 23-Mar-28 16:04 aalpy/learning_algs/deterministic_passive/RPNI.py
--rw-rw-rw-  2.0 fat        0 b- defN 22-May-11 19:51 aalpy/learning_algs/deterministic_passive/__init__.py
--rw-rw-rw-  2.0 fat     3789 b- defN 22-Nov-26 15:50 aalpy/learning_algs/deterministic_passive/active_RPNI.py
--rw-rw-rw-  2.0 fat     9298 b- defN 23-Mar-28 16:04 aalpy/learning_algs/deterministic_passive/rpni_helper_functions.py
--rw-rw-rw-  2.0 fat     6426 b- defN 22-Oct-28 12:49 aalpy/learning_algs/non_deterministic/AbstractedOnfsmLstar.py
--rw-rw-rw-  2.0 fat    15947 b- defN 22-Oct-28 12:49 aalpy/learning_algs/non_deterministic/AbstractedOnfsmObservationTable.py
--rw-rw-rw-  2.0 fat      659 b- defN 22-Oct-28 12:49 aalpy/learning_algs/non_deterministic/NonDeterministicSULWrapper.py
--rw-rw-rw-  2.0 fat     5020 b- defN 22-Oct-28 12:49 aalpy/learning_algs/non_deterministic/OnfsmLstar.py
--rw-rw-rw-  2.0 fat     7032 b- defN 22-Oct-28 12:49 aalpy/learning_algs/non_deterministic/OnfsmObservationTable.py
--rw-rw-rw-  2.0 fat     6011 b- defN 22-Oct-28 12:49 aalpy/learning_algs/non_deterministic/TraceTree.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-14 17:40 aalpy/learning_algs/non_deterministic/__init__.py
--rw-rw-rw-  2.0 fat     7764 b- defN 22-Nov-14 20:28 aalpy/learning_algs/stochastic/DifferenceChecker.py
--rw-rw-rw-  2.0 fat    25536 b- defN 22-Nov-14 20:28 aalpy/learning_algs/stochastic/SamplingBasedObservationTable.py
--rw-rw-rw-  2.0 fat     3426 b- defN 22-Sep-14 14:23 aalpy/learning_algs/stochastic/StochasticCexProcessing.py
--rw-rw-rw-  2.0 fat    10167 b- defN 22-Sep-14 14:23 aalpy/learning_algs/stochastic/StochasticLStar.py
--rw-rw-rw-  2.0 fat    13239 b- defN 22-Nov-14 20:28 aalpy/learning_algs/stochastic/StochasticTeacher.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-14 17:40 aalpy/learning_algs/stochastic/__init__.py
--rw-rw-rw-  2.0 fat     2893 b- defN 22-Apr-26 12:31 aalpy/learning_algs/stochastic_passive/ActiveAleriga.py
--rw-rw-rw-  2.0 fat    11065 b- defN 22-Nov-21 16:25 aalpy/learning_algs/stochastic_passive/Alergia.py
--rw-rw-rw-  2.0 fat     1178 b- defN 22-May-12 14:00 aalpy/learning_algs/stochastic_passive/CompatibilityChecker.py
--rw-rw-rw-  2.0 fat     3990 b- defN 22-Nov-21 16:25 aalpy/learning_algs/stochastic_passive/FPTA.py
--rw-rw-rw-  2.0 fat        0 b- defN 21-Sep-14 17:40 aalpy/learning_algs/stochastic_passive/__init__.py
--rw-rw-rw-  2.0 fat     1433 b- defN 22-Feb-15 18:15 aalpy/oracles/BreadthFirstExplorationEqOracle.py
--rw-rw-rw-  2.0 fat     3135 b- defN 21-Sep-14 17:40 aalpy/oracles/CacheBasedEqOracle.py
--rw-rw-rw-  2.0 fat     1670 b- defN 22-Jan-09 13:33 aalpy/oracles/PacOracle.py
--rw-rw-rw-  2.0 fat     2986 b- defN 22-May-13 10:37 aalpy/oracles/RandomWalkEqOracle.py
--rw-rw-rw-  2.0 fat     3438 b- defN 22-Nov-26 15:50 aalpy/oracles/RandomWordEqOracle.py
--rw-rw-rw-  2.0 fat     2816 b- defN 22-Nov-26 15:50 aalpy/oracles/StatePrefixEqOracle.py
--rw-rw-rw-  2.0 fat     2023 b- defN 22-Nov-26 15:50 aalpy/oracles/TransitionFocusOracle.py
--rw-rw-rw-  2.0 fat     2636 b- defN 22-Jan-31 13:33 aalpy/oracles/UserInputEqOracle.py
--rw-rw-rw-  2.0 fat     4673 b- defN 22-Nov-26 20:42 aalpy/oracles/WMethodEqOracle.py
--rw-rw-rw-  2.0 fat      641 b- defN 22-Jan-09 13:33 aalpy/oracles/__init__.py
--rw-rw-rw-  2.0 fat     2676 b- defN 22-Nov-26 15:50 aalpy/oracles/kWayStateCoverageEqOracle.py
--rw-rw-rw-  2.0 fat     6429 b- defN 22-Nov-26 15:50 aalpy/oracles/kWayTransitionCoverageEqOracle.py
--rw-rw-rw-  2.0 fat    21072 b- defN 22-Nov-14 20:28 aalpy/utils/AutomatonGenerators.py
--rw-rw-rw-  2.0 fat    11845 b- defN 22-Oct-28 12:52 aalpy/utils/BenchmarkSULs.py
--rw-rw-rw-  2.0 fat     2226 b- defN 21-Sep-14 17:40 aalpy/utils/DataHandler.py
--rw-rw-rw-  2.0 fat    13312 b- defN 23-Mar-28 16:04 aalpy/utils/FileHandler.py
--rw-rw-rw-  2.0 fat    10371 b- defN 22-Nov-07 19:26 aalpy/utils/HelperFunctions.py
--rw-rw-rw-  2.0 fat    15343 b- defN 23-Mar-13 18:37 aalpy/utils/ModelChecking.py
--rw-rw-rw-  2.0 fat      888 b- defN 22-Sep-14 14:23 aalpy/utils/__init__.py
--rw-rw-rw-  2.0 fat     1202 b- defN 23-Mar-28 16:10 aalpy-1.3.1.dist-info/LICENCE.txt
--rw-rw-rw-  2.0 fat    10112 b- defN 23-Mar-28 16:10 aalpy-1.3.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Mar-28 16:10 aalpy-1.3.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Mar-28 16:10 aalpy-1.3.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     7004 b- defN 23-Mar-28 16:11 aalpy-1.3.1.dist-info/RECORD
-74 files, 368427 bytes uncompressed, 101568 bytes compressed:  72.4%
+Zip file size: 113616 bytes, number of entries: 74
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Mar-19 08:28 aalpy/__init__.py
+-rw-rw-rw-  2.0 fat      448 b- defN 21-Oct-28 09:31 aalpy/paths.py
+-rw-rw-rw-  2.0 fat     3395 b- defN 22-Dec-05 09:22 aalpy/SULs/AutomataSUL.py
+-rw-rw-rw-  2.0 fat     1677 b- defN 23-Jun-12 10:49 aalpy/SULs/PyMethodSUL.py
+-rw-rw-rw-  2.0 fat      906 b- defN 21-Nov-02 09:32 aalpy/SULs/RegexSUL.py
+-rw-rw-rw-  2.0 fat     1478 b- defN 21-Nov-02 09:32 aalpy/SULs/TomitaSUL.py
+-rw-rw-rw-  2.0 fat      215 b- defN 21-Nov-02 09:32 aalpy/SULs/__init__.py
+-rw-rw-rw-  2.0 fat     2097 b- defN 23-Jun-02 06:29 aalpy/automata/Dfa.py
+-rw-rw-rw-  2.0 fat     1638 b- defN 23-Jun-02 06:29 aalpy/automata/MarkovChain.py
+-rw-rw-rw-  2.0 fat     2361 b- defN 23-Jun-02 06:29 aalpy/automata/Mdp.py
+-rw-rw-rw-  2.0 fat     1483 b- defN 23-Jun-02 06:29 aalpy/automata/MealyMachine.py
+-rw-rw-rw-  2.0 fat     2128 b- defN 23-Jun-02 06:29 aalpy/automata/MooreMachine.py
+-rw-rw-rw-  2.0 fat     2428 b- defN 23-Jun-02 06:29 aalpy/automata/Onfsm.py
+-rw-rw-rw-  2.0 fat     4212 b- defN 23-Jun-02 06:29 aalpy/automata/StochasticMealyMachine.py
+-rw-rw-rw-  2.0 fat      328 b- defN 23-Jun-05 10:36 aalpy/automata/__init__.py
+-rw-rw-rw-  2.0 fat    15912 b- defN 23-Jun-02 06:29 aalpy/base/Automaton.py
+-rw-rw-rw-  2.0 fat     5567 b- defN 22-Nov-23 08:52 aalpy/base/CacheTree.py
+-rw-rw-rw-  2.0 fat     1178 b- defN 22-Nov-03 09:46 aalpy/base/Oracle.py
+-rw-rw-rw-  2.0 fat     3978 b- defN 23-Jun-13 14:54 aalpy/base/SUL.py
+-rw-rw-rw-  2.0 fat      121 b- defN 21-Dec-13 13:25 aalpy/base/__init__.py
+-rw-rw-rw-  2.0 fat      576 b- defN 23-May-17 09:06 aalpy/learning_algs/__init__.py
+-rw-rw-rw-  2.0 fat    15997 b- defN 23-Mar-09 10:11 aalpy/learning_algs/deterministic/ClassificationTree.py
+-rw-rw-rw-  2.0 fat     3160 b- defN 22-Nov-24 07:33 aalpy/learning_algs/deterministic/CounterExampleProcessing.py
+-rw-rw-rw-  2.0 fat     5639 b- defN 23-Feb-23 11:30 aalpy/learning_algs/deterministic/KV.py
+-rw-rw-rw-  2.0 fat     7374 b- defN 22-Nov-24 07:33 aalpy/learning_algs/deterministic/LStar.py
+-rw-rw-rw-  2.0 fat     8097 b- defN 22-Nov-24 07:33 aalpy/learning_algs/deterministic/ObservationTable.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Mar-19 08:28 aalpy/learning_algs/deterministic/__init__.py
+-rw-rw-rw-  2.0 fat     3954 b- defN 23-Jun-13 15:32 aalpy/learning_algs/deterministic_passive/GeneralizedStateMerging.py
+-rw-rw-rw-  2.0 fat     7619 b- defN 23-Jun-13 15:32 aalpy/learning_algs/deterministic_passive/RPNI.py
+-rw-rw-rw-  2.0 fat        0 b- defN 22-Aug-17 08:52 aalpy/learning_algs/deterministic_passive/__init__.py
+-rw-rw-rw-  2.0 fat     1818 b- defN 23-May-17 09:08 aalpy/learning_algs/deterministic_passive/active_RPNI.py
+-rw-rw-rw-  2.0 fat     9033 b- defN 23-Jun-13 15:32 aalpy/learning_algs/deterministic_passive/rpni_helper_functions.py
+-rw-rw-rw-  2.0 fat     6280 b- defN 22-Oct-28 09:58 aalpy/learning_algs/non_deterministic/AbstractedOnfsmLstar.py
+-rw-rw-rw-  2.0 fat    15505 b- defN 22-Oct-28 09:58 aalpy/learning_algs/non_deterministic/AbstractedOnfsmObservationTable.py
+-rw-rw-rw-  2.0 fat      634 b- defN 22-Oct-28 09:58 aalpy/learning_algs/non_deterministic/NonDeterministicSULWrapper.py
+-rw-rw-rw-  2.0 fat     4870 b- defN 22-Nov-03 12:43 aalpy/learning_algs/non_deterministic/OnfsmLstar.py
+-rw-rw-rw-  2.0 fat     6828 b- defN 22-Nov-03 12:39 aalpy/learning_algs/non_deterministic/OnfsmObservationTable.py
+-rw-rw-rw-  2.0 fat     5808 b- defN 22-Oct-28 11:10 aalpy/learning_algs/non_deterministic/TraceTree.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Mar-19 08:28 aalpy/learning_algs/non_deterministic/__init__.py
+-rw-rw-rw-  2.0 fat     7587 b- defN 22-Nov-07 14:00 aalpy/learning_algs/stochastic/DifferenceChecker.py
+-rw-rw-rw-  2.0 fat    24894 b- defN 22-Nov-07 14:03 aalpy/learning_algs/stochastic/SamplingBasedObservationTable.py
+-rw-rw-rw-  2.0 fat     3296 b- defN 22-Aug-17 08:52 aalpy/learning_algs/stochastic/StochasticCexProcessing.py
+-rw-rw-rw-  2.0 fat     9953 b- defN 23-Jun-12 11:06 aalpy/learning_algs/stochastic/StochasticLStar.py
+-rw-rw-rw-  2.0 fat    12846 b- defN 22-Nov-07 14:00 aalpy/learning_algs/stochastic/StochasticTeacher.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Apr-05 18:15 aalpy/learning_algs/stochastic/__init__.py
+-rw-rw-rw-  2.0 fat     2805 b- defN 23-Feb-23 09:56 aalpy/learning_algs/stochastic_passive/ActiveAleriga.py
+-rw-rw-rw-  2.0 fat    10794 b- defN 23-Jun-13 15:32 aalpy/learning_algs/stochastic_passive/Alergia.py
+-rw-rw-rw-  2.0 fat     1847 b- defN 23-Jun-05 16:53 aalpy/learning_algs/stochastic_passive/CompatibilityChecker.py
+-rw-rw-rw-  2.0 fat     3984 b- defN 23-Jun-13 15:32 aalpy/learning_algs/stochastic_passive/FPTA.py
+-rw-rw-rw-  2.0 fat        0 b- defN 21-Oct-28 09:31 aalpy/learning_algs/stochastic_passive/__init__.py
+-rw-rw-rw-  2.0 fat     1382 b- defN 22-Nov-15 10:54 aalpy/oracles/BreadthFirstExplorationEqOracle.py
+-rw-rw-rw-  2.0 fat     3038 b- defN 22-Nov-15 10:54 aalpy/oracles/CacheBasedEqOracle.py
+-rw-rw-rw-  2.0 fat     1670 b- defN 21-Dec-23 10:55 aalpy/oracles/PacOracle.py
+-rw-rw-rw-  2.0 fat     2898 b- defN 22-Aug-17 08:52 aalpy/oracles/RandomWalkEqOracle.py
+-rw-rw-rw-  2.0 fat     3343 b- defN 22-Nov-24 07:32 aalpy/oracles/RandomWordEqOracle.py
+-rw-rw-rw-  2.0 fat     2738 b- defN 22-Nov-24 07:32 aalpy/oracles/StatePrefixEqOracle.py
+-rw-rw-rw-  2.0 fat     1970 b- defN 22-Nov-24 07:32 aalpy/oracles/TransitionFocusOracle.py
+-rw-rw-rw-  2.0 fat     2567 b- defN 22-Jan-31 15:32 aalpy/oracles/UserInputEqOracle.py
+-rw-rw-rw-  2.0 fat     4542 b- defN 22-Nov-27 15:03 aalpy/oracles/WMethodEqOracle.py
+-rw-rw-rw-  2.0 fat      630 b- defN 21-Dec-23 10:54 aalpy/oracles/__init__.py
+-rw-rw-rw-  2.0 fat     2814 b- defN 23-Mar-28 06:19 aalpy/oracles/kWayStateCoverageEqOracle.py
+-rw-rw-rw-  2.0 fat     6264 b- defN 22-Nov-24 07:37 aalpy/oracles/kWayTransitionCoverageEqOracle.py
+-rw-rw-rw-  2.0 fat    21678 b- defN 23-Jun-05 10:36 aalpy/utils/AutomatonGenerators.py
+-rw-rw-rw-  2.0 fat    17637 b- defN 23-Jun-05 10:36 aalpy/utils/BenchmarkSULs.py
+-rw-rw-rw-  2.0 fat     2153 b- defN 21-Nov-02 09:32 aalpy/utils/DataHandler.py
+-rw-rw-rw-  2.0 fat    13013 b- defN 23-May-17 09:05 aalpy/utils/FileHandler.py
+-rw-rw-rw-  2.0 fat    10073 b- defN 22-Nov-23 14:16 aalpy/utils/HelperFunctions.py
+-rw-rw-rw-  2.0 fat    15817 b- defN 23-Jun-19 11:42 aalpy/utils/ModelChecking.py
+-rw-rw-rw-  2.0 fat      939 b- defN 23-Jun-19 11:42 aalpy/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1181 b- defN 23-Jun-19 11:46 aalpy-1.3.2.dist-info/LICENCE.txt
+-rw-rw-rw-  2.0 fat    10110 b- defN 23-Jun-19 11:46 aalpy-1.3.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       97 b- defN 23-Jun-19 11:46 aalpy-1.3.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        6 b- defN 23-Jun-19 11:46 aalpy-1.3.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     7003 b- defN 23-Jun-19 11:46 aalpy-1.3.2.dist-info/RECORD
+74 files, 366311 bytes uncompressed, 102252 bytes compressed:  72.1%
```

## zipnote {}

```diff
@@ -201,23 +201,23 @@
 
 Filename: aalpy/utils/ModelChecking.py
 Comment: 
 
 Filename: aalpy/utils/__init__.py
 Comment: 
 
-Filename: aalpy-1.3.1.dist-info/LICENCE.txt
+Filename: aalpy-1.3.2.dist-info/LICENCE.txt
 Comment: 
 
-Filename: aalpy-1.3.1.dist-info/METADATA
+Filename: aalpy-1.3.2.dist-info/METADATA
 Comment: 
 
-Filename: aalpy-1.3.1.dist-info/WHEEL
+Filename: aalpy-1.3.2.dist-info/WHEEL
 Comment: 
 
-Filename: aalpy-1.3.1.dist-info/top_level.txt
+Filename: aalpy-1.3.2.dist-info/top_level.txt
 Comment: 
 
-Filename: aalpy-1.3.1.dist-info/RECORD
+Filename: aalpy-1.3.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## aalpy/paths.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-"""
-File in which necessary paths for model checking are defined.
-
-path_to_prism is the absolute or relative path to the prism executable. Note that it has to include the executable file,
-not just the folder. Eg. /usr/edi/prism/prism.bat and NOT /usr/edi/prism/
-
-If you learn one of the provided examples path to properties should be relative or
-absolute path to 'Benchmarking\prism_eval_props'.
-"""
-
-path_to_prism = None
-path_to_properties = None
+"""
+File in which necessary paths for model checking are defined.
+
+path_to_prism is the absolute or relative path to the prism executable. Note that it has to include the executable file,
+not just the folder. Eg. /usr/edi/prism/prism.bat and NOT /usr/edi/prism/
+
+If you learn one of the provided examples path to properties should be relative or
+absolute path to 'Benchmarking\prism_eval_props'.
+"""
+
+path_to_prism = None
+path_to_properties = None
```

## aalpy/SULs/AutomataSUL.py

 * *Ordering differences only*

```diff
@@ -1,166 +1,166 @@
-from aalpy.base import SUL
-from aalpy.automata import Dfa, MealyMachine, MooreMachine, Onfsm, Mdp, StochasticMealyMachine, MarkovChain
-
-
-class DfaSUL(SUL):
-    """
-    System under learning for DFAs.
-    """
-
-    def __init__(self, dfa: Dfa):
-        super().__init__()
-        self.dfa = dfa
-
-    def pre(self):
-        """
-        Resets the dfa to the initial state.
-        """
-        self.dfa.reset_to_initial()
-
-    def post(self):
-        pass
-
-    def step(self, letter):
-        """
-        If the letter is empty/None check is preform to see if the empty string is accepted by the DFA.
-
-        Args:
-
-            letter: single input or None representing the empty string
-
-        Returns:
-
-            output of the dfa.step method (whether the next state is accepted or not)
-
-        """
-        return self.dfa.step(letter)
-
-
-class MdpSUL(SUL):
-    def __init__(self, mdp: Mdp):
-        super().__init__()
-        self.mdp = mdp
-
-    def query(self, word: tuple) -> list:
-        initial_output = self.pre()
-        out = [initial_output]
-        for letter in word:
-            out.append(self.step(letter))
-        self.post()
-        return out
-
-    def pre(self):
-        self.mdp.reset_to_initial()
-        return self.mdp.current_state.output
-
-    def post(self):
-        pass
-
-    def step(self, letter):
-        return self.mdp.step(letter)
-
-
-class McSUL(SUL):
-    def __init__(self, mdp: MarkovChain):
-        super().__init__()
-        self.mc = mdp
-
-    def query(self, word: tuple) -> list:
-        initial_output = self.pre()
-        out = [initial_output]
-        for letter in word:
-            out.append(self.step(letter))
-        self.post()
-        return out
-
-    def pre(self):
-        self.mc.reset_to_initial()
-        return self.mc.current_state.output
-
-    def post(self):
-        pass
-
-    def step(self, letter=None):
-        return self.mc.step()
-
-
-class MealySUL(SUL):
-    """
-    System under learning for Mealy machines.
-    """
-
-    def __init__(self, mm: MealyMachine):
-        super().__init__()
-        self.mm = mm
-
-    def pre(self):
-        """ """
-        self.mm.reset_to_initial()
-
-    def post(self):
-        """ """
-        pass
-
-    def step(self, letter):
-        """
-        Args:
-
-            letter: single non-Null input
-
-        Returns:
-
-            output of the mealy.step method (output based on the input and the current state)
-
-        """
-        return self.mm.step(letter)
-
-
-class MooreSUL(SUL):
-    """
-    System under learning for Mealy machines.
-    """
-
-    def __init__(self, moore_machine: MooreMachine):
-        super().__init__()
-        self.mm = moore_machine
-
-    def pre(self):
-        """ """
-        self.mm.reset_to_initial()
-
-    def post(self):
-        """ """
-        pass
-
-    def step(self, letter):
-        return self.mm.step(letter)
-
-
-class OnfsmSUL(SUL):
-    def __init__(self, mdp: Onfsm):
-        super().__init__()
-        self.onfsm = mdp
-
-    def pre(self):
-        self.onfsm.reset_to_initial()
-
-    def post(self):
-        pass
-
-    def step(self, letter):
-        return self.onfsm.step(letter)
-
-
-class StochasticMealySUL(SUL):
-    def __init__(self, smm: StochasticMealyMachine):
-        super().__init__()
-        self.smm = smm
-
-    def pre(self):
-        self.smm.reset_to_initial()
-
-    def post(self):
-        pass
-
-    def step(self, letter):
-        return self.smm.step(letter)
+from aalpy.base import SUL
+from aalpy.automata import Dfa, MealyMachine, MooreMachine, Onfsm, Mdp, StochasticMealyMachine, MarkovChain
+
+
+class DfaSUL(SUL):
+    """
+    System under learning for DFAs.
+    """
+
+    def __init__(self, dfa: Dfa):
+        super().__init__()
+        self.dfa = dfa
+
+    def pre(self):
+        """
+        Resets the dfa to the initial state.
+        """
+        self.dfa.reset_to_initial()
+
+    def post(self):
+        pass
+
+    def step(self, letter):
+        """
+        If the letter is empty/None check is preform to see if the empty string is accepted by the DFA.
+
+        Args:
+
+            letter: single input or None representing the empty string
+
+        Returns:
+
+            output of the dfa.step method (whether the next state is accepted or not)
+
+        """
+        return self.dfa.step(letter)
+
+
+class MdpSUL(SUL):
+    def __init__(self, mdp: Mdp):
+        super().__init__()
+        self.mdp = mdp
+
+    def query(self, word: tuple) -> list:
+        initial_output = self.pre()
+        out = [initial_output]
+        for letter in word:
+            out.append(self.step(letter))
+        self.post()
+        return out
+
+    def pre(self):
+        self.mdp.reset_to_initial()
+        return self.mdp.current_state.output
+
+    def post(self):
+        pass
+
+    def step(self, letter):
+        return self.mdp.step(letter)
+
+
+class McSUL(SUL):
+    def __init__(self, mdp: MarkovChain):
+        super().__init__()
+        self.mc = mdp
+
+    def query(self, word: tuple) -> list:
+        initial_output = self.pre()
+        out = [initial_output]
+        for letter in word:
+            out.append(self.step(letter))
+        self.post()
+        return out
+
+    def pre(self):
+        self.mc.reset_to_initial()
+        return self.mc.current_state.output
+
+    def post(self):
+        pass
+
+    def step(self, letter=None):
+        return self.mc.step()
+
+
+class MealySUL(SUL):
+    """
+    System under learning for Mealy machines.
+    """
+
+    def __init__(self, mm: MealyMachine):
+        super().__init__()
+        self.mm = mm
+
+    def pre(self):
+        """ """
+        self.mm.reset_to_initial()
+
+    def post(self):
+        """ """
+        pass
+
+    def step(self, letter):
+        """
+        Args:
+
+            letter: single non-Null input
+
+        Returns:
+
+            output of the mealy.step method (output based on the input and the current state)
+
+        """
+        return self.mm.step(letter)
+
+
+class MooreSUL(SUL):
+    """
+    System under learning for Mealy machines.
+    """
+
+    def __init__(self, moore_machine: MooreMachine):
+        super().__init__()
+        self.mm = moore_machine
+
+    def pre(self):
+        """ """
+        self.mm.reset_to_initial()
+
+    def post(self):
+        """ """
+        pass
+
+    def step(self, letter):
+        return self.mm.step(letter)
+
+
+class OnfsmSUL(SUL):
+    def __init__(self, mdp: Onfsm):
+        super().__init__()
+        self.onfsm = mdp
+
+    def pre(self):
+        self.onfsm.reset_to_initial()
+
+    def post(self):
+        pass
+
+    def step(self, letter):
+        return self.onfsm.step(letter)
+
+
+class StochasticMealySUL(SUL):
+    def __init__(self, smm: StochasticMealyMachine):
+        super().__init__()
+        self.smm = smm
+
+    def pre(self):
+        self.smm.reset_to_initial()
+
+    def post(self):
+        pass
+
+    def step(self, letter):
+        return self.smm.step(letter)
```

## aalpy/SULs/PyMethodSUL.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-from aalpy.base import SUL
-
-
-class FunctionDecorator:
-    """
-    Decorator of methods found in the SUL class.
-    """
-
-    def __init__(self, function, args=None):
-        """
-        Args:
-
-            function: function of the class to be learned
-
-            args: arguments to be passed to the function. Either a single argument, or a list of arguments if
-                function has more than one parameter.
-        """
-
-        self.function = function
-        self.args = None
-        if args:
-            self.args = [args] if not isinstance(args, (list, tuple)) else args
-
-    def __repr__(self):
-        if self.args:
-            return f'{self.function.__name__}{self.args}'
-        return self.function.__name__
-
-
-class PyClassSUL(SUL):
-    """
-    System under learning for inferring python classes.
-    """
-    def __init__(self, python_class):
-        """
-        Args:
-
-            python_class: class to be learned
-        """
-        super().__init__()
-        self._class = python_class
-        self.sul: object = None
-
-    def pre(self):
-        """
-        Do the reset by initializing the class again or call reset method of the class
-        """
-        self.sul = self._class()
-
-    def post(self):
-        pass
-
-    def step(self, letter):
-        """
-        Executes the function(with arguments) found in letter against the SUL
-
-        Args:
-
-            letter: single input of type FunctionDecorator
-
-        Returns:
-
-            output of the function
-
-        """
-        if letter.args:
-            return getattr(self.sul, letter.function.__name__, letter)(*letter.args)
-        return getattr(self.sul, letter.function.__name__, letter)()
+from aalpy.base import SUL
+
+
+class FunctionDecorator:
+    """
+    Decorator of methods found in the SUL class.
+    """
+
+    def __init__(self, function, args=None):
+        """
+        Args:
+
+            function: function of the class to be learned
+
+            args: arguments to be passed to the function. Either a single argument, or a list of arguments if
+                function has more than one parameter.
+        """
+
+        self.function = function
+        self.args = None
+        if args:
+            self.args = [args] if not isinstance(args, (list, tuple)) else args
+
+    def __repr__(self):
+        if self.args:
+            return f'{self.function.__name__}{self.args}'
+        return self.function.__name__
+
+
+class PyClassSUL(SUL):
+    """
+    System under learning for inferring python classes.
+    """
+    def __init__(self, python_class):
+        """
+        Args:
+
+            python_class: class to be learned
+        """
+        super().__init__()
+        self._class = python_class
+        self.sul: object = None
+
+    def pre(self):
+        """
+        Do the reset by initializing the class again or call reset method of the class
+        """
+        self.sul = self._class()
+
+    def post(self):
+        pass
+
+    def step(self, letter):
+        """
+        Executes the function(with arguments) found in letter against the SUL
+
+        Args:
+
+            letter: single input of type FunctionDecorator
+
+        Returns:
+
+            output of the function
+
+        """
+        if letter.args:
+            return getattr(self.sul, letter.function.__name__, letter)(*letter.args)
+        return getattr(self.sul, letter.function.__name__, letter)()
```

## aalpy/SULs/RegexSUL.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-from aalpy.base import SUL
-import re
-
-
-class RegexSUL(SUL):
-    """
-    An example implementation of a system under learning that can be used to learn any regex expression.
-    Note that the $ is added to the expression as in this SUL only exact matches are learned.
-    """
-    def __init__(self, regex: str):
-        super().__init__()
-        self.regex = regex if regex[-1] == '$' else regex + '$'
-        self.string = ""
-
-    def pre(self):
-        self.string = ""
-        pass
-
-    def post(self):
-        self.string = ""
-        pass
-
-    def step(self, letter):
-        """
-
-        Args:
-
-            letter: single element of the input alphabet
-
-        Returns:
-
-            Whether the current string (previous string + letter) is accepted
-
-        """
-        if letter is not None:
-            self.string += str(letter)
-        return True if re.match(self.regex, self.string) else False
+from aalpy.base import SUL
+import re
+
+
+class RegexSUL(SUL):
+    """
+    An example implementation of a system under learning that can be used to learn any regex expression.
+    Note that the $ is added to the expression as in this SUL only exact matches are learned.
+    """
+    def __init__(self, regex: str):
+        super().__init__()
+        self.regex = regex if regex[-1] == '$' else regex + '$'
+        self.string = ""
+
+    def pre(self):
+        self.string = ""
+        pass
+
+    def post(self):
+        self.string = ""
+        pass
+
+    def step(self, letter):
+        """
+
+        Args:
+
+            letter: single element of the input alphabet
+
+        Returns:
+
+            Whether the current string (previous string + letter) is accepted
+
+        """
+        if letter is not None:
+            self.string += str(letter)
+        return True if re.match(self.regex, self.string) else False
```

## aalpy/SULs/TomitaSUL.py

 * *Ordering differences only*

```diff
@@ -1,68 +1,68 @@
-import re
-
-from aalpy.base import SUL
-
-
-class TomitaSUL(SUL):
-    """
-    Tomita grammars are often used as a benchmark for automata-related challenges. Simple SUL that implements all 7
-    Tomita grammars and enables their learning.
-    """
-
-    def __init__(self, tomita_level_fun):
-        super().__init__()
-        num_fun_map = {1: tomita_1, 2: tomita_2, 3: tomita_3, 4: tomita_4, 5: tomita_5, 6: tomita_6, 7: tomita_7,
-                       -3: not_tomita_3}
-        assert tomita_level_fun in num_fun_map.keys()
-        self.string = ""
-        self.tomita_level = num_fun_map[tomita_level_fun]
-
-    def pre(self):
-        self.string = ""
-        pass
-
-    def post(self):
-        self.string = ""
-        pass
-
-    def step(self, letter):
-        if input:
-            self.string += str(letter)
-        return self.tomita_level(self.string)
-
-
-_not_tomita_3 = re.compile("((0|1)*0)*1(11)*(0(0|1)*1)*0(00)*(1(0|1)*)*$")
-
-
-def tomita_1(word):
-    return "0" not in word
-
-
-def tomita_2(word):
-    return word == "10" * (int(len(word) / 2))
-
-
-def tomita_3(word):
-    if not _not_tomita_3.match(word):
-        return True
-    return False
-
-
-def not_tomita_3(word):
-    return not tomita_3(word)
-
-
-def tomita_4(word):
-    return "000" not in word
-
-
-def tomita_5(word):
-    return (word.count("0") % 2 == 0) and (word.count("1") % 2 == 0)
-
-
-def tomita_6(word):
-    return ((word.count("0") - word.count("1")) % 3) == 0
-
-
-def tomita_7(word):
-    return word.count("10") <= 1
+import re
+
+from aalpy.base import SUL
+
+
+class TomitaSUL(SUL):
+    """
+    Tomita grammars are often used as a benchmark for automata-related challenges. Simple SUL that implements all 7
+    Tomita grammars and enables their learning.
+    """
+
+    def __init__(self, tomita_level_fun):
+        super().__init__()
+        num_fun_map = {1: tomita_1, 2: tomita_2, 3: tomita_3, 4: tomita_4, 5: tomita_5, 6: tomita_6, 7: tomita_7,
+                       -3: not_tomita_3}
+        assert tomita_level_fun in num_fun_map.keys()
+        self.string = ""
+        self.tomita_level = num_fun_map[tomita_level_fun]
+
+    def pre(self):
+        self.string = ""
+        pass
+
+    def post(self):
+        self.string = ""
+        pass
+
+    def step(self, letter):
+        if input:
+            self.string += str(letter)
+        return self.tomita_level(self.string)
+
+
+_not_tomita_3 = re.compile("((0|1)*0)*1(11)*(0(0|1)*1)*0(00)*(1(0|1)*)*$")
+
+
+def tomita_1(word):
+    return "0" not in word
+
+
+def tomita_2(word):
+    return word == "10" * (int(len(word) / 2))
+
+
+def tomita_3(word):
+    if not _not_tomita_3.match(word):
+        return True
+    return False
+
+
+def not_tomita_3(word):
+    return not tomita_3(word)
+
+
+def tomita_4(word):
+    return "000" not in word
+
+
+def tomita_5(word):
+    return (word.count("0") % 2 == 0) and (word.count("1") % 2 == 0)
+
+
+def tomita_6(word):
+    return ((word.count("0") - word.count("1")) % 3) == 0
+
+
+def tomita_7(word):
+    return word.count("10") <= 1
```

## aalpy/SULs/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-from .AutomataSUL import DfaSUL, MealySUL, MooreSUL, MdpSUL, OnfsmSUL, StochasticMealySUL, McSUL
-from .PyMethodSUL import FunctionDecorator, PyClassSUL
-from .RegexSUL import RegexSUL
+from .AutomataSUL import DfaSUL, MealySUL, MooreSUL, MdpSUL, OnfsmSUL, StochasticMealySUL, McSUL
+from .PyMethodSUL import FunctionDecorator, PyClassSUL
+from .RegexSUL import RegexSUL
 from .TomitaSUL import TomitaSUL
```

## aalpy/automata/Dfa.py

```diff
@@ -1,58 +1,61 @@
-from aalpy.base import AutomatonState, DeterministicAutomaton
-
-
-class DfaState(AutomatonState):
-    """
-    Single state of a deterministic finite automaton.
-    """
-
-    def __init__(self, state_id, is_accepting=False):
-        super().__init__(state_id)
-        self.is_accepting = is_accepting
-
-
-class Dfa(DeterministicAutomaton):
-    """
-    Deterministic finite automaton.
-    """
-
-    def __init__(self, initial_state: DfaState, states):
-        super().__init__(initial_state, states)
-
-    def step(self, letter):
-        """
-        Args:
-
-            letter: single input that is looked up in the transition table of the DfaState
-
-        Returns:
-
-            True if the reached state is an accepting state, False otherwise
-        """
-        if letter is not None:
-            self.current_state = self.current_state.transitions[letter]
-        return self.current_state.is_accepting
-
-    def compute_characterization_set(self, char_set_init=None, online_suffix_closure=True, split_all_blocks=True,
-                                     return_same_states=False, raise_warning=True):
-        return super(Dfa, self).compute_characterization_set(char_set_init if char_set_init else [()],
-                                                             online_suffix_closure, split_all_blocks,
-                                                             return_same_states, raise_warning)
-
-    def compute_output_seq(self, state, sequence):
-        if not sequence:
-            return [state.is_accepting]
-        return super(Dfa, self).compute_output_seq(state, sequence)
-
-    def to_state_setup(self):
-        state_setup_dict = {}
-
-        # ensure prefixes are computed
-        self.compute_prefixes()
-
-        sorted_states = sorted(self.states, key=lambda x: len(x.prefix))
-        for s in sorted_states:
-            state_setup_dict[s.state_id] = (s.is_accepting, {k: v.state_id for k, v in s.transitions.items()})
-
-        return state_setup_dict
-
+from aalpy.base import AutomatonState, DeterministicAutomaton
+
+
+class DfaState(AutomatonState):
+    """
+    Single state of a deterministic finite automaton.
+    """
+
+    def __init__(self, state_id, is_accepting=False):
+        super().__init__(state_id)
+        self.is_accepting = is_accepting
+
+
+class Dfa(DeterministicAutomaton):
+    """
+    Deterministic finite automaton.
+    """
+
+    def __init__(self, initial_state: DfaState, states):
+        super().__init__(initial_state, states)
+
+    def step(self, letter):
+        """
+        Args:
+
+            letter: single input that is looked up in the transition table of the DfaState
+
+        Returns:
+
+            True if the reached state is an accepting state, False otherwise
+        """
+        if letter is not None:
+            self.current_state = self.current_state.transitions[letter]
+        return self.current_state.is_accepting
+
+    def compute_characterization_set(self, char_set_init=None, online_suffix_closure=True, split_all_blocks=True,
+                                     return_same_states=False, raise_warning=True):
+        return super(Dfa, self).compute_characterization_set(char_set_init if char_set_init else [()],
+                                                             online_suffix_closure, split_all_blocks,
+                                                             return_same_states, raise_warning)
+
+    def compute_output_seq(self, state, sequence):
+        if not sequence:
+            return [state.is_accepting]
+        return super(Dfa, self).compute_output_seq(state, sequence)
+
+    def to_state_setup(self):
+        state_setup_dict = {}
+
+        # ensure prefixes are computed
+        self.compute_prefixes()
+
+        sorted_states = sorted(self.states, key=lambda x: len(x.prefix))
+        for s in sorted_states:
+            state_setup_dict[s.state_id] = (s.is_accepting, {k: v.state_id for k, v in s.transitions.items()})
+
+        return state_setup_dict
+
+    def copy(self):
+        from aalpy.utils import dfa_from_state_setup
+        return dfa_from_state_setup(self.to_state_setup())
```

## aalpy/automata/MarkovChain.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-import random
-
-from aalpy.base import Automaton, AutomatonState
-
-
-class McState(AutomatonState):
-    def __init__(self, state_id, output):
-        super().__init__(state_id)
-        self.output = output
-        # transitions is a list of tuples (Node(output), probability)
-        self.transitions = list()
-
-
-class MarkovChain(Automaton):
-    """Markov Decision Process."""
-
-    def __init__(self, initial_state, states: list):
-        super().__init__(initial_state, states)
-
-    def reset_to_initial(self):
-        self.current_state = self.initial_state
-
-    def step(self, letter=None):
-        """Next step is determined based on transition probabilities of the current state.
-
-        Args:
-
-            letter: input
-
-        Returns:
-
-            output of the current state
-        """
-
-        if not self.current_state.transitions:
-            return self.current_state.output
-
-        probability_distributions = [i[1] for i in self.current_state.transitions]
-        states = [i[0] for i in self.current_state.transitions]
-
-        new_state = random.choices(states, probability_distributions, k=1)[0]
-
-        self.current_state = new_state
-        return self.current_state.output
-
-    def step_to(self, input):
-        """Performs a step on the automaton based on the input `inp` and output `out`.
-
-        Args:
-
-            input: input
-
-        Returns:
-
-            output of the reached state, None otherwise
-        """
-        for s in self.current_state.transitions:
-            if s[0].output == input:
-                self.current_state = s[0]
-                return self.current_state.output
+import random
+
+from aalpy.base import Automaton, AutomatonState
+
+
+class McState(AutomatonState):
+    def __init__(self, state_id, output):
+        super().__init__(state_id)
+        self.output = output
+        # transitions is a list of tuples (Node(output), probability)
+        self.transitions = list()
+
+
+class MarkovChain(Automaton):
+    """Markov Decision Process."""
+
+    def __init__(self, initial_state, states: list):
+        super().__init__(initial_state, states)
+
+    def reset_to_initial(self):
+        self.current_state = self.initial_state
+
+    def step(self, letter=None):
+        """Next step is determined based on transition probabilities of the current state.
+
+        Args:
+
+            letter: input
+
+        Returns:
+
+            output of the current state
+        """
+
+        if not self.current_state.transitions:
+            return self.current_state.output
+
+        probability_distributions = [i[1] for i in self.current_state.transitions]
+        states = [i[0] for i in self.current_state.transitions]
+
+        new_state = random.choices(states, probability_distributions, k=1)[0]
+
+        self.current_state = new_state
+        return self.current_state.output
+
+    def step_to(self, input):
+        """Performs a step on the automaton based on the input `inp` and output `out`.
+
+        Args:
+
+            input: input
+
+        Returns:
+
+            output of the reached state, None otherwise
+        """
+        for s in self.current_state.transitions:
+            if s[0].output == input:
+                self.current_state = s[0]
+                return self.current_state.output
         return None
```

## aalpy/automata/Mdp.py

```diff
@@ -1,62 +1,80 @@
-import random
-from collections import defaultdict
-
-from aalpy.base import Automaton, AutomatonState
-
-
-class MdpState(AutomatonState):
-    def __init__(self, state_id, output=None):
-        super().__init__(state_id)
-        self.output = output
-        # each child is a tuple (Node(output), probability)
-        self.transitions = defaultdict(list)
-
-
-class Mdp(Automaton):
-    """Markov Decision Process."""
-    def __init__(self, initial_state: MdpState, states: list):
-        super().__init__(initial_state, states)
-
-    def reset_to_initial(self):
-        self.current_state = self.initial_state
-
-    def step(self, letter):
-        """Next step is determined based on transition probabilities of the current state.
-
-        Args:
-
-            letter: input
-
-        Returns:
-
-            output of the current state
-        """
-        if letter is None:
-            return self.current_state.output
-
-        probability_distributions = [i[1] for i in self.current_state.transitions[letter]]
-        states = [i[0] for i in self.current_state.transitions[letter]]
-
-        new_state = random.choices(states, probability_distributions, k=1)[0]
-
-        self.current_state = new_state
-        return self.current_state.output
-
-    def step_to(self, inp, out):
-        """Performs a step on the automaton based on the input `inp` and output `out`.
-
-        Args:
-
-            inp: input
-            out: output
-
-        Returns:
-
-            output of the reached state, None otherwise
-        """
-        for new_state in self.current_state.transitions[inp]:
-            if new_state[0].output == out:
-                self.current_state = new_state[0]
-                return out
-        return None
-
+import random
+from collections import defaultdict
+
+from aalpy.base import Automaton, AutomatonState
+
+
+class MdpState(AutomatonState):
+    def __init__(self, state_id, output=None):
+        super().__init__(state_id)
+        self.output = output
+        # each child is a tuple (Node(output), probability)
+        self.transitions = defaultdict(list)
+
+
+class Mdp(Automaton):
+    """Markov Decision Process."""
+
+    def __init__(self, initial_state: MdpState, states: list):
+        super().__init__(initial_state, states)
+
+    def reset_to_initial(self):
+        self.current_state = self.initial_state
+
+    def step(self, letter):
+        """Next step is determined based on transition probabilities of the current state.
+
+        Args:
+
+            letter: input
+
+        Returns:
+
+            output of the current state
+        """
+        if letter is None:
+            return self.current_state.output
+
+        probability_distributions = [i[1] for i in self.current_state.transitions[letter]]
+        states = [i[0] for i in self.current_state.transitions[letter]]
+
+        new_state = random.choices(states, probability_distributions, k=1)[0]
+
+        self.current_state = new_state
+        return self.current_state.output
+
+    def step_to(self, inp, out):
+        """Performs a step on the automaton based on the input `inp` and output `out`.
+
+        Args:
+
+            inp: input
+            out: output
+
+        Returns:
+
+            output of the reached state, None otherwise
+        """
+        for new_state in self.current_state.transitions[inp]:
+            if new_state[0].output == out:
+                self.current_state = new_state[0]
+                return out
+        return None
+
+    def to_state_setup(self):
+        state_setup_dict = {}
+
+        # ensure initial state is first in the list
+        if self.states[0] != self.initial_state:
+            self.states.remove(self.initial_state)
+            self.states.insert(0, self.initial_state)
+
+        for s in self.states:
+            state_setup_dict[s.state_id] = (s.output, {k: [(node.state_id, prob) for node, prob in v]
+                                                       for k, v in s.transitions.items()})
+
+        return state_setup_dict
+
+    def copy(self):
+        from aalpy.utils import mdp_from_state_setup
+        return mdp_from_state_setup(self.to_state_setup())
```

## aalpy/automata/MealyMachine.py

```diff
@@ -1,45 +1,49 @@
-from aalpy.base import AutomatonState, DeterministicAutomaton
-
-
-class MealyState(AutomatonState):
-    """
-    Single state of a Mealy machine. Each state has an output_fun dictionary that maps inputs to outputs.
-    """
-
-    def __init__(self, state_id):
-        super().__init__(state_id)
-        self.output_fun = dict()
-
-
-class MealyMachine(DeterministicAutomaton):
-
-    def __init__(self, initial_state: MealyState, states):
-        super().__init__(initial_state, states)
-
-    def step(self, letter):
-        """
-        In Mealy machines, outputs depend on the input and the current state.
-
-            Args:
-
-                letter: single input that is looked up in the transition and output functions
-
-            Returns:
-
-                output corresponding to the input from the current state
-        """
-        output = self.current_state.output_fun[letter]
-        self.current_state = self.current_state.transitions[letter]
-        return output
-
-    def to_state_setup(self):
-        state_setup_dict = {}
-
-        # ensure prefixes are computed
-        self.compute_prefixes()
-
-        sorted_states = sorted(self.states, key=lambda x: len(x.prefix))
-        for s in sorted_states:
-            state_setup_dict[s.state_id] = {k: (s.output_fun[k], v.state_id) for k, v in s.transitions.items()}
-
-        return state_setup_dict
+from aalpy.base import AutomatonState, DeterministicAutomaton
+
+
+class MealyState(AutomatonState):
+    """
+    Single state of a Mealy machine. Each state has an output_fun dictionary that maps inputs to outputs.
+    """
+
+    def __init__(self, state_id):
+        super().__init__(state_id)
+        self.output_fun = dict()
+
+
+class MealyMachine(DeterministicAutomaton):
+
+    def __init__(self, initial_state: MealyState, states):
+        super().__init__(initial_state, states)
+
+    def step(self, letter):
+        """
+        In Mealy machines, outputs depend on the input and the current state.
+
+            Args:
+
+                letter: single input that is looked up in the transition and output functions
+
+            Returns:
+
+                output corresponding to the input from the current state
+        """
+        output = self.current_state.output_fun[letter]
+        self.current_state = self.current_state.transitions[letter]
+        return output
+
+    def to_state_setup(self):
+        state_setup_dict = {}
+
+        # ensure prefixes are computed
+        self.compute_prefixes()
+
+        sorted_states = sorted(self.states, key=lambda x: len(x.prefix))
+        for s in sorted_states:
+            state_setup_dict[s.state_id] = {k: (s.output_fun[k], v.state_id) for k, v in s.transitions.items()}
+
+        return state_setup_dict
+
+    def copy(self):
+        from aalpy.utils import mealy_from_state_setup
+        return mealy_from_state_setup(self.to_state_setup())
```

## aalpy/automata/MooreMachine.py

```diff
@@ -1,57 +1,61 @@
-from aalpy.base import AutomatonState, DeterministicAutomaton
-
-
-class MooreState(AutomatonState):
-    """
-    Single state of a Moore machine. Each state has an output value.
-    """
-
-    def __init__(self, state_id, output=None):
-        super().__init__(state_id)
-        self.output = output
-
-
-class MooreMachine(DeterministicAutomaton):
-
-    def __init__(self, initial_state: AutomatonState, states: list):
-        super().__init__(initial_state, states)
-
-    def step(self, letter):
-        """
-        In Moore machines outputs depend on the current state.
-
-        Args:
-
-            letter: single input that is looked up in the transition function leading to a new state
-
-        Returns:
-
-            the output of the reached state
-
-        """
-        if letter is not None:
-            self.current_state = self.current_state.transitions[letter]
-        return self.current_state.output
-
-    def compute_characterization_set(self, char_set_init=None, online_suffix_closure=True, split_all_blocks=True,
-                                     return_same_states=False, raise_warning=True):
-        return super(MooreMachine, self).compute_characterization_set(char_set_init if char_set_init else [()],
-                                                                      online_suffix_closure, split_all_blocks,
-                                                                      return_same_states, raise_warning)
-
-    def compute_output_seq(self, state, sequence):
-        if not sequence:
-            return [state.output]
-        return super(MooreMachine, self).compute_output_seq(state, sequence)
-
-    def to_state_setup(self):
-        state_setup_dict = {}
-
-        # ensure prefixes are computed
-        self.compute_prefixes()
-
-        sorted_states = sorted(self.states, key=lambda x: len(x.prefix))
-        for s in sorted_states:
-            state_setup_dict[s.state_id] = (s.output, {k: v.state_id for k, v in s.transitions.items()})
-
-        return state_setup_dict
+from aalpy.base import AutomatonState, DeterministicAutomaton
+
+
+class MooreState(AutomatonState):
+    """
+    Single state of a Moore machine. Each state has an output value.
+    """
+
+    def __init__(self, state_id, output=None):
+        super().__init__(state_id)
+        self.output = output
+
+
+class MooreMachine(DeterministicAutomaton):
+
+    def __init__(self, initial_state: AutomatonState, states: list):
+        super().__init__(initial_state, states)
+
+    def step(self, letter):
+        """
+        In Moore machines outputs depend on the current state.
+
+        Args:
+
+            letter: single input that is looked up in the transition function leading to a new state
+
+        Returns:
+
+            the output of the reached state
+
+        """
+        if letter is not None:
+            self.current_state = self.current_state.transitions[letter]
+        return self.current_state.output
+
+    def compute_characterization_set(self, char_set_init=None, online_suffix_closure=True, split_all_blocks=True,
+                                     return_same_states=False, raise_warning=True):
+        return super(MooreMachine, self).compute_characterization_set(char_set_init if char_set_init else [()],
+                                                                      online_suffix_closure, split_all_blocks,
+                                                                      return_same_states, raise_warning)
+
+    def compute_output_seq(self, state, sequence):
+        if not sequence:
+            return [state.output]
+        return super(MooreMachine, self).compute_output_seq(state, sequence)
+
+    def to_state_setup(self):
+        state_setup_dict = {}
+
+        # ensure prefixes are computed
+        self.compute_prefixes()
+
+        sorted_states = sorted(self.states, key=lambda x: len(x.prefix))
+        for s in sorted_states:
+            state_setup_dict[s.state_id] = (s.output, {k: v.state_id for k, v in s.transitions.items()})
+
+        return state_setup_dict
+
+    def copy(self):
+        from aalpy.utils import moore_from_state_setup
+        return moore_from_state_setup(self.to_state_setup())
```

## aalpy/automata/Onfsm.py

 * *Ordering differences only*

```diff
@@ -1,99 +1,99 @@
-from collections import defaultdict
-from random import choice
-
-from aalpy.base import Automaton, AutomatonState
-
-
-class OnfsmState(AutomatonState):
-    """ """
-    def __init__(self, state_id):
-        super().__init__(state_id)
-        # key/input maps to the list of tuples of possible output/new state [(output1, state1), (output2, state2)]
-        self.transitions = defaultdict(list)
-
-    def add_transition(self, inp, out, new_state):
-        """
-
-        Args:
-          inp: 
-          out: 
-          new_state: 
-
-        Returns:
-
-        """
-        self.transitions[inp].append((out, new_state))
-
-    def get_transition(self, input, output=None):
-        """
-
-        Args:
-          input: 
-          output:  (Default value = None)
-
-        Returns:
-
-        """
-        possible_transitions = self.transitions[input]
-        if output:
-            return next((t for t in possible_transitions if t[0] == output), None)
-        else:
-            return possible_transitions
-
-
-class Onfsm(Automaton):
-    """
-    Observable non-deterministic finite state automaton.
-    """
-    def __init__(self, initial_state: OnfsmState, states: list):
-        super().__init__(initial_state, states)
-
-    def step(self, letter):
-        """Next step is determined based on a uniform distribution over all transitions with the input 'letter'.
-
-        Args:
-
-            letter: input
-
-        Returns:
-
-            output of the probabilistically chosen transition
-
-        """
-        transition = choice(self.current_state.transitions[letter])
-        output = transition[0]
-        self.current_state = transition[1]
-        return output
-
-    def outputs_on_input(self, letter):
-        """All possible observable outputs after executing the current input 'letter'.
-
-        Args:
-
-            letter: input
-
-        Returns:
-
-            list of observable outputs
-
-        """
-        return [trans[0] for trans in self.current_state.transitions[letter]]
-
-    def step_to(self, inp, out):
-        """Performs a step on the automaton based on the input `inp` and output `out`.
-
-        Args:
-
-            inp: input
-            out: output
-
-        Returns:
-
-            output of the reached state, None otherwise
-
-        """
-        for new_state in self.current_state.transitions[inp]:
-            if new_state[0] == out:
-                self.current_state = new_state[1]
-                return out
-        return None
+from collections import defaultdict
+from random import choice
+
+from aalpy.base import Automaton, AutomatonState
+
+
+class OnfsmState(AutomatonState):
+    """ """
+    def __init__(self, state_id):
+        super().__init__(state_id)
+        # key/input maps to the list of tuples of possible output/new state [(output1, state1), (output2, state2)]
+        self.transitions = defaultdict(list)
+
+    def add_transition(self, inp, out, new_state):
+        """
+
+        Args:
+          inp: 
+          out: 
+          new_state: 
+
+        Returns:
+
+        """
+        self.transitions[inp].append((out, new_state))
+
+    def get_transition(self, input, output=None):
+        """
+
+        Args:
+          input: 
+          output:  (Default value = None)
+
+        Returns:
+
+        """
+        possible_transitions = self.transitions[input]
+        if output:
+            return next((t for t in possible_transitions if t[0] == output), None)
+        else:
+            return possible_transitions
+
+
+class Onfsm(Automaton):
+    """
+    Observable non-deterministic finite state automaton.
+    """
+    def __init__(self, initial_state: OnfsmState, states: list):
+        super().__init__(initial_state, states)
+
+    def step(self, letter):
+        """Next step is determined based on a uniform distribution over all transitions with the input 'letter'.
+
+        Args:
+
+            letter: input
+
+        Returns:
+
+            output of the probabilistically chosen transition
+
+        """
+        transition = choice(self.current_state.transitions[letter])
+        output = transition[0]
+        self.current_state = transition[1]
+        return output
+
+    def outputs_on_input(self, letter):
+        """All possible observable outputs after executing the current input 'letter'.
+
+        Args:
+
+            letter: input
+
+        Returns:
+
+            list of observable outputs
+
+        """
+        return [trans[0] for trans in self.current_state.transitions[letter]]
+
+    def step_to(self, inp, out):
+        """Performs a step on the automaton based on the input `inp` and output `out`.
+
+        Args:
+
+            inp: input
+            out: output
+
+        Returns:
+
+            output of the reached state, None otherwise
+
+        """
+        for new_state in self.current_state.transitions[inp]:
+            if new_state[0] == out:
+                self.current_state = new_state[1]
+                return out
+        return None
```

## aalpy/automata/StochasticMealyMachine.py

```diff
@@ -1,116 +1,135 @@
-import random
-from collections import defaultdict
-
-from aalpy.automata import MdpState, Mdp
-from aalpy.base import Automaton, AutomatonState
-
-
-class StochasticMealyState(AutomatonState):
-    """ """
-    def __init__(self, state_id):
-        super().__init__(state_id)
-        # each child is a tuple (newNode, output, probability)
-        self.transitions = defaultdict(list)
-
-
-class StochasticMealyMachine(Automaton):
-
-    def __init__(self, initial_state: StochasticMealyState, states: list):
-        super().__init__(initial_state, states)
-
-    def reset_to_initial(self):
-        self.current_state = self.initial_state
-
-    def step(self, letter):
-        """
-        Next step is determined based on transition probabilities of the current state.
-
-        Args:
-
-           letter: input
-
-        Returns:
-
-           output of the current state
-        """
-        prob = random.random()
-        probability_distributions = [i[2] for i in self.current_state.transitions[letter]]
-        index = 0
-        for i, p in enumerate(probability_distributions):
-            prob -= p
-            if prob <= 0:
-                index = i
-                break
-
-        transition = self.current_state.transitions[letter][index]
-        self.current_state = transition[0]
-        return transition[1]
-
-    def step_to(self, inp, out):
-        """Performs a step on the automaton based on the input `inp` and output `out`.
-
-        Args:
-
-            inp: input
-            out: output
-
-        Returns:
-
-            output of the reached state, None otherwise
-
-        """
-        for (new_state, output, prob) in self.current_state.transitions[inp]:
-            if output == out:
-                self.current_state = new_state
-                return out
-        return None
-
-    def to_mdp(self):
-        return smm_to_mdp_conversion(self)
-
-
-def smm_to_mdp_conversion(smm: StochasticMealyMachine):
-    """
-    Convert SMM to MDP.
-
-    Args:
-      smm: StochasticMealyMachine: SMM to convert
-
-    Returns:
-
-        equivalent MDP
-
-    """
-    inputs = smm.get_input_alphabet()
-    mdp_states = []
-    smm_state_to_mdp_state = dict()
-    init_state = MdpState("0", "___start___")
-    mdp_states.append(init_state)
-    for s in smm.states:
-        incoming_edges = defaultdict(list)
-        incoming_outputs = set()
-        for pre_s in smm.states:
-            for i in inputs:
-                incoming_edges[i] += filter(lambda t: t[0] == s, pre_s.transitions[i])
-                incoming_outputs.update(map(lambda t: t[1], incoming_edges[i]))
-        state_id = 0
-        for o in incoming_outputs:
-            new_state_id = s.state_id + str(state_id)
-            state_id += 1
-            new_state = MdpState(new_state_id, o)
-            mdp_states.append(new_state)
-            smm_state_to_mdp_state[(s.state_id, o)] = new_state
-
-    for s in smm.states:
-        mdp_states_for_s = {mdp_state for (s_id, o), mdp_state in smm_state_to_mdp_state.items() if s_id == s.state_id}
-        for i in inputs:
-            for outgoing_t in s.transitions[i]:
-                target_smm_state = outgoing_t[0]
-                output = outgoing_t[1]
-                prob = outgoing_t[2]
-                target_mdp_state = smm_state_to_mdp_state[(target_smm_state.state_id, output)]
-                for mdp_state in mdp_states_for_s:
-                    mdp_state.transitions[i].append((target_mdp_state, prob))
-                if s == smm.initial_state:
-                    init_state.transitions[i].append((target_mdp_state, prob))
-    return Mdp(init_state, mdp_states)
+import random
+from collections import defaultdict
+
+from aalpy.automata import MdpState, Mdp
+from aalpy.base import Automaton, AutomatonState
+
+
+class StochasticMealyState(AutomatonState):
+    """ """
+
+    def __init__(self, state_id):
+        super().__init__(state_id)
+        # each child is a tuple (newNode, output, probability)
+        self.transitions = defaultdict(list)
+
+
+class StochasticMealyMachine(Automaton):
+
+    def __init__(self, initial_state: StochasticMealyState, states: list):
+        super().__init__(initial_state, states)
+
+    def reset_to_initial(self):
+        self.current_state = self.initial_state
+
+    def step(self, letter):
+        """
+        Next step is determined based on transition probabilities of the current state.
+
+        Args:
+
+           letter: input
+
+        Returns:
+
+           output of the current state
+        """
+        prob = random.random()
+        probability_distributions = [i[2] for i in self.current_state.transitions[letter]]
+        index = 0
+        for i, p in enumerate(probability_distributions):
+            prob -= p
+            if prob <= 0:
+                index = i
+                break
+
+        transition = self.current_state.transitions[letter][index]
+        self.current_state = transition[0]
+        return transition[1]
+
+    def step_to(self, inp, out):
+        """Performs a step on the automaton based on the input `inp` and output `out`.
+
+        Args:
+
+            inp: input
+            out: output
+
+        Returns:
+
+            output of the reached state, None otherwise
+
+        """
+        for (new_state, output, prob) in self.current_state.transitions[inp]:
+            if output == out:
+                self.current_state = new_state
+                return out
+        return None
+
+    def to_mdp(self):
+        return smm_to_mdp_conversion(self)
+
+    def to_state_setup(self):
+        state_setup_dict = {}
+
+        # ensure initial state is first in the list
+        if self.states[0] != self.initial_state:
+            self.states.remove(self.initial_state)
+            self.states.insert(0, self.initial_state)
+
+        for s in self.states:
+            state_setup_dict[s.state_id] = {k: [(node.state_id, output, prob) for node, output, prob in v]
+                                            for k, v in s.transitions.items()}
+
+        return state_setup_dict
+
+    def copy(self):
+        from aalpy.utils import smm_from_state_setup
+        return smm_from_state_setup(self.to_state_setup())
+
+
+def smm_to_mdp_conversion(smm: StochasticMealyMachine):
+    """
+    Convert SMM to MDP.
+
+    Args:
+      smm: StochasticMealyMachine: SMM to convert
+
+    Returns:
+
+        equivalent MDP
+
+    """
+    inputs = smm.get_input_alphabet()
+    mdp_states = []
+    smm_state_to_mdp_state = dict()
+    init_state = MdpState("0", "___start___")
+    mdp_states.append(init_state)
+    for s in smm.states:
+        incoming_edges = defaultdict(list)
+        incoming_outputs = set()
+        for pre_s in smm.states:
+            for i in inputs:
+                incoming_edges[i] += filter(lambda t: t[0] == s, pre_s.transitions[i])
+                incoming_outputs.update(map(lambda t: t[1], incoming_edges[i]))
+        state_id = 0
+        for o in incoming_outputs:
+            new_state_id = s.state_id + str(state_id)
+            state_id += 1
+            new_state = MdpState(new_state_id, o)
+            mdp_states.append(new_state)
+            smm_state_to_mdp_state[(s.state_id, o)] = new_state
+
+    for s in smm.states:
+        mdp_states_for_s = {mdp_state for (s_id, o), mdp_state in smm_state_to_mdp_state.items() if s_id == s.state_id}
+        for i in inputs:
+            for outgoing_t in s.transitions[i]:
+                target_smm_state = outgoing_t[0]
+                output = outgoing_t[1]
+                prob = outgoing_t[2]
+                target_mdp_state = smm_state_to_mdp_state[(target_smm_state.state_id, output)]
+                for mdp_state in mdp_states_for_s:
+                    mdp_state.transitions[i].append((target_mdp_state, prob))
+                if s == smm.initial_state:
+                    init_state.transitions[i].append((target_mdp_state, prob))
+    return Mdp(init_state, mdp_states)
```

## aalpy/automata/__init__.py

 * *Ordering differences only*

```diff
@@ -1,7 +1,7 @@
-from .Dfa import Dfa, DfaState
-from .Mdp import Mdp, MdpState
-from .MealyMachine import MealyMachine, MealyState
-from .MooreMachine import MooreMachine, MooreState
-from .Onfsm import Onfsm, OnfsmState
-from .StochasticMealyMachine import StochasticMealyMachine, StochasticMealyState
-from .MarkovChain import MarkovChain, McState
+from .Dfa import Dfa, DfaState
+from .Mdp import Mdp, MdpState
+from .MealyMachine import MealyMachine, MealyState
+from .MooreMachine import MooreMachine, MooreState
+from .Onfsm import Onfsm, OnfsmState
+from .StochasticMealyMachine import StochasticMealyMachine, StochasticMealyState
+from .MarkovChain import MarkovChain, McState
```

## aalpy/base/Automaton.py

```diff
@@ -1,420 +1,424 @@
-import copy
-import warnings
-from abc import ABC, abstractmethod
-from collections import defaultdict
-from typing import Union
-
-
-class AutomatonState(ABC):
-
-    def __init__(self, state_id):
-        """
-        Single state of an automaton. Each state consists of a state id, a dictionary of transitions, where the keys are
-        inputs and the values are the corresponding target states, and a prefix that leads to the state from the initial
-        state.
-
-        Args:
-
-            state_id(Any): used for graphical representation of the state. A good practice is to keep it unique.
-
-        """
-        self.state_id = state_id
-        self.transitions = dict()
-        self.prefix = None
-
-    def get_diff_state_transitions(self) -> list:
-        """
-        Returns a list of transitions that lead to new states, not same-state transitions.
-        """
-        transitions = []
-        for trans, state in self.transitions.items():
-            if state != self:
-                transitions.append(trans)
-        return transitions
-
-    def get_same_state_transitions(self) -> list:
-        """
-        Get all transitions that lead to the same state (self loops).
-        """
-        dst = self.get_diff_state_transitions()
-        all_trans = set(self.transitions.keys())
-        return [t for t in all_trans if t not in dst]
-
-
-class Automaton(ABC):
-    """
-    Abstract class representing an automaton.
-    """
-
-    def __init__(self, initial_state, states: list):
-        """
-        Args:
-
-            initial_state (AutomatonState): initial state of the automaton
-            states (list) : list containing all states of the automaton
-
-        """
-        self.initial_state = initial_state
-        self.states = states
-        self.characterization_set: list = []
-        self.current_state = initial_state
-
-    @property
-    def size(self):
-        return len(self.states)
-
-    def reset_to_initial(self):
-        """
-        Resets the current state of the automaton to the initial state
-        """
-        self.current_state = self.initial_state
-
-    @abstractmethod
-    def step(self, letter):
-        """
-        Performs a single step on the automaton changing its current state.
-
-        Args:
-
-            letter: element of the input alphabet to be executed on the system under learning
-
-        Returns:
-
-            Output produced when executing the input letter from the current state
-
-        """
-        pass
-
-    def is_input_complete(self) -> bool:
-        """
-        Check whether all states have defined transition for all inputs
-        :return: true if automaton is input complete
-
-        Returns:
-
-            True if input complete, False otherwise
-
-        """
-        alphabet = set(self.get_input_alphabet())
-        for state in self.states:
-            if set(state.transitions.keys()) != alphabet:
-                return False
-        return True
-
-    def get_input_alphabet(self) -> list:
-        """
-        Returns the input alphabet
-        """
-        alphabet = list()
-        for s in self.states:
-            for i in s.transitions.keys():
-                if i not in alphabet:
-                    alphabet.append(i)
-        return list(alphabet)
-
-    def get_state_by_id(self, state_id) -> AutomatonState:
-        for state in self.states:
-            if state.state_id == state_id:
-                return state
-
-        return None
-
-    def __str__(self):
-        """
-        :return: A string representation of the automaton
-        """
-        from aalpy.utils import save_automaton_to_file
-        return save_automaton_to_file(self, path='learnedModel', file_type='string', round_floats=2)
-
-    def make_input_complete(self, missing_transition_go_to='self_loop'):
-        """
-        For more details check the implementation of this method in utils.HelperFunctions
-
-        missing_transition_go_to: either 'self_loop' or 'sink_state'.
-        """
-        from aalpy.utils.HelperFunctions import make_input_complete
-        make_input_complete(self, missing_transition_go_to)
-
-    def execute_sequence(self, origin_state, seq):
-        self.current_state = origin_state
-        return [self.step(s) for s in seq]
-
-    def save(self, file_path='LearnedModel'):
-        from aalpy.utils import save_automaton_to_file
-        save_automaton_to_file(self, path=file_path)
-
-    def visualize(self, path='LearnedModel', file_type='pdf', display_same_state_transitions=True):
-        from aalpy.utils import visualize_automaton
-        visualize_automaton(self, path, file_type, display_same_state_transitions)
-
-
-class DeterministicAutomaton(Automaton):
-
-    @abstractmethod
-    def step(self, letter):
-        pass
-
-    def get_shortest_path(self, origin_state: AutomatonState, target_state: AutomatonState) -> Union[tuple, None]:
-        """
-        Breath First Search over the automaton
-
-        Args:
-
-            origin_state (AutomatonState): state from which the BFS will start
-            target_state (AutomatonState): state that will be reached with the return value
-
-        Returns:
-
-            sequence of inputs that lead from origin_state to target state, or None if target state is not reachable
-            from origin state
-
-        """
-        if origin_state not in self.states or target_state not in self.states:
-            warnings.warn('Origin or target state not in automaton. Returning empty path.')
-            return ()
-
-        explored = []
-        queue = [[origin_state]]
-
-        if origin_state == target_state:
-            return ()
-
-        while queue:
-            path = queue.pop(0)
-            node = path[-1]
-            if node not in explored:
-                neighbours = node.transitions.values()
-                for neighbour in neighbours:
-                    new_path = list(path)
-                    new_path.append(neighbour)
-                    queue.append(new_path)
-                    # return path if neighbour is goal
-                    if neighbour == target_state:
-                        acc_seq = new_path[:-1]
-                        inputs = []
-                        for ind, state in enumerate(acc_seq):
-                            inputs.append(next(key for key, value in state.transitions.items()
-                                               if value == new_path[ind + 1]))
-                        return tuple(inputs)
-
-                # mark node as explored
-                explored.append(node)
-
-        return None
-
-    def is_strongly_connected(self) -> bool:
-        """
-        Check whether the automaton is strongly connected,
-        meaning that every state can be reached from every other state.
-
-        Returns:
-
-            True if strongly connected, False otherwise
-
-        """
-        import itertools
-
-        state_comb_list = itertools.permutations(self.states, 2)
-        for state_comb in state_comb_list:
-            if not self.get_shortest_path(state_comb[0], state_comb[1]):
-                return False
-        return True
-
-    def output_step(self, state, letter):
-        """
-            Given an input letter, compute the output response from a given state.
-            Args:
-                state: state from which the output response shall be computed
-                letter: an input letter from the alphabet
-
-            Returns: the single-step output response
-
-        """
-        state_save = self.current_state
-        self.current_state = state
-        output = self.step(letter)
-        self.current_state = state_save
-        return output
-
-    def find_distinguishing_seq(self, state1, state2):
-        """
-        A BFS to determine an input sequence that distinguishes two states in the automaton, i.e., a sequence such that
-        the output response from the given states is different. In a minimal automaton, this function always returns a
-        sequence different from None
-        Args:
-            state1: first state
-            state2: second state to distinguish
-
-        Returns: an input sequence distinguishing two states, or None if the states are equivalent
-
-        """
-        visited = set()
-        to_explore = [(state1, state2, [])]
-        alphabet = self.get_input_alphabet()
-        while to_explore:
-            (curr_s1, curr_s2, prefix) = to_explore.pop(0)
-            visited.add((curr_s1, curr_s2))
-            for i in alphabet:
-                o1 = self.output_step(curr_s1, i)
-                o2 = self.output_step(curr_s2, i)
-                new_prefix = prefix + [i]
-                if o1 != o2:
-                    return new_prefix
-                else:
-                    next_s1 = curr_s1.transitions[i]
-                    next_s2 = curr_s2.transitions[i]
-                    if (next_s1, next_s2) not in visited:
-                        to_explore.append((next_s1, next_s2, new_prefix))
-
-        return None
-
-    def compute_output_seq(self, state, sequence):
-        """
-        Given an input sequence, compute the output response from a given state.
-        Args:
-            state: state from which the output response shall be computed
-            sequence: an input sequence over the alphabet
-
-        Returns: the output response
-
-        """
-        state_save = self.current_state
-        output = self.execute_sequence(state, sequence)
-        self.current_state = state_save
-        return output
-
-    def is_minimal(self):
-        if not self.is_input_complete():
-            warnings.warn('Minimization of non input complete automata is not yet supported. Returning False.')
-            return False
-        return self.compute_characterization_set(raise_warning=False) is not None
-
-    def compute_characterization_set(self, char_set_init=None,
-                                     online_suffix_closure=True,
-                                     split_all_blocks=True,
-                                     return_same_states=False,
-                                     raise_warning=True):
-        """
-        Computation of a characterization set, that is, a set of sequences that can distinguish all states in the
-        automation. The implementation follows the approach for finding multiple preset diagnosing experiments described
-        by Arthur Gill in "Introduction to the Theory of Finite State Machines".
-        Some optional parameterized adaptations, e.g., for computing suffix-closed sets target the application in
-        L*-based learning and conformance testing.
-        The function only works for minimal automata.
-        Args:
-            char_set_init: a list of sequence that will be included in the characterization set, e.g., the input
-                        alphabet. A empty sequance is added to this list when using automata with state labels
-                        (DFA and Moore)
-            online_suffix_closure: if true, ensures suffix closedness of the characterization set at every computation
-                                step
-            split_all_blocks: if false, the computation follows the original tree-based strategy, where newly computed
-                        sequences are only checked on a subset of the states to be distinguished
-                        if true, sequences are used to distinguish all states, yielding a potentially smaller set, which
-                        is useful for conformance testing and learning
-            return_same_states: if True, a single distinguishable pair of states will be returned, or None None if there
-                        are no non-distinguishable states
-            raise_warning: prints warning message if characterization set cannot be computed
-
-        Returns: a characterization set or None if a non-minimal automaton is passed to the function
-
-        """
-        blocks = list()
-        blocks.append(copy.copy(self.states))
-        char_set = [] if not char_set_init else char_set_init
-        if char_set_init:
-            for seq in char_set_init:
-                blocks = self._split_blocks(blocks, seq)
-
-        while True:
-            # Given a partition (of states), this function returns a block with at least two elements.
-            try:
-                block_to_split = next(filter(lambda b: len(b) > 1, blocks))
-            except StopIteration:
-                block_to_split = None
-
-            if not block_to_split:
-                break
-            split_state1 = block_to_split[0]
-            split_state2 = block_to_split[1]
-            dist_seq = self.find_distinguishing_seq(split_state1, split_state2)
-            if dist_seq is None:
-                if return_same_states:
-                    return split_state1, split_state2
-
-                if raise_warning:
-                    warnings.warn("Automaton is non-canonical: could not compute characterization set."
-                                  "Returning None.")
-                return None
-
-            # in L*-based learning, we use suffix-closed column labels, so it makes sense to use a suffix-closed
-            # char set in this context
-            if online_suffix_closure:
-                dist_seq_closure = [tuple(dist_seq[len(dist_seq) - i - 1:]) for i in range(len(dist_seq))]
-            else:
-                dist_seq_closure = [tuple(dist_seq)]
-
-            # the standard approach described by Gill, computes a sequence that splits one block and really only splits
-            # one block, that is, it is only applied to the states in said block
-            # in L*-based learning we combine every prefix with every, therefore it makes sense to apply the sequence
-            # on all blocks and split all
-            if split_all_blocks:
-                for seq in dist_seq_closure:
-                    # seq may be in char_set if we do the closure on the fly
-                    if seq in char_set:
-                        continue
-                    char_set.append(seq)
-                    blocks = self._split_blocks(blocks, seq)
-            else:
-                blocks.remove(block_to_split)
-                new_blocks = [block_to_split]
-                for seq in dist_seq_closure:
-                    char_set.append(seq)
-                    new_blocks = self._split_blocks(new_blocks, seq)
-                for new_block in new_blocks:
-                    blocks.append(new_block)
-
-        char_set = list(set(char_set))
-        if return_same_states:
-            return None, None
-        return char_set
-
-    def _split_blocks(self, blocks, seq):
-        """
-        Refines a partition of states (blocks) using the output response to a given input sequence seq.
-        Args:
-            blocks: a partition of states
-            seq: an input sequence
-
-        Returns: a refined partition of states
-
-        """
-        new_blocks = []
-        for block in blocks:
-            block_after_split = defaultdict(list)
-            for state in block:
-                output_seq = tuple(self.compute_output_seq(state, seq))
-                block_after_split[output_seq].append(state)
-            for new_block in block_after_split.values():
-                new_blocks.append(new_block)
-        return new_blocks
-
-    def compute_prefixes(self):
-        for s in self.states:
-            if not s.prefix:
-                s.prefix = self.get_shortest_path(self.initial_state, s)
-
-    def minimize(self):
-        if not self.is_input_complete():
-            warnings.warn('Minimization of non input complete automata is not yet supported.\n Model not minimized.')
-            return
-
-        s1, s2 = self.compute_characterization_set(return_same_states=True)
-        while s1 and s2:
-            for s in self.states:
-                for i, new_state in s.transitions.items():
-                    if new_state == s2:
-                        s.transitions[i] = s1
-            self.states.remove(s2)
-            s1, s2 = self.compute_characterization_set(return_same_states=True)
-
-        self.compute_prefixes()
+import copy
+import warnings
+from abc import ABC, abstractmethod
+from collections import defaultdict
+from typing import Union
+
+
+class AutomatonState(ABC):
+
+    def __init__(self, state_id):
+        """
+        Single state of an automaton. Each state consists of a state id, a dictionary of transitions, where the keys are
+        inputs and the values are the corresponding target states, and a prefix that leads to the state from the initial
+        state.
+
+        Args:
+
+            state_id(Any): used for graphical representation of the state. A good practice is to keep it unique.
+
+        """
+        self.state_id = state_id
+        self.transitions = dict()
+        self.prefix = None
+
+    def get_diff_state_transitions(self) -> list:
+        """
+        Returns a list of transitions that lead to new states, not same-state transitions.
+        """
+        transitions = []
+        for trans, state in self.transitions.items():
+            if state != self:
+                transitions.append(trans)
+        return transitions
+
+    def get_same_state_transitions(self) -> list:
+        """
+        Get all transitions that lead to the same state (self loops).
+        """
+        dst = self.get_diff_state_transitions()
+        all_trans = set(self.transitions.keys())
+        return [t for t in all_trans if t not in dst]
+
+
+class Automaton(ABC):
+    """
+    Abstract class representing an automaton.
+    """
+
+    def __init__(self, initial_state, states: list):
+        """
+        Args:
+
+            initial_state (AutomatonState): initial state of the automaton
+            states (list) : list containing all states of the automaton
+
+        """
+        self.initial_state = initial_state
+        self.states = states
+        self.characterization_set: list = []
+        self.current_state = initial_state
+
+    @property
+    def size(self):
+        return len(self.states)
+
+    def reset_to_initial(self):
+        """
+        Resets the current state of the automaton to the initial state
+        """
+        self.current_state = self.initial_state
+
+    @abstractmethod
+    def step(self, letter):
+        """
+        Performs a single step on the automaton changing its current state.
+
+        Args:
+
+            letter: element of the input alphabet to be executed on the system under learning
+
+        Returns:
+
+            Output produced when executing the input letter from the current state
+
+        """
+        pass
+
+    def is_input_complete(self) -> bool:
+        """
+        Check whether all states have defined transition for all inputs
+        :return: true if automaton is input complete
+
+        Returns:
+
+            True if input complete, False otherwise
+
+        """
+        alphabet = set(self.get_input_alphabet())
+        for state in self.states:
+            if set(state.transitions.keys()) != alphabet:
+                return False
+        return True
+
+    def get_input_alphabet(self) -> list:
+        """
+        Returns the input alphabet
+        """
+        alphabet = list()
+        for s in self.states:
+            for i in s.transitions.keys():
+                if i not in alphabet:
+                    alphabet.append(i)
+        return list(alphabet)
+
+    def get_state_by_id(self, state_id) -> AutomatonState:
+        for state in self.states:
+            if state.state_id == state_id:
+                return state
+
+        return None
+
+    def __str__(self):
+        """
+        :return: A string representation of the automaton
+        """
+        from aalpy.utils import save_automaton_to_file
+        return save_automaton_to_file(self, path='learnedModel', file_type='string', round_floats=2)
+
+    def make_input_complete(self, missing_transition_go_to='self_loop'):
+        """
+        For more details check the implementation of this method in utils.HelperFunctions
+
+        missing_transition_go_to: either 'self_loop' or 'sink_state'.
+        """
+        from aalpy.utils.HelperFunctions import make_input_complete
+        make_input_complete(self, missing_transition_go_to)
+
+    def execute_sequence(self, origin_state, seq):
+        self.current_state = origin_state
+        return [self.step(s) for s in seq]
+
+    def save(self, file_path='LearnedModel'):
+        from aalpy.utils import save_automaton_to_file
+        save_automaton_to_file(self, path=file_path)
+
+    def visualize(self, path='LearnedModel', file_type='pdf', display_same_state_transitions=True):
+        from aalpy.utils import visualize_automaton
+        visualize_automaton(self, path, file_type, display_same_state_transitions)
+
+
+class DeterministicAutomaton(Automaton):
+
+    @abstractmethod
+    def step(self, letter):
+        pass
+
+    def get_shortest_path(self, origin_state: AutomatonState, target_state: AutomatonState) -> Union[tuple, None]:
+        """
+        Breath First Search over the automaton
+
+        Args:
+
+            origin_state (AutomatonState): state from which the BFS will start
+            target_state (AutomatonState): state that will be reached with the return value
+
+        Returns:
+
+            sequence of inputs that lead from origin_state to target state, or None if target state is not reachable
+            from origin state
+
+        """
+        if origin_state not in self.states or target_state not in self.states:
+            warnings.warn('Origin or target state not in automaton. Returning empty path.')
+            return ()
+
+        explored = []
+        queue = [[origin_state]]
+
+        if origin_state == target_state:
+            return ()
+
+        while queue:
+            path = queue.pop(0)
+            node = path[-1]
+            if node not in explored:
+                neighbours = node.transitions.values()
+                for neighbour in neighbours:
+                    new_path = list(path)
+                    new_path.append(neighbour)
+                    queue.append(new_path)
+                    # return path if neighbour is goal
+                    if neighbour == target_state:
+                        acc_seq = new_path[:-1]
+                        inputs = []
+                        for ind, state in enumerate(acc_seq):
+                            inputs.append(next(key for key, value in state.transitions.items()
+                                               if value == new_path[ind + 1]))
+                        return tuple(inputs)
+
+                # mark node as explored
+                explored.append(node)
+
+        return None
+
+    def is_strongly_connected(self) -> bool:
+        """
+        Check whether the automaton is strongly connected,
+        meaning that every state can be reached from every other state.
+
+        Returns:
+
+            True if strongly connected, False otherwise
+
+        """
+        import itertools
+
+        state_comb_list = itertools.permutations(self.states, 2)
+        for state_comb in state_comb_list:
+            if not self.get_shortest_path(state_comb[0], state_comb[1]):
+                return False
+        return True
+
+    def output_step(self, state, letter):
+        """
+            Given an input letter, compute the output response from a given state.
+            Args:
+                state: state from which the output response shall be computed
+                letter: an input letter from the alphabet
+
+            Returns: the single-step output response
+
+        """
+        state_save = self.current_state
+        self.current_state = state
+        output = self.step(letter)
+        self.current_state = state_save
+        return output
+
+    def find_distinguishing_seq(self, state1, state2):
+        """
+        A BFS to determine an input sequence that distinguishes two states in the automaton, i.e., a sequence such that
+        the output response from the given states is different. In a minimal automaton, this function always returns a
+        sequence different from None
+        Args:
+            state1: first state
+            state2: second state to distinguish
+
+        Returns: an input sequence distinguishing two states, or None if the states are equivalent
+
+        """
+        visited = set()
+        to_explore = [(state1, state2, [])]
+        alphabet = self.get_input_alphabet()
+        while to_explore:
+            (curr_s1, curr_s2, prefix) = to_explore.pop(0)
+            visited.add((curr_s1, curr_s2))
+            for i in alphabet:
+                o1 = self.output_step(curr_s1, i)
+                o2 = self.output_step(curr_s2, i)
+                new_prefix = prefix + [i]
+                if o1 != o2:
+                    return new_prefix
+                else:
+                    next_s1 = curr_s1.transitions[i]
+                    next_s2 = curr_s2.transitions[i]
+                    if (next_s1, next_s2) not in visited:
+                        to_explore.append((next_s1, next_s2, new_prefix))
+
+        return None
+
+    def compute_output_seq(self, state, sequence):
+        """
+        Given an input sequence, compute the output response from a given state.
+        Args:
+            state: state from which the output response shall be computed
+            sequence: an input sequence over the alphabet
+
+        Returns: the output response
+
+        """
+        state_save = self.current_state
+        output = self.execute_sequence(state, sequence)
+        self.current_state = state_save
+        return output
+
+    def is_minimal(self):
+        if not self.is_input_complete():
+            warnings.warn('Minimization of non input complete automata is not yet supported. Returning False.')
+            return False
+        return self.compute_characterization_set(raise_warning=False) is not None
+
+    def compute_characterization_set(self, char_set_init=None,
+                                     online_suffix_closure=True,
+                                     split_all_blocks=True,
+                                     return_same_states=False,
+                                     raise_warning=True):
+        """
+        Computation of a characterization set, that is, a set of sequences that can distinguish all states in the
+        automation. The implementation follows the approach for finding multiple preset diagnosing experiments described
+        by Arthur Gill in "Introduction to the Theory of Finite State Machines".
+        Some optional parameterized adaptations, e.g., for computing suffix-closed sets target the application in
+        L*-based learning and conformance testing.
+        The function only works for minimal automata.
+        Args:
+            char_set_init: a list of sequence that will be included in the characterization set, e.g., the input
+                        alphabet. A empty sequance is added to this list when using automata with state labels
+                        (DFA and Moore)
+            online_suffix_closure: if true, ensures suffix closedness of the characterization set at every computation
+                                step
+            split_all_blocks: if false, the computation follows the original tree-based strategy, where newly computed
+                        sequences are only checked on a subset of the states to be distinguished
+                        if true, sequences are used to distinguish all states, yielding a potentially smaller set, which
+                        is useful for conformance testing and learning
+            return_same_states: if True, a single distinguishable pair of states will be returned, or None None if there
+                        are no non-distinguishable states
+            raise_warning: prints warning message if characterization set cannot be computed
+
+        Returns: a characterization set or None if a non-minimal automaton is passed to the function
+
+        """
+        blocks = list()
+        blocks.append(copy.copy(self.states))
+        char_set = [] if not char_set_init else char_set_init
+        if char_set_init:
+            for seq in char_set_init:
+                blocks = self._split_blocks(blocks, seq)
+
+        while True:
+            # Given a partition (of states), this function returns a block with at least two elements.
+            try:
+                block_to_split = next(filter(lambda b: len(b) > 1, blocks))
+            except StopIteration:
+                block_to_split = None
+
+            if not block_to_split:
+                break
+            split_state1 = block_to_split[0]
+            split_state2 = block_to_split[1]
+            dist_seq = self.find_distinguishing_seq(split_state1, split_state2)
+            if dist_seq is None:
+                if return_same_states:
+                    return split_state1, split_state2
+
+                if raise_warning:
+                    warnings.warn("Automaton is non-canonical: could not compute characterization set."
+                                  "Returning None.")
+                return None
+
+            # in L*-based learning, we use suffix-closed column labels, so it makes sense to use a suffix-closed
+            # char set in this context
+            if online_suffix_closure:
+                dist_seq_closure = [tuple(dist_seq[len(dist_seq) - i - 1:]) for i in range(len(dist_seq))]
+            else:
+                dist_seq_closure = [tuple(dist_seq)]
+
+            # the standard approach described by Gill, computes a sequence that splits one block and really only splits
+            # one block, that is, it is only applied to the states in said block
+            # in L*-based learning we combine every prefix with every, therefore it makes sense to apply the sequence
+            # on all blocks and split all
+            if split_all_blocks:
+                for seq in dist_seq_closure:
+                    # seq may be in char_set if we do the closure on the fly
+                    if seq in char_set:
+                        continue
+                    char_set.append(seq)
+                    blocks = self._split_blocks(blocks, seq)
+            else:
+                blocks.remove(block_to_split)
+                new_blocks = [block_to_split]
+                for seq in dist_seq_closure:
+                    char_set.append(seq)
+                    new_blocks = self._split_blocks(new_blocks, seq)
+                for new_block in new_blocks:
+                    blocks.append(new_block)
+
+        char_set = list(set(char_set))
+        if return_same_states:
+            return None, None
+        return char_set
+
+    def _split_blocks(self, blocks, seq):
+        """
+        Refines a partition of states (blocks) using the output response to a given input sequence seq.
+        Args:
+            blocks: a partition of states
+            seq: an input sequence
+
+        Returns: a refined partition of states
+
+        """
+        new_blocks = []
+        for block in blocks:
+            block_after_split = defaultdict(list)
+            for state in block:
+                output_seq = tuple(self.compute_output_seq(state, seq))
+                block_after_split[output_seq].append(state)
+            for new_block in block_after_split.values():
+                new_blocks.append(new_block)
+        return new_blocks
+
+    def compute_prefixes(self):
+        for s in self.states:
+            if not s.prefix:
+                s.prefix = self.get_shortest_path(self.initial_state, s)
+
+    def minimize(self):
+        if not self.is_input_complete():
+            warnings.warn('Minimization of non input complete automata is not yet supported.\n Model not minimized.')
+            return
+
+        s1, s2 = self.compute_characterization_set(return_same_states=True)
+        while s1 and s2:
+            for s in self.states:
+                for i, new_state in s.transitions.items():
+                    if new_state == s2:
+                        s.transitions[i] = s1
+            self.states.remove(s2)
+            s1, s2 = self.compute_characterization_set(return_same_states=True)
+
+        self.compute_prefixes()
+
+    @abstractmethod
+    def copy(self):
+        return
```

## aalpy/base/CacheTree.py

 * *Ordering differences only*

```diff
@@ -1,169 +1,169 @@
-class Node(object):
-    __slots__ = ['value', 'children']
-
-    def __init__(self, value=None):
-        self.value = value
-        self.children = {}
-
-
-class CacheTree:
-    """
-    Tree in which all membership queries and corresponding outputs/values are stored. Membership queries update the tree
-    and while updating, check if determinism is maintained.
-    Root node corresponds to the initial state, and from that point on, for every new input/output pair, a new child is
-    created where the output is the value of the child, and the input is the transition leading from the parent to the
-    child.
-    """
-
-    def __init__(self):
-        self.root_node = Node()
-        self.curr_node = None
-        self.inputs = ()
-        self.outputs = ()
-
-    def reset(self):
-        self.curr_node = self.root_node
-        self.inputs = ()
-        self.outputs = ()
-
-    def step_in_cache(self, inp, out):
-        """
-        Preform a step in the cache. If output exist for the current state, and is not the same as `out`, throw
-        the non-determinism violation error and abort learning.
-        Args:
-
-            inp: input
-            out: output
-
-        """
-        self.inputs += (inp,)
-        self.outputs += (out,)
-        if inp is None:
-            self.root_node.value = out
-            return
-
-        if inp not in self.curr_node.children.keys():
-            node = Node(out)
-            self.curr_node.children[inp] = node
-        else:
-            node = self.curr_node.children[inp]
-            if node.value != out:
-                expected_seq = self.outputs[:-1]
-                expected_seq += (node.value,)
-                msg = f'Non-determinism detected.\n' \
-                      f'Error inserting: {self.inputs}\n' \
-                      f'Conflict detected: {node.value} vs {out}\n' \
-                      f'Expected Output: {expected_seq}\n' \
-                      f'Received output: {self.outputs}'
-                raise SystemExit(msg)
-        self.curr_node = node
-
-    def in_cache(self, input_seq: tuple):
-        """
-        Check if the result of the membership query for input_seq is cached is in the tree. If it is, return the
-        corresponding output sequence.
-
-        Args:
-
-            input_seq: corresponds to the membership query
-
-        Returns:
-
-            outputs associated with inputs if it is in the query, None otherwise
-
-        """
-        curr_node = self.root_node
-
-        output_seq = ()
-        for letter in input_seq:
-            if letter in curr_node.children.keys():
-                curr_node = curr_node.children[letter]
-                output_seq += (curr_node.value,)
-            else:
-                return None
-
-        return output_seq
-
-    def add_to_cache(self, input_sequence, output_sequence):
-        """
-        Add input-output sequence to cache
-        """
-        self.reset()
-        for i, o in zip(input_sequence, output_sequence):
-            self.step_in_cache(i, o)
-
-
-class CacheDict:
-    """
-    Dictionary in which all membership queries and corresponding outputs/values are stored. Membership queries update
-    the tree and while updating, check if determinism is maintained.
-    Root node corresponds to the initial state, and from that point on, for every new input/output pair, a new child is
-    created where the output is the value of the child, and the input is the transition leading from the parent to the
-    child.
-    """
-
-    def __init__(self):
-        self.cache_dict = dict()
-        self.inputs = ()
-
-    def reset(self):
-        self.inputs = ()
-        pass
-
-    def step_in_cache(self, inp, out):
-        """
-        Preform a step in the cache. If output exist for the current state, and is not the same as `out`, throw
-        the non-determinism violation error and abort learning.
-        Args:
-
-            inp: input
-            out: output
-
-        """
-
-        if inp is None:
-            return self.cache_dict[()]
-
-        self.inputs += (inp,)
-
-        if self.inputs not in self.cache_dict.keys():
-            self.cache_dict[self.inputs] = out
-        else:
-            cache_output = self.cache_dict[self.inputs]
-            if cache_output != out:
-                expected_seq = self.get_output_sequence(self.inputs)
-                received_seq = expected_seq[:-1] + (out,)
-                msg = f'Non-determinism detected.\n' \
-                      f'Error inserting: {self.inputs}\n' \
-                      f'Conflict detected: {cache_output} vs {out}\n' \
-                      f'Expected Output: {expected_seq}\n' \
-                      f'Received output: {received_seq}'
-                raise SystemExit(msg)
-
-    def in_cache(self, input_seq: tuple):
-        """
-        Check if the result of the membership query for input_seq is cached is in the tree. If it is, return the
-        corresponding output sequence.
-
-        Args:
-
-            input_seq: corresponds to the membership query
-
-        Returns:
-
-            outputs associated with inputs if it is in the query, None otherwise
-
-        """
-        if input_seq in self.cache_dict.keys():
-            return self.get_output_sequence(input_seq)
-        return None
-
-    def add_to_cache(self, input_sequence, output_sequence):
-        """
-        Add input-output sequence to cache
-        """
-        for i in range(1, len(input_sequence) + 1):
-            self.cache_dict[input_sequence[:i]] = output_sequence[i-1]
-
-    def get_output_sequence(self, input_seq):
-        return tuple(self.cache_dict[input_seq[:i]] for i in range(1, len(input_seq) + 1))
+class Node(object):
+    __slots__ = ['value', 'children']
+
+    def __init__(self, value=None):
+        self.value = value
+        self.children = {}
+
+
+class CacheTree:
+    """
+    Tree in which all membership queries and corresponding outputs/values are stored. Membership queries update the tree
+    and while updating, check if determinism is maintained.
+    Root node corresponds to the initial state, and from that point on, for every new input/output pair, a new child is
+    created where the output is the value of the child, and the input is the transition leading from the parent to the
+    child.
+    """
+
+    def __init__(self):
+        self.root_node = Node()
+        self.curr_node = None
+        self.inputs = ()
+        self.outputs = ()
+
+    def reset(self):
+        self.curr_node = self.root_node
+        self.inputs = ()
+        self.outputs = ()
+
+    def step_in_cache(self, inp, out):
+        """
+        Preform a step in the cache. If output exist for the current state, and is not the same as `out`, throw
+        the non-determinism violation error and abort learning.
+        Args:
+
+            inp: input
+            out: output
+
+        """
+        self.inputs += (inp,)
+        self.outputs += (out,)
+        if inp is None:
+            self.root_node.value = out
+            return
+
+        if inp not in self.curr_node.children.keys():
+            node = Node(out)
+            self.curr_node.children[inp] = node
+        else:
+            node = self.curr_node.children[inp]
+            if node.value != out:
+                expected_seq = self.outputs[:-1]
+                expected_seq += (node.value,)
+                msg = f'Non-determinism detected.\n' \
+                      f'Error inserting: {self.inputs}\n' \
+                      f'Conflict detected: {node.value} vs {out}\n' \
+                      f'Expected Output: {expected_seq}\n' \
+                      f'Received output: {self.outputs}'
+                raise SystemExit(msg)
+        self.curr_node = node
+
+    def in_cache(self, input_seq: tuple):
+        """
+        Check if the result of the membership query for input_seq is cached is in the tree. If it is, return the
+        corresponding output sequence.
+
+        Args:
+
+            input_seq: corresponds to the membership query
+
+        Returns:
+
+            outputs associated with inputs if it is in the query, None otherwise
+
+        """
+        curr_node = self.root_node
+
+        output_seq = ()
+        for letter in input_seq:
+            if letter in curr_node.children.keys():
+                curr_node = curr_node.children[letter]
+                output_seq += (curr_node.value,)
+            else:
+                return None
+
+        return output_seq
+
+    def add_to_cache(self, input_sequence, output_sequence):
+        """
+        Add input-output sequence to cache
+        """
+        self.reset()
+        for i, o in zip(input_sequence, output_sequence):
+            self.step_in_cache(i, o)
+
+
+class CacheDict:
+    """
+    Dictionary in which all membership queries and corresponding outputs/values are stored. Membership queries update
+    the tree and while updating, check if determinism is maintained.
+    Root node corresponds to the initial state, and from that point on, for every new input/output pair, a new child is
+    created where the output is the value of the child, and the input is the transition leading from the parent to the
+    child.
+    """
+
+    def __init__(self):
+        self.cache_dict = dict()
+        self.inputs = ()
+
+    def reset(self):
+        self.inputs = ()
+        pass
+
+    def step_in_cache(self, inp, out):
+        """
+        Preform a step in the cache. If output exist for the current state, and is not the same as `out`, throw
+        the non-determinism violation error and abort learning.
+        Args:
+
+            inp: input
+            out: output
+
+        """
+
+        if inp is None:
+            return self.cache_dict[()]
+
+        self.inputs += (inp,)
+
+        if self.inputs not in self.cache_dict.keys():
+            self.cache_dict[self.inputs] = out
+        else:
+            cache_output = self.cache_dict[self.inputs]
+            if cache_output != out:
+                expected_seq = self.get_output_sequence(self.inputs)
+                received_seq = expected_seq[:-1] + (out,)
+                msg = f'Non-determinism detected.\n' \
+                      f'Error inserting: {self.inputs}\n' \
+                      f'Conflict detected: {cache_output} vs {out}\n' \
+                      f'Expected Output: {expected_seq}\n' \
+                      f'Received output: {received_seq}'
+                raise SystemExit(msg)
+
+    def in_cache(self, input_seq: tuple):
+        """
+        Check if the result of the membership query for input_seq is cached is in the tree. If it is, return the
+        corresponding output sequence.
+
+        Args:
+
+            input_seq: corresponds to the membership query
+
+        Returns:
+
+            outputs associated with inputs if it is in the query, None otherwise
+
+        """
+        if input_seq in self.cache_dict.keys():
+            return self.get_output_sequence(input_seq)
+        return None
+
+    def add_to_cache(self, input_sequence, output_sequence):
+        """
+        Add input-output sequence to cache
+        """
+        for i in range(1, len(input_sequence) + 1):
+            self.cache_dict[input_sequence[:i]] = output_sequence[i-1]
+
+    def get_output_sequence(self, input_seq):
+        return tuple(self.cache_dict[input_seq[:i]] for i in range(1, len(input_seq) + 1))
```

## aalpy/base/Oracle.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-from abc import ABC, abstractmethod
-
-from aalpy.base import SUL
-
-
-class Oracle(ABC):
-    """Abstract class implemented by all equivalence oracles."""
-
-    def __init__(self, alphabet: list, sul: SUL):
-        """
-        Default constructor for all equivalence oracles.
-
-        Args:
-
-            alphabet: input alphabet
-            sul: system under learning
-        """
-
-        self.alphabet = alphabet
-        self.sul = sul
-        self.num_queries = 0
-        self.num_steps = 0
-
-    @abstractmethod
-    def find_cex(self, hypothesis):
-        """
-        Return a counterexample (inputs) that displays different behavior on system under learning and
-        current hypothesis.
-
-        Args:
-
-          hypothesis: current hypothesis
-
-        Returns:
-
-            tuple or list containing counterexample inputs, None if no counterexample is found
-        """
-        pass
-
-    def reset_hyp_and_sul(self, hypothesis):
-        """
-        Reset SUL and hypothesis to initial state.
-
-        Args:
-
-            hypothesis: current hypothesis
-
-        """
-        hypothesis.reset_to_initial()
-        self.sul.post()
-        self.sul.pre()
+from abc import ABC, abstractmethod
+
+from aalpy.base import SUL
+
+
+class Oracle(ABC):
+    """Abstract class implemented by all equivalence oracles."""
+
+    def __init__(self, alphabet: list, sul: SUL):
+        """
+        Default constructor for all equivalence oracles.
+
+        Args:
+
+            alphabet: input alphabet
+            sul: system under learning
+        """
+
+        self.alphabet = alphabet
+        self.sul = sul
+        self.num_queries = 0
+        self.num_steps = 0
+
+    @abstractmethod
+    def find_cex(self, hypothesis):
+        """
+        Return a counterexample (inputs) that displays different behavior on system under learning and
+        current hypothesis.
+
+        Args:
+
+          hypothesis: current hypothesis
+
+        Returns:
+
+            tuple or list containing counterexample inputs, None if no counterexample is found
+        """
+        pass
+
+    def reset_hyp_and_sul(self, hypothesis):
+        """
+        Reset SUL and hypothesis to initial state.
+
+        Args:
+
+            hypothesis: current hypothesis
+
+        """
+        hypothesis.reset_to_initial()
+        self.sul.post()
+        self.sul.pre()
         self.num_queries += 1
```

## aalpy/base/SUL.py

 * *Ordering differences only*

```diff
@@ -1,142 +1,142 @@
-from abc import ABC, abstractmethod
-
-from aalpy.base.CacheTree import CacheTree, CacheDict
-
-
-class SUL(ABC):
-    """
-    System Under Learning (SUL) abstract class. Defines the interaction between the learning algorithm and the system
-    under learning. All systems under learning have to implement this class, as it is
-    passed to the learning algorithm and the equivalence oracle.
-    """
-
-    def __init__(self):
-        self.num_queries = 0
-        self.num_steps = 0
-        self.num_cached_queries = 0
-
-    def query(self, word: tuple) -> list:
-        """
-        Performs a membership query on the SUL. Before the query, pre() method is called and after the query post()
-        method is called. Each letter in the word (input in the input sequence) is executed using the step method.
-
-        Args:
-
-            word: membership query (word consisting of letters/inputs)
-
-        Returns:
-
-            list of outputs, where the i-th output corresponds to the output of the system after the i-th input
-
-        """
-        self.pre()
-        # Empty string for DFA
-        if len(word) == 0:
-            out = [self.step(None)]
-        else:
-            out = [self.step(letter) for letter in word]
-        self.post()
-        self.num_queries += 1
-        self.num_steps += len(word)
-        return out
-
-    @abstractmethod
-    def pre(self):
-        """
-        Resets the system. Called after post method in the equivalence query.
-        """
-        pass
-
-    @abstractmethod
-    def post(self):
-        """
-        Performs additional cleanup on the system in necessary. Called before pre method in the equivalence query.
-        """
-        pass
-
-    @abstractmethod
-    def step(self, letter):
-        """
-        Executes an action on the system under learning and returns its result.
-
-        Args:
-
-            letter: Single input that is executed on the SUL.
-
-        Returns:
-
-            Output received after executing the input.
-
-        """
-        pass
-
-
-class CacheSUL(SUL):
-    """
-    System under learning that keeps a multiset of all queries in memory.
-    This multiset/cache is encoded as a tree.
-    """
-
-    def __init__(self, sul: SUL, cache_type='tree'):
-        super().__init__()
-        self.sul = sul
-        self.cache = CacheTree() if cache_type == 'tree' else CacheDict()
-
-    def query(self, word):
-        """
-        Performs a membership query on the SUL if and only if `word` is not a prefix of any trace in the cache.
-        Before the query, pre() method is called and after the query post()
-        method is called. Each letter in the word (input in the input sequence) is executed using the step method.
-
-        Args:
-
-            word: membership query (word consisting of letters/inputs)
-
-        Returns:
-
-            list of outputs, where the i-th output corresponds to the output of the system after the i-th input
-
-        """
-        cached_query = self.cache.in_cache(word)
-        if cached_query:
-            self.num_cached_queries += 1
-            return cached_query
-
-        # get outputs using default query method
-        out = self.sul.query(word)
-
-        # add input/outputs to tree
-        self.cache.reset()
-        for i, o in zip(word, out):
-            self.cache.step_in_cache(i, o)
-
-        self.num_queries += 1
-        self.num_steps += len(word)
-        return out
-
-    def pre(self):
-        """
-        Reset the system under learning and current node in the cache tree.
-        """
-        self.cache.reset()
-        self.sul.pre()
-
-    def post(self):
-        self.sul.post()
-
-    def step(self, letter):
-        """
-        Executes an action on the system under learning, adds it to the cache and returns its result.
-
-        Args:
-
-           letter: Single input that is executed on the SUL.
-
-        Returns:
-
-           Output received after executing the input.
-
-        """
-        out = self.sul.step(letter)
-        self.cache.step_in_cache(letter, out)
-        return out
+from abc import ABC, abstractmethod
+
+from aalpy.base.CacheTree import CacheTree, CacheDict
+
+
+class SUL(ABC):
+    """
+    System Under Learning (SUL) abstract class. Defines the interaction between the learning algorithm and the system
+    under learning. All systems under learning have to implement this class, as it is
+    passed to the learning algorithm and the equivalence oracle.
+    """
+
+    def __init__(self):
+        self.num_queries = 0
+        self.num_steps = 0
+        self.num_cached_queries = 0
+
+    def query(self, word: tuple) -> list:
+        """
+        Performs a membership query on the SUL. Before the query, pre() method is called and after the query post()
+        method is called. Each letter in the word (input in the input sequence) is executed using the step method.
+
+        Args:
+
+            word: membership query (word consisting of letters/inputs)
+
+        Returns:
+
+            list of outputs, where the i-th output corresponds to the output of the system after the i-th input
+
+        """
+        self.pre()
+        # Empty string for DFA
+        if len(word) == 0:
+            out = [self.step(None)]
+        else:
+            out = [self.step(letter) for letter in word]
+        self.post()
+        self.num_queries += 1
+        self.num_steps += len(word)
+        return out
+
+    @abstractmethod
+    def pre(self):
+        """
+        Resets the system. Called after post method in the equivalence query.
+        """
+        pass
+
+    @abstractmethod
+    def post(self):
+        """
+        Performs additional cleanup on the system in necessary. Called before pre method in the equivalence query.
+        """
+        pass
+
+    @abstractmethod
+    def step(self, letter):
+        """
+        Executes an action on the system under learning and returns its result.
+
+        Args:
+
+            letter: Single input that is executed on the SUL.
+
+        Returns:
+
+            Output received after executing the input.
+
+        """
+        pass
+
+
+class CacheSUL(SUL):
+    """
+    System under learning that keeps a multiset of all queries in memory.
+    This multiset/cache is encoded as a tree.
+    """
+
+    def __init__(self, sul: SUL, cache_type='tree'):
+        super().__init__()
+        self.sul = sul
+        self.cache = CacheTree() if cache_type == 'tree' else CacheDict()
+
+    def query(self, word):
+        """
+        Performs a membership query on the SUL if and only if `word` is not a prefix of any trace in the cache.
+        Before the query, pre() method is called and after the query post()
+        method is called. Each letter in the word (input in the input sequence) is executed using the step method.
+
+        Args:
+
+            word: membership query (word consisting of letters/inputs)
+
+        Returns:
+
+            list of outputs, where the i-th output corresponds to the output of the system after the i-th input
+
+        """
+        cached_query = self.cache.in_cache(word)
+        if cached_query:
+            self.num_cached_queries += 1
+            return cached_query
+
+        # get outputs using default query method
+        out = self.sul.query(word)
+
+        # add input/outputs to tree
+        self.cache.reset()
+        for i, o in zip(word, out):
+            self.cache.step_in_cache(i, o)
+
+        self.num_queries += 1
+        self.num_steps += len(word)
+        return out
+
+    def pre(self):
+        """
+        Reset the system under learning and current node in the cache tree.
+        """
+        self.cache.reset()
+        self.sul.pre()
+
+    def post(self):
+        self.sul.post()
+
+    def step(self, letter):
+        """
+        Executes an action on the system under learning, adds it to the cache and returns its result.
+
+        Args:
+
+           letter: Single input that is executed on the SUL.
+
+        Returns:
+
+           Output received after executing the input.
+
+        """
+        out = self.sul.step(letter)
+        self.cache.step_in_cache(letter, out)
+        return out
```

## aalpy/base/__init__.py

 * *Ordering differences only*

```diff
@@ -1,3 +1,3 @@
-from .Automaton import Automaton, AutomatonState, DeterministicAutomaton
-from .Oracle import Oracle
-from .SUL import SUL
+from .Automaton import Automaton, AutomatonState, DeterministicAutomaton
+from .Oracle import Oracle
+from .SUL import SUL
```

## aalpy/learning_algs/__init__.py

```diff
@@ -1,9 +1,10 @@
-# public API for running automata learning algorithms
-from .deterministic.LStar import run_Lstar
-from .deterministic.KV import run_KV
-from .non_deterministic.OnfsmLstar import run_non_det_Lstar
-from .non_deterministic.AbstractedOnfsmLstar import run_abstracted_ONFSM_Lstar
-from .stochastic.StochasticLStar import run_stochastic_Lstar
-from .stochastic_passive.Alergia import run_Alergia, run_JAlergia
-from .stochastic_passive.ActiveAleriga import run_active_Alergia
-from .deterministic_passive.RPNI import run_RPNI
+# public API for running automata learning algorithms
+from .deterministic.LStar import run_Lstar
+from .deterministic.KV import run_KV
+from .non_deterministic.OnfsmLstar import run_non_det_Lstar
+from .non_deterministic.AbstractedOnfsmLstar import run_abstracted_ONFSM_Lstar
+from .stochastic.StochasticLStar import run_stochastic_Lstar
+from .stochastic_passive.Alergia import run_Alergia, run_JAlergia
+from .stochastic_passive.ActiveAleriga import run_active_Alergia
+from .deterministic_passive.RPNI import run_RPNI
+from .deterministic_passive.active_RPNI import run_active_RPNI
```

## aalpy/learning_algs/deterministic/ClassificationTree.py

 * *Ordering differences only*

```diff
@@ -1,386 +1,386 @@
-from collections import defaultdict
-
-from aalpy.automata import DfaState, Dfa, MealyState, MealyMachine, MooreState, MooreMachine
-from aalpy.base import SUL
-
-automaton_class = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
-
-
-class CTNode:
-    __slots__ = ['parent', 'path_to_node']
-
-    def __init__(self, parent, path_to_node):
-        self.parent = parent
-        self.path_to_node = path_to_node
-
-    def is_leaf(self):
-        pass
-
-
-class CTInternalNode(CTNode):
-    __slots__ = ['distinguishing_string', 'children']
-
-    def __init__(self, distinguishing_string: tuple, parent, path_to_node):
-        super().__init__(parent, path_to_node)
-        self.distinguishing_string = distinguishing_string
-        self.children = defaultdict(None)  # {True: None, False: None}
-
-    def is_leaf(self):
-        return False
-
-
-class CTLeafNode(CTNode):
-    __slots__ = ['access_string']
-
-    def __init__(self, access_string: tuple, parent, path_to_node):
-        super().__init__(parent, path_to_node)
-        self.access_string = access_string
-
-    def __repr__(self):
-        return f"{self.__class__.__name__} '{self.access_string}'"
-
-    @property
-    def output(self):
-        c, p = self, self.parent
-        while p.parent:
-            c = p
-            p = p.parent
-        for output, child in p.children.items():
-            if child == c:
-                return output
-        assert False
-
-    def is_leaf(self):
-        return True
-
-
-class ClassificationTree:
-    def __init__(self, alphabet: list, sul: SUL, automaton_type: str, cex: tuple):
-        self.sul = sul
-        self.alphabet = alphabet
-        self.automaton_type = automaton_type
-
-        self.leaf_nodes = {}
-        self.query_cache = dict()
-
-        self.sifting_cache = {}
-
-        if self.automaton_type == "dfa" or self.automaton_type == 'moore':
-            initial_output = sul.query(())[-1]
-            cex_output = sul.query(cex)[-1]
-
-            self.query_cache[()] = initial_output
-
-            self.root = CTInternalNode(distinguishing_string=tuple(), parent=None, path_to_node=None)
-
-            initial_output_node = CTLeafNode(access_string=tuple(), parent=self.root, path_to_node=initial_output)
-            cex_output_node = CTLeafNode(access_string=cex, parent=self.root, path_to_node=cex_output)
-
-            self.root.children[initial_output] = initial_output_node
-            self.root.children[cex_output] = cex_output_node
-
-            self.leaf_nodes[tuple()] = initial_output_node
-            self.leaf_nodes[cex] = cex_output_node
-
-        else:
-            self.root = CTInternalNode(distinguishing_string=(cex[-1],), parent=None, path_to_node=None)
-
-            hypothesis_output = sul.query((cex[-1],))[-1]
-            cex_output = sul.query(cex)[-1]
-
-            hypothesis_output_node = CTLeafNode(access_string=tuple(), parent=self.root, path_to_node=hypothesis_output)
-            cex_output_node = CTLeafNode(access_string=cex[:-1], parent=self.root, path_to_node=cex_output)
-
-            self.root.children[hypothesis_output] = hypothesis_output_node
-            self.root.children[cex_output] = cex_output_node
-
-            self.leaf_nodes[tuple()] = self.root.children[hypothesis_output]
-            self.leaf_nodes[cex[:-1]] = self.root.children[cex_output]
-
-    def _sift(self, word):
-        """
-        Sifting a word into the classification tree.
-        Starting at the root, at every inner node (a CTInternalNode),
-        we branch into the child, depending on the result of the
-        membership query (word * node.distinguishing_string). Repeated until a leaf
-        (a CTLeafNode) is reached, which is the result of the sifting.
-
-        Args:
-
-            word: the word to sift into the discrimination tree (a tuple of all letters)
-
-        Returns:
-
-            the CTLeafNode that is reached by the sifting operation.
-        """
-        for letter in word:
-            assert letter is None or letter in self.alphabet
-
-        if word in self.sifting_cache:
-            return self.sifting_cache[word]
-
-        node = self.root
-        while not node.is_leaf():
-            query = word + node.distinguishing_string
-
-            if query not in self.query_cache.keys():
-                mq_result = self.sul.query(query)
-                # keep track of transitions (this might miss some due to other caching, but rest can be obtained from
-                # cache in gen hyp)
-                if self.automaton_type == 'mealy' and word not in self.query_cache.keys():
-                    self.query_cache[word] = mq_result[len(word) - 1]
-
-                mq_result = mq_result[-1]
-                self.query_cache[query] = mq_result
-            else:
-                mq_result = self.query_cache[query]
-
-            if mq_result not in node.children.keys():
-                new_leaf = CTLeafNode(access_string=word, parent=node, path_to_node=mq_result)
-                self.leaf_nodes[word] = new_leaf
-                node.children[mq_result] = new_leaf
-
-            node = node.children[mq_result]
-
-        self.sifting_cache[word] = node
-        assert node.is_leaf()
-        return node
-
-    def gen_hypothesis(self):
-        # for each CTLeafNode of this CT,
-        # create a state in the hypothesis that is labeled by that
-        # node's access string. The start state is the empty word
-        states = {}
-        initial_state = None
-        state_counter = 0
-        for node in self.leaf_nodes.values():
-
-            if self.automaton_type != "mealy":
-                # output = self._query_and_update_cache(node.access_string)
-                if self.automaton_type == 'dfa':
-                    new_state = DfaState(state_id=f's{state_counter}', is_accepting=node.output)
-                else:
-                    new_state = MooreState(state_id=f's{state_counter}', output=node.output)
-            else:
-                new_state = MealyState(state_id=f's{state_counter}')
-
-            new_state.prefix = node.access_string
-            if new_state.prefix == ():
-                initial_state = new_state
-            states[new_state.prefix] = new_state
-            state_counter += 1
-        assert initial_state is not None
-
-        # For each access state s of the hypothesis and each letter b in the
-        # alphabet, compute the b-transition out of state s by sifting s.state_id*b
-        states_for_transitions = list(states.values())
-        for state in states_for_transitions:
-            for letter in self.alphabet:
-                transition_target_node = self._sift(state.prefix + (letter,))
-                transition_target_access_string = transition_target_node.access_string
-
-                if self.automaton_type != "dfa" and transition_target_access_string not in states:
-                    if self.automaton_type == 'mealy':
-                        new_state = MealyState(state_id=f's{state_counter}')
-                    else:
-                        output = self._query_and_update_cache(transition_target_access_string)
-                        new_state = MooreState(state_id=f's{state_counter}', output=output)
-
-                    new_state.prefix = transition_target_access_string
-                    states_for_transitions.append(new_state)
-                    states[new_state.prefix] = new_state
-                    state_counter += 1
-
-                state.transitions[letter] = states[transition_target_access_string]
-
-                if self.automaton_type == "mealy":
-                    state.output_fun[letter] = self._query_and_update_cache(state.prefix + (letter,))
-
-        return automaton_class[self.automaton_type](initial_state=initial_state, states=list(states.values()))
-
-    def _least_common_ancestor(self, node_1_id, node_2_id):
-        """
-        Find the distinguishing string of the least common ancestor
-        of the leaf nodes node_1 and node_2. Both nodes have to exist.
-        Adapted from https://www.geeksforgeeks.org/lowest-common-ancestor-binary-tree-set-1/
-
-        Args:
-
-            node_1_id: first leaf node's id
-            node_2_id: second leaf node's id
-
-        Returns:
-
-            the distinguishing string of the lca
-
-        """
-
-        def ancestor(parent, node):
-            for child in parent.children.values():
-                if child.is_leaf():
-                    if child.access_string == node:
-                        return True
-                else:
-                    next_ancestor = ancestor(child, node)
-                    if next_ancestor:
-                        return True
-            return False
-
-        def findLCA(n1_id, n2_id):
-            node = self.leaf_nodes[n1_id]
-            parent = node.parent
-            while parent:
-                if ancestor(parent, n2_id):
-                    return parent
-                if parent.parent:
-                    parent = parent.parent
-                else:
-                    return parent
-            return None
-
-        return findLCA(node_1_id, node_2_id).distinguishing_string
-
-    def update(self, cex: tuple, hypothesis):
-        """
-        Updates the classification tree based on a counterexample.
-        - For each prefix cex[:i] of the counterexample, get
-              s_i      = self.sift(cex[:i])    and
-              s_star_i = id of the state with the access sequence cex[:i]
-                         in the hypothesis
-          and let j be the least i such that s_i != s_star_i.
-        - Replace the CTLeafNode labeled with the access string of the state
-          that is reached by the sequence cex[:j-1] in the hypothesis
-          with an CTInternalNode with two CTLeafNodes: one keeps the old
-          access string, and one gets the new access string cex[:j-1].
-          The internal node is labeled with the distinguishing string (cex[j-1],*d),
-          where d is the distinguishing string of the LCA of s_i and s_star_i.
-
-        Args:
-            cex: the counterexample used to update the tree
-            hypothesis: the former (wrong) hypothesis
-
-        """
-        j = d = None
-        for i in range(1, len(cex) + 1):
-            s_i = self._sift(cex[:i]).access_string
-            hypothesis.execute_sequence(hypothesis.initial_state, cex[:i])
-            s_star_i = hypothesis.current_state.prefix
-            if s_i != s_star_i:
-                j = i
-                d = self._least_common_ancestor(s_i, s_star_i)
-                break
-        if j is None and d is None:
-            j = len(cex)
-            d = []
-        assert j is not None and d is not None
-
-        hypothesis.execute_sequence(hypothesis.initial_state, cex[:j - 1] or tuple())
-
-        self._insert_new_leaf(discriminator=(cex[j - 1], *d),
-                              old_leaf_access_string=hypothesis.current_state.prefix,
-                              new_leaf_access_string=tuple(cex[:j - 1]) or tuple(),
-                              new_leaf_position=self.sul.query((*cex[:j - 1], *(cex[j - 1], *d)))[-1])
-
-    def update_rs(self, cex: tuple, hypothesis):
-        """
-        Updates the classification tree based on a counterexample,
-        using Rivest & Schapire's counterexample processing
-        - Replace the CTLeafNode labeled with the access string of the state
-          that is reached by the sequence cex[:j-1] in the hypothesis
-          with an CTInternalNode with two CTLeafNodes: one keeps the old
-          access string, and one gets the new access string cex[:j-1].
-          The internal node is labeled with the distinguishing string (cex[j-1],*d),
-          where d is the distinguishing string of the LCA of s_i and s_star_i.
-
-        Args:
-            cex: the counterexample used to update the tree
-            hypothesis: the former (wrong) hypothesis
-
-        """
-        from aalpy.learning_algs.deterministic.CounterExampleProcessing import rs_cex_processing
-        v = max(rs_cex_processing(self.sul, cex, hypothesis, suffix_closedness=True), key=len)
-        a = cex[len(cex) - len(v) - 1]
-        u = cex[:len(cex) - len(v) - 1]
-        assert (*u, a, *v) == cex
-
-        hypothesis.execute_sequence(hypothesis.initial_state, u)
-        u_state = hypothesis.current_state.prefix
-        hypothesis.step(a)
-        ua_state = hypothesis.current_state.prefix
-
-        if self.automaton_type == 'dfa':
-            new_leaf_position = not hypothesis.execute_sequence(hypothesis.initial_state, cex)[-1]
-        else:
-            new_leaf_position = self.sul.query(cex)[-1]
-
-        self._insert_new_leaf(discriminator=v,
-                              old_leaf_access_string=ua_state,
-                              new_leaf_access_string=(*u_state, a),
-                              new_leaf_position=new_leaf_position)
-
-    def _insert_new_leaf(self, discriminator, old_leaf_access_string, new_leaf_access_string, new_leaf_position):
-        """
-        Inserts a new leaf in the classification tree by:
-        - moving the leaf node specified by <old_leaf_access_string> down one level
-        - inserting an internal node  at the former position of the old node (i.e. as the parent of the old node)
-        - adding a new leaf node with <new_leaf_access_string> as child of the new internal node / sibling of the old node
-        Could also be thought of as 'splitting' the old node into two (one of which keeps the old access string and one
-        of which gets the new one) with <discriminator> as the distinguishing string between the two.
-
-        where one of the resulting nodes keeps the old
-        node's access string and the other gets new_leaf_access_string.
-        Args:
-            discriminator: The distinguishing string of the new internal node
-            old_leaf_access_string: The access string specifying the leaf node to be 'split' (or rather moved down)
-            new_leaf_access_string: The access string of the leaf node that will be created
-            new_leaf_position: The path from the new internal node to the new leaf node
-
-        Returns:
-
-        """
-        if self.automaton_type == "dfa":
-            other_leaf_position = not new_leaf_position
-        else:
-            # check if this query is in the node cache
-            other_leaf_position = self.sul.query((*old_leaf_access_string, *discriminator))[-1]
-
-        old_leaf = self.leaf_nodes[old_leaf_access_string]
-
-        # create an internal node at the same position as the old leaf node
-        discriminator_node = CTInternalNode(distinguishing_string=discriminator,
-                                            parent=old_leaf.parent, path_to_node=old_leaf.path_to_node)
-
-        # create the new leaf node and add it as child of the internal node
-        new_leaf = CTLeafNode(access_string=new_leaf_access_string,
-                              parent=discriminator_node,
-                              path_to_node=new_leaf_position)
-        self.leaf_nodes[new_leaf_access_string] = new_leaf
-
-        # redirect the old nodes former parent to the internal node
-        old_leaf.parent.children[old_leaf.path_to_node] = discriminator_node
-
-        # add the internal node as parent of the old leaf
-        old_leaf.parent = discriminator_node
-        old_leaf.path_to_node = other_leaf_position
-
-        # set the two nodes as children of the internal node
-        discriminator_node.children[new_leaf_position] = new_leaf
-        discriminator_node.children[other_leaf_position] = old_leaf
-
-        # sifting cache update
-        sifting_cache_outdated = []
-        if old_leaf in self.sifting_cache.values():
-            for prefix, node in self.sifting_cache.items():
-                if old_leaf == node:
-                    sifting_cache_outdated.append(prefix)
-
-        for to_delete in sifting_cache_outdated:
-            del self.sifting_cache[to_delete]
-
-    def _query_and_update_cache(self, word):
-        if word in self.query_cache.keys():
-            output = self.query_cache[word]
-        else:
-            output = self.sul.query(word)[-1]
-            self.query_cache[word] = output
-        return output
+from collections import defaultdict
+
+from aalpy.automata import DfaState, Dfa, MealyState, MealyMachine, MooreState, MooreMachine
+from aalpy.base import SUL
+
+automaton_class = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
+
+
+class CTNode:
+    __slots__ = ['parent', 'path_to_node']
+
+    def __init__(self, parent, path_to_node):
+        self.parent = parent
+        self.path_to_node = path_to_node
+
+    def is_leaf(self):
+        pass
+
+
+class CTInternalNode(CTNode):
+    __slots__ = ['distinguishing_string', 'children']
+
+    def __init__(self, distinguishing_string: tuple, parent, path_to_node):
+        super().__init__(parent, path_to_node)
+        self.distinguishing_string = distinguishing_string
+        self.children = defaultdict(None)  # {True: None, False: None}
+
+    def is_leaf(self):
+        return False
+
+
+class CTLeafNode(CTNode):
+    __slots__ = ['access_string']
+
+    def __init__(self, access_string: tuple, parent, path_to_node):
+        super().__init__(parent, path_to_node)
+        self.access_string = access_string
+
+    def __repr__(self):
+        return f"{self.__class__.__name__} '{self.access_string}'"
+
+    @property
+    def output(self):
+        c, p = self, self.parent
+        while p.parent:
+            c = p
+            p = p.parent
+        for output, child in p.children.items():
+            if child == c:
+                return output
+        assert False
+
+    def is_leaf(self):
+        return True
+
+
+class ClassificationTree:
+    def __init__(self, alphabet: list, sul: SUL, automaton_type: str, cex: tuple):
+        self.sul = sul
+        self.alphabet = alphabet
+        self.automaton_type = automaton_type
+
+        self.leaf_nodes = {}
+        self.query_cache = dict()
+
+        self.sifting_cache = {}
+
+        if self.automaton_type == "dfa" or self.automaton_type == 'moore':
+            initial_output = sul.query(())[-1]
+            cex_output = sul.query(cex)[-1]
+
+            self.query_cache[()] = initial_output
+
+            self.root = CTInternalNode(distinguishing_string=tuple(), parent=None, path_to_node=None)
+
+            initial_output_node = CTLeafNode(access_string=tuple(), parent=self.root, path_to_node=initial_output)
+            cex_output_node = CTLeafNode(access_string=cex, parent=self.root, path_to_node=cex_output)
+
+            self.root.children[initial_output] = initial_output_node
+            self.root.children[cex_output] = cex_output_node
+
+            self.leaf_nodes[tuple()] = initial_output_node
+            self.leaf_nodes[cex] = cex_output_node
+
+        else:
+            self.root = CTInternalNode(distinguishing_string=(cex[-1],), parent=None, path_to_node=None)
+
+            hypothesis_output = sul.query((cex[-1],))[-1]
+            cex_output = sul.query(cex)[-1]
+
+            hypothesis_output_node = CTLeafNode(access_string=tuple(), parent=self.root, path_to_node=hypothesis_output)
+            cex_output_node = CTLeafNode(access_string=cex[:-1], parent=self.root, path_to_node=cex_output)
+
+            self.root.children[hypothesis_output] = hypothesis_output_node
+            self.root.children[cex_output] = cex_output_node
+
+            self.leaf_nodes[tuple()] = self.root.children[hypothesis_output]
+            self.leaf_nodes[cex[:-1]] = self.root.children[cex_output]
+
+    def _sift(self, word):
+        """
+        Sifting a word into the classification tree.
+        Starting at the root, at every inner node (a CTInternalNode),
+        we branch into the child, depending on the result of the
+        membership query (word * node.distinguishing_string). Repeated until a leaf
+        (a CTLeafNode) is reached, which is the result of the sifting.
+
+        Args:
+
+            word: the word to sift into the discrimination tree (a tuple of all letters)
+
+        Returns:
+
+            the CTLeafNode that is reached by the sifting operation.
+        """
+        for letter in word:
+            assert letter is None or letter in self.alphabet
+
+        if word in self.sifting_cache:
+            return self.sifting_cache[word]
+
+        node = self.root
+        while not node.is_leaf():
+            query = word + node.distinguishing_string
+
+            if query not in self.query_cache.keys():
+                mq_result = self.sul.query(query)
+                # keep track of transitions (this might miss some due to other caching, but rest can be obtained from
+                # cache in gen hyp)
+                if self.automaton_type == 'mealy' and word not in self.query_cache.keys():
+                    self.query_cache[word] = mq_result[len(word) - 1]
+
+                mq_result = mq_result[-1]
+                self.query_cache[query] = mq_result
+            else:
+                mq_result = self.query_cache[query]
+
+            if mq_result not in node.children.keys():
+                new_leaf = CTLeafNode(access_string=word, parent=node, path_to_node=mq_result)
+                self.leaf_nodes[word] = new_leaf
+                node.children[mq_result] = new_leaf
+
+            node = node.children[mq_result]
+
+        self.sifting_cache[word] = node
+        assert node.is_leaf()
+        return node
+
+    def gen_hypothesis(self):
+        # for each CTLeafNode of this CT,
+        # create a state in the hypothesis that is labeled by that
+        # node's access string. The start state is the empty word
+        states = {}
+        initial_state = None
+        state_counter = 0
+        for node in self.leaf_nodes.values():
+
+            if self.automaton_type != "mealy":
+                # output = self._query_and_update_cache(node.access_string)
+                if self.automaton_type == 'dfa':
+                    new_state = DfaState(state_id=f's{state_counter}', is_accepting=node.output)
+                else:
+                    new_state = MooreState(state_id=f's{state_counter}', output=node.output)
+            else:
+                new_state = MealyState(state_id=f's{state_counter}')
+
+            new_state.prefix = node.access_string
+            if new_state.prefix == ():
+                initial_state = new_state
+            states[new_state.prefix] = new_state
+            state_counter += 1
+        assert initial_state is not None
+
+        # For each access state s of the hypothesis and each letter b in the
+        # alphabet, compute the b-transition out of state s by sifting s.state_id*b
+        states_for_transitions = list(states.values())
+        for state in states_for_transitions:
+            for letter in self.alphabet:
+                transition_target_node = self._sift(state.prefix + (letter,))
+                transition_target_access_string = transition_target_node.access_string
+
+                if self.automaton_type != "dfa" and transition_target_access_string not in states:
+                    if self.automaton_type == 'mealy':
+                        new_state = MealyState(state_id=f's{state_counter}')
+                    else:
+                        output = self._query_and_update_cache(transition_target_access_string)
+                        new_state = MooreState(state_id=f's{state_counter}', output=output)
+
+                    new_state.prefix = transition_target_access_string
+                    states_for_transitions.append(new_state)
+                    states[new_state.prefix] = new_state
+                    state_counter += 1
+
+                state.transitions[letter] = states[transition_target_access_string]
+
+                if self.automaton_type == "mealy":
+                    state.output_fun[letter] = self._query_and_update_cache(state.prefix + (letter,))
+
+        return automaton_class[self.automaton_type](initial_state=initial_state, states=list(states.values()))
+
+    def _least_common_ancestor(self, node_1_id, node_2_id):
+        """
+        Find the distinguishing string of the least common ancestor
+        of the leaf nodes node_1 and node_2. Both nodes have to exist.
+        Adapted from https://www.geeksforgeeks.org/lowest-common-ancestor-binary-tree-set-1/
+
+        Args:
+
+            node_1_id: first leaf node's id
+            node_2_id: second leaf node's id
+
+        Returns:
+
+            the distinguishing string of the lca
+
+        """
+
+        def ancestor(parent, node):
+            for child in parent.children.values():
+                if child.is_leaf():
+                    if child.access_string == node:
+                        return True
+                else:
+                    next_ancestor = ancestor(child, node)
+                    if next_ancestor:
+                        return True
+            return False
+
+        def findLCA(n1_id, n2_id):
+            node = self.leaf_nodes[n1_id]
+            parent = node.parent
+            while parent:
+                if ancestor(parent, n2_id):
+                    return parent
+                if parent.parent:
+                    parent = parent.parent
+                else:
+                    return parent
+            return None
+
+        return findLCA(node_1_id, node_2_id).distinguishing_string
+
+    def update(self, cex: tuple, hypothesis):
+        """
+        Updates the classification tree based on a counterexample.
+        - For each prefix cex[:i] of the counterexample, get
+              s_i      = self.sift(cex[:i])    and
+              s_star_i = id of the state with the access sequence cex[:i]
+                         in the hypothesis
+          and let j be the least i such that s_i != s_star_i.
+        - Replace the CTLeafNode labeled with the access string of the state
+          that is reached by the sequence cex[:j-1] in the hypothesis
+          with an CTInternalNode with two CTLeafNodes: one keeps the old
+          access string, and one gets the new access string cex[:j-1].
+          The internal node is labeled with the distinguishing string (cex[j-1],*d),
+          where d is the distinguishing string of the LCA of s_i and s_star_i.
+
+        Args:
+            cex: the counterexample used to update the tree
+            hypothesis: the former (wrong) hypothesis
+
+        """
+        j = d = None
+        for i in range(1, len(cex) + 1):
+            s_i = self._sift(cex[:i]).access_string
+            hypothesis.execute_sequence(hypothesis.initial_state, cex[:i])
+            s_star_i = hypothesis.current_state.prefix
+            if s_i != s_star_i:
+                j = i
+                d = self._least_common_ancestor(s_i, s_star_i)
+                break
+        if j is None and d is None:
+            j = len(cex)
+            d = []
+        assert j is not None and d is not None
+
+        hypothesis.execute_sequence(hypothesis.initial_state, cex[:j - 1] or tuple())
+
+        self._insert_new_leaf(discriminator=(cex[j - 1], *d),
+                              old_leaf_access_string=hypothesis.current_state.prefix,
+                              new_leaf_access_string=tuple(cex[:j - 1]) or tuple(),
+                              new_leaf_position=self.sul.query((*cex[:j - 1], *(cex[j - 1], *d)))[-1])
+
+    def update_rs(self, cex: tuple, hypothesis):
+        """
+        Updates the classification tree based on a counterexample,
+        using Rivest & Schapire's counterexample processing
+        - Replace the CTLeafNode labeled with the access string of the state
+          that is reached by the sequence cex[:j-1] in the hypothesis
+          with an CTInternalNode with two CTLeafNodes: one keeps the old
+          access string, and one gets the new access string cex[:j-1].
+          The internal node is labeled with the distinguishing string (cex[j-1],*d),
+          where d is the distinguishing string of the LCA of s_i and s_star_i.
+
+        Args:
+            cex: the counterexample used to update the tree
+            hypothesis: the former (wrong) hypothesis
+
+        """
+        from aalpy.learning_algs.deterministic.CounterExampleProcessing import rs_cex_processing
+        v = max(rs_cex_processing(self.sul, cex, hypothesis, suffix_closedness=True), key=len)
+        a = cex[len(cex) - len(v) - 1]
+        u = cex[:len(cex) - len(v) - 1]
+        assert (*u, a, *v) == cex
+
+        hypothesis.execute_sequence(hypothesis.initial_state, u)
+        u_state = hypothesis.current_state.prefix
+        hypothesis.step(a)
+        ua_state = hypothesis.current_state.prefix
+
+        if self.automaton_type == 'dfa':
+            new_leaf_position = not hypothesis.execute_sequence(hypothesis.initial_state, cex)[-1]
+        else:
+            new_leaf_position = self.sul.query(cex)[-1]
+
+        self._insert_new_leaf(discriminator=v,
+                              old_leaf_access_string=ua_state,
+                              new_leaf_access_string=(*u_state, a),
+                              new_leaf_position=new_leaf_position)
+
+    def _insert_new_leaf(self, discriminator, old_leaf_access_string, new_leaf_access_string, new_leaf_position):
+        """
+        Inserts a new leaf in the classification tree by:
+        - moving the leaf node specified by <old_leaf_access_string> down one level
+        - inserting an internal node  at the former position of the old node (i.e. as the parent of the old node)
+        - adding a new leaf node with <new_leaf_access_string> as child of the new internal node / sibling of the old node
+        Could also be thought of as 'splitting' the old node into two (one of which keeps the old access string and one
+        of which gets the new one) with <discriminator> as the distinguishing string between the two.
+
+        where one of the resulting nodes keeps the old
+        node's access string and the other gets new_leaf_access_string.
+        Args:
+            discriminator: The distinguishing string of the new internal node
+            old_leaf_access_string: The access string specifying the leaf node to be 'split' (or rather moved down)
+            new_leaf_access_string: The access string of the leaf node that will be created
+            new_leaf_position: The path from the new internal node to the new leaf node
+
+        Returns:
+
+        """
+        if self.automaton_type == "dfa":
+            other_leaf_position = not new_leaf_position
+        else:
+            # check if this query is in the node cache
+            other_leaf_position = self.sul.query((*old_leaf_access_string, *discriminator))[-1]
+
+        old_leaf = self.leaf_nodes[old_leaf_access_string]
+
+        # create an internal node at the same position as the old leaf node
+        discriminator_node = CTInternalNode(distinguishing_string=discriminator,
+                                            parent=old_leaf.parent, path_to_node=old_leaf.path_to_node)
+
+        # create the new leaf node and add it as child of the internal node
+        new_leaf = CTLeafNode(access_string=new_leaf_access_string,
+                              parent=discriminator_node,
+                              path_to_node=new_leaf_position)
+        self.leaf_nodes[new_leaf_access_string] = new_leaf
+
+        # redirect the old nodes former parent to the internal node
+        old_leaf.parent.children[old_leaf.path_to_node] = discriminator_node
+
+        # add the internal node as parent of the old leaf
+        old_leaf.parent = discriminator_node
+        old_leaf.path_to_node = other_leaf_position
+
+        # set the two nodes as children of the internal node
+        discriminator_node.children[new_leaf_position] = new_leaf
+        discriminator_node.children[other_leaf_position] = old_leaf
+
+        # sifting cache update
+        sifting_cache_outdated = []
+        if old_leaf in self.sifting_cache.values():
+            for prefix, node in self.sifting_cache.items():
+                if old_leaf == node:
+                    sifting_cache_outdated.append(prefix)
+
+        for to_delete in sifting_cache_outdated:
+            del self.sifting_cache[to_delete]
+
+    def _query_and_update_cache(self, word):
+        if word in self.query_cache.keys():
+            output = self.query_cache[word]
+        else:
+            output = self.sul.query(word)[-1]
+            self.query_cache[word] = output
+        return output
```

## aalpy/learning_algs/deterministic/CounterExampleProcessing.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-from aalpy.base import SUL
-from aalpy.utils.HelperFunctions import all_suffixes, all_prefixes
-
-
-def counterexample_successfully_processed(sul, cex, hypothesis):
-    cex_outputs = sul.query(cex)
-    hyp_outputs = hypothesis.execute_sequence(hypothesis.initial_state, cex)
-    return cex_outputs[-1] == hyp_outputs[-1]
-
-
-def longest_prefix_cex_processing(s_union_s_dot_a: list, cex: tuple, closedness='suffix'):
-    """
-    Suffix processing strategy found in Shahbaz-Groz paper 'Inferring Mealy Machines'.
-    It splits the counterexample into prefix and suffix. The prefix is the longest element of the S union S.A that
-    matches the beginning of the counterexample. By removing such prefixes from counterexample, no consistency check
-    is needed.
-
-    Args:
-
-        s_union_s_dot_a: list of all prefixes found in observation table sorted from shortest to longest
-        cex: counterexample
-        closedness: either 'suffix' or 'prefix'. (Default value = 'suffix')
-        s_union_s_dot_a: list:
-        cex: tuple: counterexample
-
-    Returns:
-
-        suffixes to add to the E set
-
-    """
-    prefixes = s_union_s_dot_a
-    prefixes.reverse()
-    trimmed_suffix = None
-
-    for p in prefixes:
-        if p == cex[:len(p)]:
-            trimmed_suffix = cex[len(p):]
-            break
-
-    trimmed_suffix = trimmed_suffix if trimmed_suffix else cex
-    suffixes = all_suffixes(trimmed_suffix) if closedness == 'suffix' else all_prefixes(trimmed_suffix)
-    suffixes.reverse()
-    return suffixes
-
-
-def rs_cex_processing(sul: SUL, cex: tuple, hypothesis, suffix_closedness=True, closedness='suffix'):
-    """Riverst-Schapire counter example processing.
-
-    Args:
-
-        sul: system under learning
-        cex: found counterexample
-        hypothesis: hypothesis on which counterexample was found
-        suffix_closedness: If true all suffixes will be added, else just one (Default value = True)
-        closedness: either 'suffix' or 'prefix'. (Default value = 'suffix')
-        sul: SUL: system under learning
-        cex: tuple: counterexample
-
-    Returns:
-
-        suffixes to be added to the E set
-
-    """
-    cex_out = sul.query(cex)
-    cex_input = list(cex)
-
-    lower = 1
-    upper = len(cex_input) - 2
-
-    while True:
-        hypothesis.reset_to_initial()
-        mid = (lower + upper) // 2
-
-        # arr[:n] -> first n values
-        # arr[n:] -> last n values
-
-        for s_p in cex_input[:mid]:
-            hypothesis.step(s_p)
-        s_bracket = hypothesis.current_state.prefix
-
-        d = tuple(cex_input[mid:])
-        mq = sul.query(s_bracket + d)
-
-        if mq[-1] == cex_out[-1]:  # only check if the last element is the same as the cex
-            lower = mid + 1
-            if upper < lower:
-                suffix = d[1:]
-                break
-        else:
-            upper = mid - 1
-            if upper < lower:
-                suffix = d
-                break
-
-    if suffix_closedness:
-        suffixes = all_suffixes(suffix) if closedness == 'suffix' else all_prefixes(suffix)
-        suffixes.reverse()
-        suffix_to_query = suffixes
-    else:
-        suffix_to_query = [suffix]
-    return suffix_to_query
+from aalpy.base import SUL
+from aalpy.utils.HelperFunctions import all_suffixes, all_prefixes
+
+
+def counterexample_successfully_processed(sul, cex, hypothesis):
+    cex_outputs = sul.query(cex)
+    hyp_outputs = hypothesis.execute_sequence(hypothesis.initial_state, cex)
+    return cex_outputs[-1] == hyp_outputs[-1]
+
+
+def longest_prefix_cex_processing(s_union_s_dot_a: list, cex: tuple, closedness='suffix'):
+    """
+    Suffix processing strategy found in Shahbaz-Groz paper 'Inferring Mealy Machines'.
+    It splits the counterexample into prefix and suffix. The prefix is the longest element of the S union S.A that
+    matches the beginning of the counterexample. By removing such prefixes from counterexample, no consistency check
+    is needed.
+
+    Args:
+
+        s_union_s_dot_a: list of all prefixes found in observation table sorted from shortest to longest
+        cex: counterexample
+        closedness: either 'suffix' or 'prefix'. (Default value = 'suffix')
+        s_union_s_dot_a: list:
+        cex: tuple: counterexample
+
+    Returns:
+
+        suffixes to add to the E set
+
+    """
+    prefixes = s_union_s_dot_a
+    prefixes.reverse()
+    trimmed_suffix = None
+
+    for p in prefixes:
+        if p == cex[:len(p)]:
+            trimmed_suffix = cex[len(p):]
+            break
+
+    trimmed_suffix = trimmed_suffix if trimmed_suffix else cex
+    suffixes = all_suffixes(trimmed_suffix) if closedness == 'suffix' else all_prefixes(trimmed_suffix)
+    suffixes.reverse()
+    return suffixes
+
+
+def rs_cex_processing(sul: SUL, cex: tuple, hypothesis, suffix_closedness=True, closedness='suffix'):
+    """Riverst-Schapire counter example processing.
+
+    Args:
+
+        sul: system under learning
+        cex: found counterexample
+        hypothesis: hypothesis on which counterexample was found
+        suffix_closedness: If true all suffixes will be added, else just one (Default value = True)
+        closedness: either 'suffix' or 'prefix'. (Default value = 'suffix')
+        sul: SUL: system under learning
+        cex: tuple: counterexample
+
+    Returns:
+
+        suffixes to be added to the E set
+
+    """
+    cex_out = sul.query(cex)
+    cex_input = list(cex)
+
+    lower = 1
+    upper = len(cex_input) - 2
+
+    while True:
+        hypothesis.reset_to_initial()
+        mid = (lower + upper) // 2
+
+        # arr[:n] -> first n values
+        # arr[n:] -> last n values
+
+        for s_p in cex_input[:mid]:
+            hypothesis.step(s_p)
+        s_bracket = hypothesis.current_state.prefix
+
+        d = tuple(cex_input[mid:])
+        mq = sul.query(s_bracket + d)
+
+        if mq[-1] == cex_out[-1]:  # only check if the last element is the same as the cex
+            lower = mid + 1
+            if upper < lower:
+                suffix = d[1:]
+                break
+        else:
+            upper = mid - 1
+            if upper < lower:
+                suffix = d
+                break
+
+    if suffix_closedness:
+        suffixes = all_suffixes(suffix) if closedness == 'suffix' else all_prefixes(suffix)
+        suffixes.reverse()
+        suffix_to_query = suffixes
+    else:
+        suffix_to_query = [suffix]
+    return suffix_to_query
```

## aalpy/learning_algs/deterministic/KV.py

 * *Ordering differences only*

```diff
@@ -1,158 +1,158 @@
-import time
-
-from aalpy.automata import Dfa, DfaState, MealyState, MealyMachine, MooreState, MooreMachine
-from aalpy.base import Oracle, SUL
-from aalpy.utils.HelperFunctions import print_learning_info
-from .ClassificationTree import ClassificationTree
-from .CounterExampleProcessing import counterexample_successfully_processed
-from ...base.SUL import CacheSUL
-
-print_options = [0, 1, 2, 3]
-counterexample_processing_strategy = [None, 'rs']
-automaton_class = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
-
-
-def run_KV(alphabet: list, sul: SUL, eq_oracle: Oracle, automaton_type, cex_processing='rs',
-           max_learning_rounds=None, cache_and_non_det_check=True, return_data=False, print_level=2):
-    """
-    Executes the KV algorithm.
-
-    Args:
-
-        alphabet: input alphabet
-
-        sul: system under learning
-
-        eq_oracle: equivalence oracle
-
-        automaton_type: type of automaton to be learned. One of 'dfa', 'mealy', 'moore'
-
-        cex_processing: None for no counterexample processing, or 'rs' for Rivest & Schapire counterexample processing
-
-        max_learning_rounds: number of learning rounds after which learning will terminate (Default value = None)
-
-        cache_and_non_det_check: Use caching and non-determinism checks (Default value = True)
-
-        return_data: if True, a map containing all information(runtime/#queries/#steps) will be returned
-            (Default value = False)
-
-        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
-            (Default value = 2)
-
-
-    Returns:
-
-        automaton of type automaton_type (dict containing all information about learning if 'return_data' is True)
-
-    """
-
-    assert print_level in print_options
-    assert cex_processing in counterexample_processing_strategy
-    assert automaton_type in [*automaton_class]
-
-    start_time = time.time()
-    eq_query_time = 0
-    learning_rounds = 0
-
-    if cache_and_non_det_check:
-        # Wrap the sul in the CacheSUL, so that all steps/queries are cached
-        sul = CacheSUL(sul)
-        eq_oracle.sul = sul
-
-    if automaton_type != 'mealy':
-        # Do a membership query on the empty string to determine whether
-        # the start state of the SUL is accepting or rejecting
-        empty_string_mq = sul.query(tuple())[-1]
-
-        # Construct a hypothesis automaton that consists simply of this
-        # single (accepting or rejecting) state with self-loops for
-        # all transitions.
-        if automaton_type == 'dfa':
-            initial_state = DfaState(state_id='s0', is_accepting=empty_string_mq)
-        else:
-            initial_state = MooreState(state_id='s0', output=empty_string_mq)
-    else:
-        initial_state = MealyState(state_id='s0')
-
-    initial_state.prefix = tuple()
-
-    for a in alphabet:
-        initial_state.transitions[a] = initial_state
-        if automaton_type == 'mealy':
-            initial_state.output_fun[a] = sul.query((a,))[-1]
-
-    hypothesis = automaton_class[automaton_type](initial_state, [initial_state])
-
-    # Perform an equivalence query on this automaton
-    eq_query_start = time.time()
-    cex = eq_oracle.find_cex(hypothesis)
-
-    eq_query_time += time.time() - eq_query_start
-    if cex is not None:
-        cex = tuple(cex)
-
-        # initialise the classification tree to have a root
-        # labeled with the empty word as the distinguishing string
-        # and two leaves labeled with access strings cex and empty word
-        classification_tree = ClassificationTree(alphabet=alphabet, sul=sul, automaton_type=automaton_type, cex=cex)
-
-        while True:
-            learning_rounds += 1
-            if max_learning_rounds and learning_rounds - 1 == max_learning_rounds:
-                break
-
-            hypothesis = classification_tree.gen_hypothesis()
-
-            if print_level == 2:
-                print(f'\rHypothesis {learning_rounds}: {hypothesis.size} states.', end="")
-
-            if print_level == 3:
-                # would be nice to have an option to print classification tree
-                print(f'Hypothesis {learning_rounds}: {hypothesis.size} states.')
-
-            if counterexample_successfully_processed(sul, cex, hypothesis):
-                # Perform an equivalence query on this automaton
-                eq_query_start = time.time()
-                cex = eq_oracle.find_cex(hypothesis)
-                eq_query_time += time.time() - eq_query_start
-
-                if cex is None:
-                    break
-                else:
-                    cex = tuple(cex)
-
-                if print_level == 3:
-                    print('Counterexample', cex)
-
-            if cex_processing == 'rs':
-                classification_tree.update_rs(cex, hypothesis)
-            else:
-                classification_tree.update(cex, hypothesis)
-
-    total_time = round(time.time() - start_time, 2)
-    eq_query_time = round(eq_query_time, 2)
-    learning_time = round(total_time - eq_query_time, 2)
-
-    info = {
-        'learning_rounds': learning_rounds,
-        'automaton_size': hypothesis.size,
-        'queries_learning': sul.num_queries,
-        'steps_learning': sul.num_steps,
-        'queries_eq_oracle': eq_oracle.num_queries,
-        'steps_eq_oracle': eq_oracle.num_steps,
-        'learning_time': learning_time,
-        'eq_oracle_time': eq_query_time,
-        'total_time': total_time,
-        'cache_saved': sul.num_cached_queries,
-    }
-
-    if print_level > 0:
-        if print_level == 2:
-            print("")
-        print_learning_info(info)
-
-    if return_data:
-        return hypothesis, info
-
-    return hypothesis
-
+import time
+
+from aalpy.automata import Dfa, DfaState, MealyState, MealyMachine, MooreState, MooreMachine
+from aalpy.base import Oracle, SUL
+from aalpy.utils.HelperFunctions import print_learning_info
+from .ClassificationTree import ClassificationTree
+from .CounterExampleProcessing import counterexample_successfully_processed
+from ...base.SUL import CacheSUL
+
+print_options = [0, 1, 2, 3]
+counterexample_processing_strategy = [None, 'rs']
+automaton_class = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
+
+
+def run_KV(alphabet: list, sul: SUL, eq_oracle: Oracle, automaton_type, cex_processing='rs',
+           max_learning_rounds=None, cache_and_non_det_check=True, return_data=False, print_level=2):
+    """
+    Executes the KV algorithm.
+
+    Args:
+
+        alphabet: input alphabet
+
+        sul: system under learning
+
+        eq_oracle: equivalence oracle
+
+        automaton_type: type of automaton to be learned. One of 'dfa', 'mealy', 'moore'
+
+        cex_processing: None for no counterexample processing, or 'rs' for Rivest & Schapire counterexample processing
+
+        max_learning_rounds: number of learning rounds after which learning will terminate (Default value = None)
+
+        cache_and_non_det_check: Use caching and non-determinism checks (Default value = True)
+
+        return_data: if True, a map containing all information(runtime/#queries/#steps) will be returned
+            (Default value = False)
+
+        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
+            (Default value = 2)
+
+
+    Returns:
+
+        automaton of type automaton_type (dict containing all information about learning if 'return_data' is True)
+
+    """
+
+    assert print_level in print_options
+    assert cex_processing in counterexample_processing_strategy
+    assert automaton_type in [*automaton_class]
+
+    start_time = time.time()
+    eq_query_time = 0
+    learning_rounds = 0
+
+    if cache_and_non_det_check:
+        # Wrap the sul in the CacheSUL, so that all steps/queries are cached
+        sul = CacheSUL(sul)
+        eq_oracle.sul = sul
+
+    if automaton_type != 'mealy':
+        # Do a membership query on the empty string to determine whether
+        # the start state of the SUL is accepting or rejecting
+        empty_string_mq = sul.query(tuple())[-1]
+
+        # Construct a hypothesis automaton that consists simply of this
+        # single (accepting or rejecting) state with self-loops for
+        # all transitions.
+        if automaton_type == 'dfa':
+            initial_state = DfaState(state_id='s0', is_accepting=empty_string_mq)
+        else:
+            initial_state = MooreState(state_id='s0', output=empty_string_mq)
+    else:
+        initial_state = MealyState(state_id='s0')
+
+    initial_state.prefix = tuple()
+
+    for a in alphabet:
+        initial_state.transitions[a] = initial_state
+        if automaton_type == 'mealy':
+            initial_state.output_fun[a] = sul.query((a,))[-1]
+
+    hypothesis = automaton_class[automaton_type](initial_state, [initial_state])
+
+    # Perform an equivalence query on this automaton
+    eq_query_start = time.time()
+    cex = eq_oracle.find_cex(hypothesis)
+
+    eq_query_time += time.time() - eq_query_start
+    if cex is not None:
+        cex = tuple(cex)
+
+        # initialise the classification tree to have a root
+        # labeled with the empty word as the distinguishing string
+        # and two leaves labeled with access strings cex and empty word
+        classification_tree = ClassificationTree(alphabet=alphabet, sul=sul, automaton_type=automaton_type, cex=cex)
+
+        while True:
+            learning_rounds += 1
+            if max_learning_rounds and learning_rounds - 1 == max_learning_rounds:
+                break
+
+            hypothesis = classification_tree.gen_hypothesis()
+
+            if print_level == 2:
+                print(f'\rHypothesis {learning_rounds}: {hypothesis.size} states.', end="")
+
+            if print_level == 3:
+                # would be nice to have an option to print classification tree
+                print(f'Hypothesis {learning_rounds}: {hypothesis.size} states.')
+
+            if counterexample_successfully_processed(sul, cex, hypothesis):
+                # Perform an equivalence query on this automaton
+                eq_query_start = time.time()
+                cex = eq_oracle.find_cex(hypothesis)
+                eq_query_time += time.time() - eq_query_start
+
+                if cex is None:
+                    break
+                else:
+                    cex = tuple(cex)
+
+                if print_level == 3:
+                    print('Counterexample', cex)
+
+            if cex_processing == 'rs':
+                classification_tree.update_rs(cex, hypothesis)
+            else:
+                classification_tree.update(cex, hypothesis)
+
+    total_time = round(time.time() - start_time, 2)
+    eq_query_time = round(eq_query_time, 2)
+    learning_time = round(total_time - eq_query_time, 2)
+
+    info = {
+        'learning_rounds': learning_rounds,
+        'automaton_size': hypothesis.size,
+        'queries_learning': sul.num_queries,
+        'steps_learning': sul.num_steps,
+        'queries_eq_oracle': eq_oracle.num_queries,
+        'steps_eq_oracle': eq_oracle.num_steps,
+        'learning_time': learning_time,
+        'eq_oracle_time': eq_query_time,
+        'total_time': total_time,
+        'cache_saved': sul.num_cached_queries,
+    }
+
+    if print_level > 0:
+        if print_level == 2:
+            print("")
+        print_learning_info(info)
+
+    if return_data:
+        return hypothesis, info
+
+    return hypothesis
+
```

## aalpy/learning_algs/deterministic/LStar.py

 * *Ordering differences only*

```diff
@@ -1,181 +1,181 @@
-import time
-
-from aalpy.base import Oracle, SUL
-from aalpy.utils.HelperFunctions import extend_set, print_learning_info, print_observation_table, all_prefixes
-from .CounterExampleProcessing import longest_prefix_cex_processing, rs_cex_processing, \
-    counterexample_successfully_processed
-from .ObservationTable import ObservationTable
-from ...base.SUL import CacheSUL
-
-counterexample_processing_strategy = [None, 'rs', 'longest_prefix']
-closedness_options = ['suffix_all', 'suffix_single']
-print_options = [0, 1, 2, 3]
-
-
-def run_Lstar(alphabet: list, sul: SUL, eq_oracle: Oracle, automaton_type, samples=None,
-              closing_strategy='shortest_first', cex_processing='rs',
-              e_set_suffix_closed=False, all_prefixes_in_obs_table=True,
-              max_learning_rounds=None, cache_and_non_det_check=True, return_data=False, print_level=2):
-    """
-    Executes L* algorithm.
-
-    Args:
-
-        alphabet: input alphabet
-
-        sul: system under learning
-
-        eq_oracle: equivalence oracle
-
-        automaton_type: type of automaton to be learned. Either 'dfa', 'mealy' or 'moore'.
-
-        samples: input output traces provided to the learning algorithm. They are added to cache and could reduce
-        total interaction with the system. Syntax: list of [(input_sequence, output_sequence)] or None
-
-        closing_strategy: closing strategy used in the close method. Either 'longest_first', 'shortest_first' or
-            'single' (Default value = 'shortest_first')
-
-        cex_processing: Counterexample processing strategy. Either None, 'rs' (Riverst-Schapire) or 'longest_prefix'.
-            (Default value = 'rs')
-
-        e_set_suffix_closed: True option ensures that E set is suffix closed,
-            False adds just a single suffix per counterexample.
-
-        all_prefixes_in_obs_table: if True, entries of observation table will contain the whole output of the whole
-            suffix, otherwise just the last output meaning that all prefixes of the suffix will be added.
-            If False, just a single suffix will be added.
-
-        max_learning_rounds: number of learning rounds after which learning will terminate (Default value = None)
-
-        cache_and_non_det_check: Use caching and non-determinism checks (Default value = True)
-
-        return_data: if True, a map containing all information(runtime/#queries/#steps) will be returned
-            (Default value = False)
-
-        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
-            (Default value = 2)
-
-    Returns:
-
-        automaton of type automaton_type (dict containing all information about learning if 'return_data' is True)
-
-    """
-
-    assert cex_processing in counterexample_processing_strategy
-    assert print_level in print_options
-
-    if cache_and_non_det_check or samples is not None:
-        # Wrap the sul in the CacheSUL, so that all steps/queries are cached
-        sul = CacheSUL(sul)
-        eq_oracle.sul = sul
-
-        if samples:
-            for input_seq, output_seq in samples:
-                sul.cache.add_to_cache(input_seq, output_seq)
-
-    start_time = time.time()
-    eq_query_time = 0
-    learning_rounds = 0
-    hypothesis = None
-
-    observation_table = ObservationTable(alphabet, sul, automaton_type, all_prefixes_in_obs_table)
-
-    # Initial update of observation table, for empty row
-    observation_table.update_obs_table()
-    cex = None
-
-    while True:
-        if max_learning_rounds and learning_rounds == max_learning_rounds:
-            break
-
-        # Make observation table consistent (iff there is no counterexample processing)
-        if not cex_processing:
-            inconsistent_rows = observation_table.get_causes_of_inconsistency()
-            while inconsistent_rows is not None:
-                added_suffix = extend_set(observation_table.E, inconsistent_rows)
-                observation_table.update_obs_table(e_set=added_suffix)
-                inconsistent_rows = observation_table.get_causes_of_inconsistency()
-
-        # Close observation table
-        rows_to_close = observation_table.get_rows_to_close(closing_strategy)
-        while rows_to_close is not None:
-            rows_to_query = []
-            for row in rows_to_close:
-                observation_table.S.append(row)
-                rows_to_query.extend([row + (a,) for a in alphabet])
-            observation_table.update_obs_table(s_set=rows_to_query)
-            rows_to_close = observation_table.get_rows_to_close(closing_strategy)
-
-        # Generate hypothesis
-        hypothesis = observation_table.gen_hypothesis(no_cex_processing_used=cex_processing is None)
-        # Find counterexample if none has previously been found (first round) and cex is successfully processed
-        # (not a counterexample in the current hypothesis)
-        if cex is None or counterexample_successfully_processed(sul, cex, hypothesis):
-            learning_rounds += 1
-
-            if print_level > 1:
-                print(f'Hypothesis {learning_rounds}: {len(hypothesis.states)} states.')
-
-            if print_level == 3:
-                print_observation_table(observation_table, 'det')
-
-            eq_query_start = time.time()
-            cex = eq_oracle.find_cex(hypothesis)
-            eq_query_time += time.time() - eq_query_start
-
-        # If no counterexample is found, return the hypothesis
-        if cex is None:
-            break
-
-        # make sure counterexample is a tuple in case oracle returns a list
-        cex = tuple(cex)
-
-        if print_level == 3:
-            print('Counterexample', cex)
-
-        # Process counterexample and ask membership queries
-        if not cex_processing:
-            s_to_update = []
-            added_rows = extend_set(observation_table.S, all_prefixes(cex))
-            s_to_update.extend(added_rows)
-            for p in added_rows:
-                s_to_update.extend([p + (a,) for a in alphabet])
-
-            observation_table.update_obs_table(s_set=s_to_update)
-            continue
-
-        elif cex_processing == 'longest_prefix':
-            cex_suffixes = longest_prefix_cex_processing(observation_table.S + list(observation_table.s_dot_a()),
-                                                         cex, closedness='suffix')
-        else:
-            cex_suffixes = rs_cex_processing(sul, cex, hypothesis, e_set_suffix_closed, closedness='suffix')
-
-        added_suffixes = extend_set(observation_table.E, cex_suffixes)
-        observation_table.update_obs_table(e_set=added_suffixes)
-
-    total_time = round(time.time() - start_time, 2)
-    eq_query_time = round(eq_query_time, 2)
-    learning_time = round(total_time - eq_query_time, 2)
-
-    info = {
-        'learning_rounds': learning_rounds,
-        'automaton_size': hypothesis.size,
-        'queries_learning': sul.num_queries,
-        'steps_learning': sul.num_steps,
-        'queries_eq_oracle': eq_oracle.num_queries,
-        'steps_eq_oracle': eq_oracle.num_steps,
-        'learning_time': learning_time,
-        'eq_oracle_time': eq_query_time,
-        'total_time': total_time,
-        'characterization_set': observation_table.E
-    }
-    if cache_and_non_det_check:
-        info['cache_saved'] = sul.num_cached_queries
-
-    if print_level > 0:
-        print_learning_info(info)
-
-    if return_data:
-        return hypothesis, info
-
-    return hypothesis
+import time
+
+from aalpy.base import Oracle, SUL
+from aalpy.utils.HelperFunctions import extend_set, print_learning_info, print_observation_table, all_prefixes
+from .CounterExampleProcessing import longest_prefix_cex_processing, rs_cex_processing, \
+    counterexample_successfully_processed
+from .ObservationTable import ObservationTable
+from ...base.SUL import CacheSUL
+
+counterexample_processing_strategy = [None, 'rs', 'longest_prefix']
+closedness_options = ['suffix_all', 'suffix_single']
+print_options = [0, 1, 2, 3]
+
+
+def run_Lstar(alphabet: list, sul: SUL, eq_oracle: Oracle, automaton_type, samples=None,
+              closing_strategy='shortest_first', cex_processing='rs',
+              e_set_suffix_closed=False, all_prefixes_in_obs_table=True,
+              max_learning_rounds=None, cache_and_non_det_check=True, return_data=False, print_level=2):
+    """
+    Executes L* algorithm.
+
+    Args:
+
+        alphabet: input alphabet
+
+        sul: system under learning
+
+        eq_oracle: equivalence oracle
+
+        automaton_type: type of automaton to be learned. Either 'dfa', 'mealy' or 'moore'.
+
+        samples: input output traces provided to the learning algorithm. They are added to cache and could reduce
+        total interaction with the system. Syntax: list of [(input_sequence, output_sequence)] or None
+
+        closing_strategy: closing strategy used in the close method. Either 'longest_first', 'shortest_first' or
+            'single' (Default value = 'shortest_first')
+
+        cex_processing: Counterexample processing strategy. Either None, 'rs' (Riverst-Schapire) or 'longest_prefix'.
+            (Default value = 'rs')
+
+        e_set_suffix_closed: True option ensures that E set is suffix closed,
+            False adds just a single suffix per counterexample.
+
+        all_prefixes_in_obs_table: if True, entries of observation table will contain the whole output of the whole
+            suffix, otherwise just the last output meaning that all prefixes of the suffix will be added.
+            If False, just a single suffix will be added.
+
+        max_learning_rounds: number of learning rounds after which learning will terminate (Default value = None)
+
+        cache_and_non_det_check: Use caching and non-determinism checks (Default value = True)
+
+        return_data: if True, a map containing all information(runtime/#queries/#steps) will be returned
+            (Default value = False)
+
+        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
+            (Default value = 2)
+
+    Returns:
+
+        automaton of type automaton_type (dict containing all information about learning if 'return_data' is True)
+
+    """
+
+    assert cex_processing in counterexample_processing_strategy
+    assert print_level in print_options
+
+    if cache_and_non_det_check or samples is not None:
+        # Wrap the sul in the CacheSUL, so that all steps/queries are cached
+        sul = CacheSUL(sul)
+        eq_oracle.sul = sul
+
+        if samples:
+            for input_seq, output_seq in samples:
+                sul.cache.add_to_cache(input_seq, output_seq)
+
+    start_time = time.time()
+    eq_query_time = 0
+    learning_rounds = 0
+    hypothesis = None
+
+    observation_table = ObservationTable(alphabet, sul, automaton_type, all_prefixes_in_obs_table)
+
+    # Initial update of observation table, for empty row
+    observation_table.update_obs_table()
+    cex = None
+
+    while True:
+        if max_learning_rounds and learning_rounds == max_learning_rounds:
+            break
+
+        # Make observation table consistent (iff there is no counterexample processing)
+        if not cex_processing:
+            inconsistent_rows = observation_table.get_causes_of_inconsistency()
+            while inconsistent_rows is not None:
+                added_suffix = extend_set(observation_table.E, inconsistent_rows)
+                observation_table.update_obs_table(e_set=added_suffix)
+                inconsistent_rows = observation_table.get_causes_of_inconsistency()
+
+        # Close observation table
+        rows_to_close = observation_table.get_rows_to_close(closing_strategy)
+        while rows_to_close is not None:
+            rows_to_query = []
+            for row in rows_to_close:
+                observation_table.S.append(row)
+                rows_to_query.extend([row + (a,) for a in alphabet])
+            observation_table.update_obs_table(s_set=rows_to_query)
+            rows_to_close = observation_table.get_rows_to_close(closing_strategy)
+
+        # Generate hypothesis
+        hypothesis = observation_table.gen_hypothesis(no_cex_processing_used=cex_processing is None)
+        # Find counterexample if none has previously been found (first round) and cex is successfully processed
+        # (not a counterexample in the current hypothesis)
+        if cex is None or counterexample_successfully_processed(sul, cex, hypothesis):
+            learning_rounds += 1
+
+            if print_level > 1:
+                print(f'Hypothesis {learning_rounds}: {len(hypothesis.states)} states.')
+
+            if print_level == 3:
+                print_observation_table(observation_table, 'det')
+
+            eq_query_start = time.time()
+            cex = eq_oracle.find_cex(hypothesis)
+            eq_query_time += time.time() - eq_query_start
+
+        # If no counterexample is found, return the hypothesis
+        if cex is None:
+            break
+
+        # make sure counterexample is a tuple in case oracle returns a list
+        cex = tuple(cex)
+
+        if print_level == 3:
+            print('Counterexample', cex)
+
+        # Process counterexample and ask membership queries
+        if not cex_processing:
+            s_to_update = []
+            added_rows = extend_set(observation_table.S, all_prefixes(cex))
+            s_to_update.extend(added_rows)
+            for p in added_rows:
+                s_to_update.extend([p + (a,) for a in alphabet])
+
+            observation_table.update_obs_table(s_set=s_to_update)
+            continue
+
+        elif cex_processing == 'longest_prefix':
+            cex_suffixes = longest_prefix_cex_processing(observation_table.S + list(observation_table.s_dot_a()),
+                                                         cex, closedness='suffix')
+        else:
+            cex_suffixes = rs_cex_processing(sul, cex, hypothesis, e_set_suffix_closed, closedness='suffix')
+
+        added_suffixes = extend_set(observation_table.E, cex_suffixes)
+        observation_table.update_obs_table(e_set=added_suffixes)
+
+    total_time = round(time.time() - start_time, 2)
+    eq_query_time = round(eq_query_time, 2)
+    learning_time = round(total_time - eq_query_time, 2)
+
+    info = {
+        'learning_rounds': learning_rounds,
+        'automaton_size': hypothesis.size,
+        'queries_learning': sul.num_queries,
+        'steps_learning': sul.num_steps,
+        'queries_eq_oracle': eq_oracle.num_queries,
+        'steps_eq_oracle': eq_oracle.num_steps,
+        'learning_time': learning_time,
+        'eq_oracle_time': eq_query_time,
+        'total_time': total_time,
+        'characterization_set': observation_table.E
+    }
+    if cache_and_non_det_check:
+        info['cache_saved'] = sul.num_cached_queries
+
+    if print_level > 0:
+        print_learning_info(info)
+
+    if return_data:
+        return hypothesis, info
+
+    return hypothesis
```

## aalpy/learning_algs/deterministic/ObservationTable.py

 * *Ordering differences only*

```diff
@@ -1,219 +1,219 @@
-from collections import defaultdict
-
-from aalpy.base import Automaton, SUL
-from aalpy.automata import Dfa, DfaState, MealyState, MealyMachine, MooreMachine, MooreState
-
-aut_type = ['dfa', 'mealy', 'moore']
-closing_options = ['shortest_first', 'longest_first', 'single', 'single_longest']
-
-
-class ObservationTable:
-    def __init__(self, alphabet: list, sul: SUL, automaton_type, prefixes_in_cell=False):
-        """
-        Constructor of the observation table. Initial queries are asked in the constructor.
-
-        Args:
-
-            alphabet: input alphabet
-            sul: system under learning
-            automaton_type: automaton type, one of ['dfa', 'mealy', 'moore']
-
-        Returns:
-
-        """
-        assert automaton_type in aut_type
-        assert alphabet is not None and sul is not None
-        self.automaton_type = automaton_type
-
-        # If True add prefixes of each element of E set to a cell, else only add the output
-        self.prefixes_in_cell = prefixes_in_cell
-
-        self.A = [tuple([a]) for a in alphabet]
-        self.S = list()  # prefixes of S
-        # DFA's can also take whole alphabet in E, this convention follows Angluin's paper
-        self.E = [] if self.automaton_type == 'dfa' else [tuple([a]) for a in alphabet]
-        # For performance reasons, the T function maps S to a tuple where element at index i is the element of the E
-        # set of index i. Therefore it is important to keep E set ordered and ask membership queries only when needed
-        # and in correct order. It would make more sense to implement it as a defaultdict(dict) where you can access
-        # elements via self.T[s][e], but it causes significant performance hit.
-        self.T = defaultdict(tuple)
-
-        self.sul = sul
-        empty_word = tuple()
-        self.S.append(empty_word)
-
-        # DFAs and Moore machines use empty word for identification of accepting states/state outputs
-        if self.automaton_type == 'dfa' or self.automaton_type == 'moore':
-            self.E.insert(0, empty_word)
-
-    def get_rows_to_close(self, closing_strategy='longest_first'):
-        """
-        Get rows for that need to be closed. Row selection is done according to closing_strategy.
-        The length of the row is defined by the length of the prefix corresponding to the row in the S set.
-        longest_first -> get all rows that need to be closed and ask membership queries for the longest row first
-        shortest_first -> get all rows that need to be closed and ask membership queries for the shortest row first
-        single -> find and ask membership query for the single row
-        single_longest -> returns single longest row to close
-
-        Args:
-
-            closing_strategy: one of ['shortest_first', 'longest_first', 'single'] (Default value = 'longest_first')
-
-        Returns:
-
-            list if non-closed exist, None otherwise: rows that will be moved to S set and closed
-
-        """
-        assert closing_strategy in closing_options
-        rows_to_close = []
-        row_values = set()
-
-        s_rows = {self.T[s] for s in self.S}
-
-        for t in self.s_dot_a():
-            row_t = self.T[t]
-            if row_t not in s_rows and row_t not in row_values:
-                rows_to_close.append(t)
-                row_values.add(row_t)
-
-                if closing_strategy == 'single':
-                    return rows_to_close
-
-        if not rows_to_close:
-            return None
-
-        if 'longest' in closing_strategy:
-            rows_to_close.sort(key=len, reverse=True)
-            if closing_strategy == 'longest_first':
-                return rows_to_close
-            if closing_strategy == 'single_longest':
-                return [rows_to_close[0]]
-
-        return rows_to_close
-
-    def get_causes_of_inconsistency(self):
-        """
-        If the two rows in the S set are the same, but their one letter extensions are not, this method founds
-        the cause of inconsistency and returns it.
-        :return:
-
-        Returns:
-
-            a+e values that are the causes of inconsistency
-
-        """
-        for i, s1 in enumerate(self.S):
-            for s2 in self.S[i + 1:]:
-                if self.T[s1] == self.T[s2]:
-                    for a in self.A:
-                        if self.T[s1 + a] != self.T[s2 + a]:
-                            for index, e in enumerate(self.E):
-                                if self.T[s1 + a][index] != self.T[s2 + a][index]:
-                                    return [(a + e)]
-
-        return None
-
-    def s_dot_a(self):
-        """
-        Helper generator function that returns extended S, or S.A set.
-        """
-        s_set = set(self.S)
-        for s in self.S:
-            for a in self.A:
-                if s + a not in s_set:
-                    yield s + a
-
-    def update_obs_table(self, s_set: list = None, e_set: list = None):
-        """
-        Perform the membership queries.
-
-        Args:
-
-            s_set: Prefixes of S set on which to preform membership queries. If None, then whole S set will be used.
-
-            e_set: Suffixes of E set on which to perform membership queries. If None, then whole E set will be used.
-
-        Returns:
-
-        """
-
-        update_S = s_set if s_set else list(self.S) + list(self.s_dot_a())
-        update_E = e_set if e_set else self.E
-
-        # This could save few queries
-        update_S.reverse()
-
-        for s in update_S:
-            for e in update_E:
-                if len(self.T[s]) != len(self.E):
-                    output = tuple(self.sul.query(s + e))
-                    if self.prefixes_in_cell and len(e) > 1:
-                        obs_table_entry = tuple([output[-len(e):]],)
-                    else:
-                        obs_table_entry = (output[-1],)
-                    self.T[s] += obs_table_entry
-
-    def gen_hypothesis(self, no_cex_processing_used=False) -> Automaton:
-        """
-        Generate automaton based on the values found in the observation table.
-        :return:
-
-        Args:
-
-            check_for_duplicate_rows:  (Default value = False)
-
-        Returns:
-
-            Automaton of type `automaton_type`
-
-        """
-        state_distinguish = dict()
-        states_dict = dict()
-        initial_state = None
-        automaton_class = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
-
-        s_set = self.S
-        # Added check for the algorithm without counterexample processing
-        if no_cex_processing_used:
-            s_set = self._get_row_representatives()
-
-        # create states based on S set
-        stateCounter = 0
-        for prefix in s_set:
-            state_id = f's{stateCounter}'
-
-            if self.automaton_type == 'dfa':
-                states_dict[prefix] = DfaState(state_id)
-                states_dict[prefix].is_accepting = self.T[prefix][0]
-            elif self.automaton_type == 'moore':
-                states_dict[prefix] = MooreState(state_id, output=self.T[prefix][0])
-            else:
-                states_dict[prefix] = MealyState(state_id)
-
-            states_dict[prefix].prefix = prefix
-            state_distinguish[tuple(self.T[prefix])] = states_dict[prefix]
-
-            if not prefix:
-                initial_state = states_dict[prefix]
-            stateCounter += 1
-
-        # add transitions based on extended S set
-        for prefix in s_set:
-            for a in self.A:
-                state_in_S = state_distinguish[self.T[prefix + a]]
-                states_dict[prefix].transitions[a[0]] = state_in_S
-                if self.automaton_type == 'mealy':
-                    states_dict[prefix].output_fun[a[0]] = self.T[prefix][self.E.index(a)]
-
-        automaton = automaton_class[self.automaton_type](initial_state, list(states_dict.values()))
-        automaton.characterization_set = self.E
-
-        return automaton
-
-    def _get_row_representatives(self):
-        self.S.sort(key=len)
-        representatives = defaultdict(list)
-        for prefix in self.S:
-            representatives[self.T[prefix]].append(prefix)
-
-        return [r[0] for r in representatives.values()]
+from collections import defaultdict
+
+from aalpy.base import Automaton, SUL
+from aalpy.automata import Dfa, DfaState, MealyState, MealyMachine, MooreMachine, MooreState
+
+aut_type = ['dfa', 'mealy', 'moore']
+closing_options = ['shortest_first', 'longest_first', 'single', 'single_longest']
+
+
+class ObservationTable:
+    def __init__(self, alphabet: list, sul: SUL, automaton_type, prefixes_in_cell=False):
+        """
+        Constructor of the observation table. Initial queries are asked in the constructor.
+
+        Args:
+
+            alphabet: input alphabet
+            sul: system under learning
+            automaton_type: automaton type, one of ['dfa', 'mealy', 'moore']
+
+        Returns:
+
+        """
+        assert automaton_type in aut_type
+        assert alphabet is not None and sul is not None
+        self.automaton_type = automaton_type
+
+        # If True add prefixes of each element of E set to a cell, else only add the output
+        self.prefixes_in_cell = prefixes_in_cell
+
+        self.A = [tuple([a]) for a in alphabet]
+        self.S = list()  # prefixes of S
+        # DFA's can also take whole alphabet in E, this convention follows Angluin's paper
+        self.E = [] if self.automaton_type == 'dfa' else [tuple([a]) for a in alphabet]
+        # For performance reasons, the T function maps S to a tuple where element at index i is the element of the E
+        # set of index i. Therefore it is important to keep E set ordered and ask membership queries only when needed
+        # and in correct order. It would make more sense to implement it as a defaultdict(dict) where you can access
+        # elements via self.T[s][e], but it causes significant performance hit.
+        self.T = defaultdict(tuple)
+
+        self.sul = sul
+        empty_word = tuple()
+        self.S.append(empty_word)
+
+        # DFAs and Moore machines use empty word for identification of accepting states/state outputs
+        if self.automaton_type == 'dfa' or self.automaton_type == 'moore':
+            self.E.insert(0, empty_word)
+
+    def get_rows_to_close(self, closing_strategy='longest_first'):
+        """
+        Get rows for that need to be closed. Row selection is done according to closing_strategy.
+        The length of the row is defined by the length of the prefix corresponding to the row in the S set.
+        longest_first -> get all rows that need to be closed and ask membership queries for the longest row first
+        shortest_first -> get all rows that need to be closed and ask membership queries for the shortest row first
+        single -> find and ask membership query for the single row
+        single_longest -> returns single longest row to close
+
+        Args:
+
+            closing_strategy: one of ['shortest_first', 'longest_first', 'single'] (Default value = 'longest_first')
+
+        Returns:
+
+            list if non-closed exist, None otherwise: rows that will be moved to S set and closed
+
+        """
+        assert closing_strategy in closing_options
+        rows_to_close = []
+        row_values = set()
+
+        s_rows = {self.T[s] for s in self.S}
+
+        for t in self.s_dot_a():
+            row_t = self.T[t]
+            if row_t not in s_rows and row_t not in row_values:
+                rows_to_close.append(t)
+                row_values.add(row_t)
+
+                if closing_strategy == 'single':
+                    return rows_to_close
+
+        if not rows_to_close:
+            return None
+
+        if 'longest' in closing_strategy:
+            rows_to_close.sort(key=len, reverse=True)
+            if closing_strategy == 'longest_first':
+                return rows_to_close
+            if closing_strategy == 'single_longest':
+                return [rows_to_close[0]]
+
+        return rows_to_close
+
+    def get_causes_of_inconsistency(self):
+        """
+        If the two rows in the S set are the same, but their one letter extensions are not, this method founds
+        the cause of inconsistency and returns it.
+        :return:
+
+        Returns:
+
+            a+e values that are the causes of inconsistency
+
+        """
+        for i, s1 in enumerate(self.S):
+            for s2 in self.S[i + 1:]:
+                if self.T[s1] == self.T[s2]:
+                    for a in self.A:
+                        if self.T[s1 + a] != self.T[s2 + a]:
+                            for index, e in enumerate(self.E):
+                                if self.T[s1 + a][index] != self.T[s2 + a][index]:
+                                    return [(a + e)]
+
+        return None
+
+    def s_dot_a(self):
+        """
+        Helper generator function that returns extended S, or S.A set.
+        """
+        s_set = set(self.S)
+        for s in self.S:
+            for a in self.A:
+                if s + a not in s_set:
+                    yield s + a
+
+    def update_obs_table(self, s_set: list = None, e_set: list = None):
+        """
+        Perform the membership queries.
+
+        Args:
+
+            s_set: Prefixes of S set on which to preform membership queries. If None, then whole S set will be used.
+
+            e_set: Suffixes of E set on which to perform membership queries. If None, then whole E set will be used.
+
+        Returns:
+
+        """
+
+        update_S = s_set if s_set else list(self.S) + list(self.s_dot_a())
+        update_E = e_set if e_set else self.E
+
+        # This could save few queries
+        update_S.reverse()
+
+        for s in update_S:
+            for e in update_E:
+                if len(self.T[s]) != len(self.E):
+                    output = tuple(self.sul.query(s + e))
+                    if self.prefixes_in_cell and len(e) > 1:
+                        obs_table_entry = tuple([output[-len(e):]],)
+                    else:
+                        obs_table_entry = (output[-1],)
+                    self.T[s] += obs_table_entry
+
+    def gen_hypothesis(self, no_cex_processing_used=False) -> Automaton:
+        """
+        Generate automaton based on the values found in the observation table.
+        :return:
+
+        Args:
+
+            check_for_duplicate_rows:  (Default value = False)
+
+        Returns:
+
+            Automaton of type `automaton_type`
+
+        """
+        state_distinguish = dict()
+        states_dict = dict()
+        initial_state = None
+        automaton_class = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
+
+        s_set = self.S
+        # Added check for the algorithm without counterexample processing
+        if no_cex_processing_used:
+            s_set = self._get_row_representatives()
+
+        # create states based on S set
+        stateCounter = 0
+        for prefix in s_set:
+            state_id = f's{stateCounter}'
+
+            if self.automaton_type == 'dfa':
+                states_dict[prefix] = DfaState(state_id)
+                states_dict[prefix].is_accepting = self.T[prefix][0]
+            elif self.automaton_type == 'moore':
+                states_dict[prefix] = MooreState(state_id, output=self.T[prefix][0])
+            else:
+                states_dict[prefix] = MealyState(state_id)
+
+            states_dict[prefix].prefix = prefix
+            state_distinguish[tuple(self.T[prefix])] = states_dict[prefix]
+
+            if not prefix:
+                initial_state = states_dict[prefix]
+            stateCounter += 1
+
+        # add transitions based on extended S set
+        for prefix in s_set:
+            for a in self.A:
+                state_in_S = state_distinguish[self.T[prefix + a]]
+                states_dict[prefix].transitions[a[0]] = state_in_S
+                if self.automaton_type == 'mealy':
+                    states_dict[prefix].output_fun[a[0]] = self.T[prefix][self.E.index(a)]
+
+        automaton = automaton_class[self.automaton_type](initial_state, list(states_dict.values()))
+        automaton.characterization_set = self.E
+
+        return automaton
+
+    def _get_row_representatives(self):
+        self.S.sort(key=len)
+        representatives = defaultdict(list)
+        for prefix in self.S:
+            representatives[self.T[prefix]].append(prefix)
+
+        return [r[0] for r in representatives.values()]
```

## aalpy/learning_algs/deterministic_passive/GeneralizedStateMerging.py

```diff
@@ -1,150 +1,106 @@
-import queue
-import time
-from typing import Tuple
-
-from aalpy.learning_algs.deterministic_passive.rpni_helper_functions import to_automaton, RpniNode, StateMerging
-from aalpy.utils import save_automaton_to_file
-
-
-class GeneralizedStateMerging:
-    def __init__(self, data, automaton_type, print_info=True):
-        self.data = data
-        self.final_automaton_type = automaton_type
-        self.automaton_type = automaton_type if automaton_type != 'dfa' else 'moore'
-        self.print_info = print_info
-
-        pta_construction_start = time.time()
-        self.merger = StateMerging(data, self.automaton_type)
-        self.log = []
-
-        if self.print_info:
-            print(f'PTA Construction Time: {round(time.time() - pta_construction_start, 2)}')
-
-    def run_rpni(self):
-        start_time = time.time()
-
-        # sorted list of states already considered
-        red_states = [self.merger.root]
-        # used to get the minimal non-red state
-        blue_states = list(red_states[0].children.values())
-
-        while blue_states:
-            blue_state = min(list(blue_states), key=lambda x: len(x.prefix))
-
-            partition = None
-            for red_state in red_states:
-                partition = self._partition_from_merge(red_state, blue_state)
-                if partition is not None:
-                    break
-
-            if partition is None:
-                self.log.append(["promote", (blue_state.prefix,)])
-                red_states.append(blue_state)
-                if self.print_info:
-                    print(f'\rCurrent automaton size: {len(red_states)}', end="")
-            else:
-                self.log.append(["merge", (red_state.prefix, blue_state.prefix)])
-
-                # use the partition for merging
-                for node in partition.keys():
-                    block = partition[node]
-                    # assert RpniNode.compatible(node, block)
-                    node.output = block.output
-                    node.children = block.children
-
-                node = self.merger.root.get_child_by_prefix(blue_state.prefix[:-1])
-                node.children[blue_state.prefix[-1]] = red_state
-                # self.merge(red_state, blue_state)
-
-            blue_states.clear()
-            for r in red_states:
-                for c in r.children.values():
-                    if c not in red_states:
-                        blue_states.append(c)
-
-        if self.print_info:
-            print(f'\nRPNI-GSM Learning Time: {round(time.time() - start_time, 2)}')
-            print(f'RPNI-GSM Learned {len(red_states)} state automaton.')
-
-        return to_automaton(red_states, self.final_automaton_type)
-
-    def merge(self, red_state, blue_state):
-        if not self.merger.merge(red_state, blue_state):
-            print(f"error on command: {self.log[-1]}")
-            v1 = StateMerging.replay_log_on_pta(self.data, self.log[:-1], self.automaton_type)
-            v2 = StateMerging.replay_log_on_pta(self.data, self.log, self.automaton_type)
-            save_automaton_to_file(v1, "pre", "pdf")
-            save_automaton_to_file(v2, "post", "pdf")
-            raise AssertionError(f"error on command: {self.log[-1]}")
-
-    def _compatible_states_future(self, red, blue):
-        """
-        Compatibility check based on futures
-        """
-
-        if self.automaton_type == 'mealy':
-            raise NotImplementedError()
-
-        # this is done by tracking which (red,blue) pairs have been visited
-
-        overwrites = []
-
-        def revert_overrides():
-            for overwrite in overwrites:
-                overwrite.output = None
-
-        q: queue.Queue[Tuple[RpniNode, RpniNode]] = queue.Queue()
-        q.put((red, blue))
-        while not q.empty():
-            red, blue = q.get()
-            if not RpniNode.compatible_outputs(red, blue):
-                revert_overrides()
-                return False
-            if red.output is None and blue.output is not None:
-                red.output = blue.output
-                overwrites.append(red)
-            for symbol in blue.children.keys():
-                if symbol in red.children.keys():
-                    q.put((red.children[symbol], blue.children[symbol]))
-        revert_overrides()
-        return True
-
-    def _partition_from_merge(self, red: RpniNode, blue: RpniNode):
-        """
-        Compatibility check based on partitions
-        """
-
-        partitions = dict()
-        q = queue.Queue()
-        q.put((red, blue))
-
-        while not q.empty():
-            red, blue = q.get()
-
-            def get_partition(node: RpniNode):
-                if node not in partitions:
-                    p = node.shallow_copy()
-                    partitions[node] = p
-                else:
-                    p = partitions[node]
-                return p
-
-            partition = get_partition(red)
-
-            if not RpniNode.compatible_outputs(partition, blue):
-                return None
-            if self.automaton_type == 'moore' and partition.output is None:
-                partition.output = blue.output
-            if self.automaton_type == 'mealy':
-                for key in filter(lambda k: k not in partition.output or partition.output[k] is None, blue.output):
-                    partition.output[key] = blue.output[key]
-
-            partitions[blue] = partition
-
-            for symbol, blue_child in blue.children.items():
-                if symbol in partition.children.keys():
-                    q.put((partition.children[symbol], blue_child))
-                else:
-                    # blue_child is blue after merging if there is a red state in the partition
-                    partition.children[symbol] = blue_child
-        return partitions
+import queue
+import time
+
+from aalpy.learning_algs.deterministic_passive.rpni_helper_functions import to_automaton, RpniNode, createPTA
+
+
+class GeneralizedStateMerging:
+    def __init__(self, data, automaton_type, print_info=True):
+        self.data = data
+        self.final_automaton_type = automaton_type
+        self.automaton_type = automaton_type if automaton_type != 'dfa' else 'moore'
+        self.print_info = print_info
+
+        pta_construction_start = time.time()
+        self.root = createPTA(data, self.automaton_type)
+        self.log = []
+
+        if self.print_info:
+            print(f'PTA Construction Time: {round(time.time() - pta_construction_start, 2)}')
+
+    def run_rpni(self):
+        start_time = time.time()
+
+        # sorted list of states already considered
+        red_states = [self.root]
+        # used to get the minimal non-red state
+        blue_states = list(red_states[0].children.values())
+
+        while blue_states:
+            blue_state = min(list(blue_states))
+
+            partition = None
+            for red_state in red_states:
+                partition = self._partition_from_merge(red_state, blue_state)
+                if partition is not None:
+                    break
+
+            if partition is None:
+                self.log.append(["promote", (blue_state.prefix,)])
+                red_states.append(blue_state)
+                if self.print_info:
+                    print(f'\rCurrent automaton size: {len(red_states)}', end="")
+            else:
+                self.log.append(["merge", (red_state.prefix, blue_state.prefix)])
+
+                # use the partition for merging
+                for node in partition.keys():
+                    block = partition[node]
+                    # assert RpniNode.compatible(node, block)
+                    node.output = block.output
+                    node.children = block.children
+
+                node = self.root.get_child_by_prefix(blue_state.prefix[:-1])
+                node.children[blue_state.prefix[-1]] = red_state
+
+            blue_states.clear()
+            for r in red_states:
+                for c in r.children.values():
+                    if c not in red_states:
+                        blue_states.append(c)
+
+        if self.print_info:
+            print(f'\nRPNI-GSM Learning Time: {round(time.time() - start_time, 2)}')
+            print(f'RPNI-GSM Learned {len(red_states)} state automaton.')
+
+        return to_automaton(red_states, self.final_automaton_type)
+
+    def _partition_from_merge(self, red: RpniNode, blue: RpniNode):
+        """
+        Compatibility check based on partitions
+        """
+
+        partitions = dict()
+        q = queue.Queue()
+        q.put((red, blue))
+
+        while not q.empty():
+            red, blue = q.get()
+
+            def get_partition(node: RpniNode):
+                if node not in partitions:
+                    p = node.shallow_copy()
+                    partitions[node] = p
+                else:
+                    p = partitions[node]
+                return p
+
+            partition = get_partition(red)
+
+            if not RpniNode.compatible_outputs(partition, blue):
+                return None
+            if self.automaton_type == 'moore' and partition.output is None:
+                partition.output = blue.output
+            if self.automaton_type == 'mealy':
+                for key in filter(lambda k: k not in partition.output or partition.output[k] is None, blue.output):
+                    partition.output[key] = blue.output[key]
+
+            partitions[blue] = partition
+
+            for symbol, blue_child in blue.children.items():
+                if symbol in partition.children.keys():
+                    q.put((partition.children[symbol], blue_child))
+                else:
+                    # blue_child is blue after merging if there is a red state in the partition
+                    partition.children[symbol] = blue_child
+        return partitions
```

## aalpy/learning_algs/deterministic_passive/RPNI.py

```diff
@@ -1,188 +1,188 @@
-import time
-from bisect import insort
-from typing import Union
-
-from aalpy.base import DeterministicAutomaton
-from aalpy.learning_algs.deterministic_passive.GeneralizedStateMerging import GeneralizedStateMerging
-from aalpy.learning_algs.deterministic_passive.rpni_helper_functions import to_automaton, createPTA, \
-    check_sequence, extract_unique_sequences
-
-
-class RPNI:
-    def __init__(self, data, automaton_type, print_info=True):
-        self.data = data
-        self.automaton_type = automaton_type
-        self.print_info = print_info
-
-        pta_construction_start = time.time()
-        self.root_node = createPTA(data, automaton_type)
-        self.test_data = extract_unique_sequences(self.root_node)
-
-        if self.print_info:
-            print(f'PTA Construction Time: {round(time.time() - pta_construction_start, 2)}')
-
-    def run_rpni(self):
-        start_time = time.time()
-
-        red = [self.root_node]
-        blue = list(red[0].children.values())
-        while blue:
-            lex_min_blue = min(list(blue), key=lambda x: len(x.prefix))
-            merged = False
-
-            for red_state in red:
-                if not self._compatible_states(red_state, lex_min_blue):
-                    continue
-                merge_candidate = self._merge(red_state, lex_min_blue, copy_nodes=True)
-                if self._compatible(merge_candidate):
-                    self._merge(red_state, lex_min_blue)
-                    merged = True
-                    break
-
-            if not merged:
-                insort(red, lex_min_blue)
-                if self.print_info:
-                    print(f'\rCurrent automaton size: {len(red)}', end="")
-
-            blue.clear()
-            for r in red:
-                for c in r.children.values():
-                    if c not in red:
-                        blue.append(c)
-
-        if self.print_info:
-            print(f'\nRPNI Learning Time: {round(time.time() - start_time, 2)}')
-            print(f'RPNI Learned {len(red)} state automaton.')
-
-        assert sorted(red, key=lambda x: len(x.prefix)) == red
-        return to_automaton(red, self.automaton_type)
-
-    def _compatible(self, root_node):
-        """
-        Check if current model is compatible with the data.
-        """
-        for sequence in self.test_data:
-            if not check_sequence(root_node, sequence, automaton_type=self.automaton_type):
-                return False
-        return True
-
-    def _compatible_states(self, red_node, blue_node):
-        """
-        Only allow merging of states that have same output(s).
-        """
-        if self.automaton_type != 'mealy':
-            # None is compatible with everything
-            return red_node.output == blue_node.output or red_node.output is None or blue_node.output is None
-        else:
-            red_io = {i: o for i, o in red_node.children.keys()}
-            blue_io = {i: o for i, o in blue_node.children.keys()}
-            for common_i in set(red_io.keys()).intersection(blue_io.keys()):
-                if red_io[common_i] != blue_io[common_i]:
-                    return False
-        return True
-
-    def _merge(self, red_node, lex_min_blue, copy_nodes=False):
-        """
-        Merge two states and return the root node of resulting model.
-        """
-        root_node = self.root_node.copy() if copy_nodes else self.root_node
-        lex_min_blue = lex_min_blue.copy() if copy_nodes else lex_min_blue
-
-        red_node_in_tree = root_node
-        for p in red_node.prefix:
-            red_node_in_tree = red_node_in_tree.children[p]
-
-        to_update = root_node
-        for p in lex_min_blue.prefix[:-1]:
-            to_update = to_update.children[p]
-
-        to_update.children[lex_min_blue.prefix[-1]] = red_node_in_tree
-
-        if self.automaton_type != 'mealy':
-            self._fold(red_node_in_tree, lex_min_blue)
-        else:
-            self._fold_mealy(red_node_in_tree, lex_min_blue)
-
-        return root_node
-
-    def _fold(self, red_node, blue_node):
-        # Change the output of red only to concrete output, ignore None
-        red_node.output = blue_node.output if blue_node.output is not None else red_node.output
-
-        for i in blue_node.children.keys():
-            if i in red_node.children.keys():
-                self._fold(red_node.children[i], blue_node.children[i])
-            else:
-                red_node.children[i] = blue_node.children[i]
-
-    def _fold_mealy(self, red_node, blue_node):
-        blue_io_map = {i: o for i, o in blue_node.children.keys()}
-
-        updated_keys = {}
-        for io, val in red_node.children.items():
-            o = blue_io_map[io[0]] if io[0] in blue_io_map.keys() else io[1]
-            updated_keys[(io[0], o)] = val
-
-        red_node.children = updated_keys
-
-        for io in blue_node.children.keys():
-            if io in red_node.children.keys():
-                self._fold_mealy(red_node.children[io], blue_node.children[io])
-            else:
-                red_node.children[io] = blue_node.children[io]
-
-
-def run_RPNI(data, automaton_type, algorithm='gsm',
-             input_completeness=None, print_info=True) -> Union[DeterministicAutomaton, None]:
-    """
-    Run RPNI, a deterministic passive model learning algorithm.
-    Resulting model conforms to the provided data.
-    For more information on RPNI, check out AALpy' Wiki:
-    https://github.com/DES-Lab/AALpy/wiki/RPNI---Passive-Deterministic-Automata-Learning
-
-    Args:
-
-        data: sequence of input sequences and corresponding label. Eg. [[(i1,i2,i3, ...), label], ...]
-        automaton_type: either 'dfa', 'mealy', 'moore'. Note that for 'mealy' machine learning, data has to be prefix-closed.
-        algorithm: either 'gsm' (generalized state merging) or 'classic' for base RPNI implementation. GSM is much faster and less resource intensive.
-        input_completeness: either None, 'sink_state', or 'self_loop'. If None, learned model could be input incomplete,
-        sink_state will lead all undefined inputs form some state to the sink state, whereas self_loop will simply create
-        a self loop. In case of Mealy learning output of the added transition will be 'epsilon'.
-        print_info: print learning progress and runtime information
-
-    Returns:
-
-        Model conforming to the data, or None if data is non-deterministic.
-    """
-    assert algorithm in {'gsm', 'classic'}
-    assert automaton_type in {'dfa', 'mealy', 'moore'}
-    assert input_completeness in {None, 'self_loop', 'sink_state'}
-
-    if algorithm == 'classic':
-        rpni = RPNI(data, automaton_type, print_info)
-
-        if rpni.root_node is None:
-            print('Data provided to RPNI is not deterministic. Ensure that the data is deterministic, '
-                  'or consider using Alergia.')
-            return None
-    else:
-        rpni = GeneralizedStateMerging(data, automaton_type, print_info)
-
-        if rpni.merger.root is None:
-            print('Data provided to RPNI is not deterministic. Ensure that the data is deterministic, '
-                  'or consider using Alergia.')
-            return None
-
-    learned_model = rpni.run_rpni()
-
-    if not learned_model.is_input_complete():
-        if not input_completeness:
-            if print_info:
-                print('Warning: Learned Model is not input complete (inputs not defined for all states). '
-                      'Consider calling .make_input_complete()')
-        else:
-            if print_info:
-                print(f'Learned model was not input complete. Adapting it with {input_completeness} transitions.')
-            learned_model.make_input_complete(input_completeness)
-
-    return learned_model
+import time
+from bisect import insort
+from typing import Union
+
+from aalpy.base import DeterministicAutomaton
+from aalpy.learning_algs.deterministic_passive.GeneralizedStateMerging import GeneralizedStateMerging
+from aalpy.learning_algs.deterministic_passive.rpni_helper_functions import to_automaton, createPTA, \
+    check_sequence, extract_unique_sequences
+
+
+class RPNI:
+    def __init__(self, data, automaton_type, print_info=True):
+        self.data = data
+        self.automaton_type = automaton_type
+        self.print_info = print_info
+
+        pta_construction_start = time.time()
+        self.root_node = createPTA(data, automaton_type)
+        self.test_data = extract_unique_sequences(self.root_node)
+
+        if self.print_info:
+            print(f'PTA Construction Time: {round(time.time() - pta_construction_start, 2)}')
+
+    def run_rpni(self):
+        start_time = time.time()
+
+        red = [self.root_node]
+        blue = list(red[0].children.values())
+        while blue:
+            lex_min_blue = min(list(blue))
+            merged = False
+
+            for red_state in red:
+                if not self._compatible_states(red_state, lex_min_blue):
+                    continue
+                merge_candidate = self._merge(red_state, lex_min_blue, copy_nodes=True)
+                if self._compatible(merge_candidate):
+                    self._merge(red_state, lex_min_blue)
+                    merged = True
+                    break
+
+            if not merged:
+                insort(red, lex_min_blue)
+                if self.print_info:
+                    print(f'\rCurrent automaton size: {len(red)}', end="")
+
+            blue.clear()
+            for r in red:
+                for c in r.children.values():
+                    if c not in red:
+                        blue.append(c)
+
+        if self.print_info:
+            print(f'\nRPNI Learning Time: {round(time.time() - start_time, 2)}')
+            print(f'RPNI Learned {len(red)} state automaton.')
+
+        assert sorted(red, key=lambda x: len(x.prefix)) == red
+        return to_automaton(red, self.automaton_type)
+
+    def _compatible(self, root_node):
+        """
+        Check if current model is compatible with the data.
+        """
+        for sequence in self.test_data:
+            if not check_sequence(root_node, sequence, automaton_type=self.automaton_type):
+                return False
+        return True
+
+    def _compatible_states(self, red_node, blue_node):
+        """
+        Only allow merging of states that have same output(s).
+        """
+        if self.automaton_type != 'mealy':
+            # None is compatible with everything
+            return red_node.output == blue_node.output or red_node.output is None or blue_node.output is None
+        else:
+            red_io = {i: o for i, o in red_node.children.keys()}
+            blue_io = {i: o for i, o in blue_node.children.keys()}
+            for common_i in set(red_io.keys()).intersection(blue_io.keys()):
+                if red_io[common_i] != blue_io[common_i]:
+                    return False
+        return True
+
+    def _merge(self, red_node, lex_min_blue, copy_nodes=False):
+        """
+        Merge two states and return the root node of resulting model.
+        """
+        root_node = self.root_node.copy() if copy_nodes else self.root_node
+        lex_min_blue = lex_min_blue.copy() if copy_nodes else lex_min_blue
+
+        red_node_in_tree = root_node
+        for p in red_node.prefix:
+            red_node_in_tree = red_node_in_tree.children[p]
+
+        to_update = root_node
+        for p in lex_min_blue.prefix[:-1]:
+            to_update = to_update.children[p]
+
+        to_update.children[lex_min_blue.prefix[-1]] = red_node_in_tree
+
+        if self.automaton_type != 'mealy':
+            self._fold(red_node_in_tree, lex_min_blue)
+        else:
+            self._fold_mealy(red_node_in_tree, lex_min_blue)
+
+        return root_node
+
+    def _fold(self, red_node, blue_node):
+        # Change the output of red only to concrete output, ignore None
+        red_node.output = blue_node.output if blue_node.output is not None else red_node.output
+
+        for i in blue_node.children.keys():
+            if i in red_node.children.keys():
+                self._fold(red_node.children[i], blue_node.children[i])
+            else:
+                red_node.children[i] = blue_node.children[i]
+
+    def _fold_mealy(self, red_node, blue_node):
+        blue_io_map = {i: o for i, o in blue_node.children.keys()}
+
+        updated_keys = {}
+        for io, val in red_node.children.items():
+            o = blue_io_map[io[0]] if io[0] in blue_io_map.keys() else io[1]
+            updated_keys[(io[0], o)] = val
+
+        red_node.children = updated_keys
+
+        for io in blue_node.children.keys():
+            if io in red_node.children.keys():
+                self._fold_mealy(red_node.children[io], blue_node.children[io])
+            else:
+                red_node.children[io] = blue_node.children[io]
+
+
+def run_RPNI(data, automaton_type, algorithm='gsm',
+             input_completeness=None, print_info=True) -> Union[DeterministicAutomaton, None]:
+    """
+    Run RPNI, a deterministic passive model learning algorithm.
+    Resulting model conforms to the provided data.
+    For more information on RPNI, check out AALpy' Wiki:
+    https://github.com/DES-Lab/AALpy/wiki/RPNI---Passive-Deterministic-Automata-Learning
+
+    Args:
+
+        data: sequence of input sequences and corresponding label. Eg. [[(i1,i2,i3, ...), label], ...]
+        automaton_type: either 'dfa', 'mealy', 'moore'. Note that for 'mealy' machine learning, data has to be prefix-closed.
+        algorithm: either 'gsm' (generalized state merging) or 'classic' for base RPNI implementation. GSM is much faster and less resource intensive.
+        input_completeness: either None, 'sink_state', or 'self_loop'. If None, learned model could be input incomplete,
+        sink_state will lead all undefined inputs form some state to the sink state, whereas self_loop will simply create
+        a self loop. In case of Mealy learning output of the added transition will be 'epsilon'.
+        print_info: print learning progress and runtime information
+
+    Returns:
+
+        Model conforming to the data, or None if data is non-deterministic.
+    """
+    assert algorithm in {'gsm', 'classic'}
+    assert automaton_type in {'dfa', 'mealy', 'moore'}
+    assert input_completeness in {None, 'self_loop', 'sink_state'}
+
+    if algorithm == 'classic':
+        rpni = RPNI(data, automaton_type, print_info)
+
+        if rpni.root_node is None:
+            print('Data provided to RPNI is not deterministic. Ensure that the data is deterministic, '
+                  'or consider using Alergia.')
+            return None
+    else:
+        rpni = GeneralizedStateMerging(data, automaton_type, print_info)
+
+        if rpni.root is None:
+            print('Data provided to RPNI is not deterministic. Ensure that the data is deterministic, '
+                  'or consider using Alergia.')
+            return None
+
+    learned_model = rpni.run_rpni()
+
+    if not learned_model.is_input_complete():
+        if not input_completeness:
+            if print_info:
+                print('Warning: Learned Model is not input complete (inputs not defined for all states). '
+                      'Consider calling .make_input_complete()')
+        else:
+            if print_info:
+                print(f'Learned model was not input complete. Adapting it with {input_completeness} transitions.')
+            learned_model.make_input_complete(input_completeness)
+
+    return learned_model
```

## aalpy/learning_algs/deterministic_passive/active_RPNI.py

```diff
@@ -1,113 +1,62 @@
-import random
-import time
-
-from aalpy.SULs import DfaSUL
-from aalpy.base.SUL import CacheSUL
-from aalpy.learning_algs import run_RPNI, run_Lstar
-from aalpy.oracles import RandomWalkEqOracle
-from aalpy.utils import convert_i_o_traces_for_RPNI, get_Angluin_dfa, generate_random_deterministic_automata
-from aalpy.utils.HelperFunctions import print_learning_info
-from aalpy.learning_algs.deterministic.CounterExampleProcessing import rs_cex_processing
-
-
-def ensure_input_completeness(alphabet, hypothesis, input_completeness_method):
-    hypothesis_alphabet = hypothesis.get_input_alphabet()
-    if set(alphabet) == set(hypothesis_alphabet):
-        return hypothesis
-
-    for i in alphabet:
-        if i not in hypothesis_alphabet:
-            hypothesis.initial_state.transitions[i] = hypothesis.initial_state
-
-    hypothesis.make_input_complete(input_completeness_method)
-    return hypothesis
-
-
-def extend_data_set(inputs, outputs, data):
-    io_trace = [[(i, o) for i, o in zip(inputs, outputs)]]
-    data.extend(convert_i_o_traces_for_RPNI(io_trace))
-
-
-def run_active_RPNI(alphabet, sul, eq_oracle, automaton_type, num_random_samples, print_level=2):
-    data = []
-
-    start_time = time.time()
-    eq_query_time = 0
-
-    char_set = [(a,) for a in alphabet]
-
-    sul = CacheSUL(sul)
-    eq_oracle.sul = sul
-
-    for a in char_set:
-        outputs = sul.query(a)
-        extend_data_set(a, outputs, data)
-
-    learning_rounds = 0
-    while True:
-        learning_rounds += 1
-        hypothesis = run_RPNI(data, automaton_type, input_completeness='self_loop', print_info=False)
-
-        for state in hypothesis.states:
-            state.prefix = hypothesis.get_shortest_path(hypothesis.initial_state, state)
-        # hypothesis = ensure_input_completeness(alphabet, hypothesis, 'self_loop')
-
-        if print_level > 1:
-            print(f'Learning round {learning_rounds}: {hypothesis.size} states.')
-
-        eq_query_start = time.time()
-        cex = eq_oracle.find_cex(hypothesis)
-        eq_query_time += time.time() - eq_query_start
-
-        if cex is None:
-            break
-
-        dist_suffix = rs_cex_processing(sul, cex, hypothesis, suffix_closedness=False)[0]
-        char_set.append(dist_suffix)
-
-        cex_output = sul.query(cex)
-        extend_data_set(cex, cex_output, data)
-
-        for state in hypothesis.states:
-            for suffix in char_set:
-                inputs = state.prefix + suffix
-                outputs = sul.query(inputs)
-                extend_data_set(inputs, outputs, data)
-
-        if print_level == 3:
-            print('Counterexample', cex)
-
-    total_time = round(time.time() - start_time, 2)
-    eq_query_time = round(eq_query_time, 2)
-    learning_time = round(total_time - eq_query_time, 2)
-
-    info = {
-        'learning_rounds': learning_rounds,
-        'automaton_size': hypothesis.size,
-        'queries_learning': sul.num_queries,
-        'steps_learning': sul.num_steps,
-        'queries_eq_oracle': eq_oracle.num_queries,
-        'steps_eq_oracle': eq_oracle.num_steps,
-        'learning_time': learning_time,
-        'eq_oracle_time': eq_query_time,
-        'total_time': total_time,
-    }
-
-    if print_level > 0:
-        print_learning_info(info)
-
-    return hypothesis
-
-
-model = generate_random_deterministic_automata('dfa', 5, input_alphabet_size=4, output_alphabet_size=2)
-sul = DfaSUL(model)
-alphabet = model.get_input_alphabet()
-eq_oracle = RandomWalkEqOracle(alphabet, sul, )
-
-print('Active RPNI')
-learned_model = run_active_RPNI(alphabet, sul, eq_oracle, 'dfa', num_random_samples=1)
-
-print('L*')
-eq_oracle = RandomWalkEqOracle(alphabet, sul, )
-learned_model = run_Lstar(alphabet, sul, eq_oracle, 'dfa',)
-
+from abc import ABC, abstractmethod
+from random import randint, choice
+
+from aalpy.learning_algs import run_RPNI
+from aalpy.utils import convert_i_o_traces_for_RPNI
+
+
+class RpniActiveSampler(ABC):
+    """
+    Abstract class whose implementations are used to provide samples for active passive learning.
+    """
+
+    @abstractmethod
+    def sample(self, sul, model):
+        """
+        Abstract method implementing sampling strategy.
+
+        Args:
+
+            sul: system under learning
+            model: current learned model
+
+        Returns:
+
+            Data to be added to the data set for the passive RPNI learning in its data-format.
+
+        """
+        pass
+
+
+class RandomWordSampler(RpniActiveSampler):
+    def __init__(self, num_walks, min_walk_len, max_walk_len):
+        self.num_walks = num_walks
+        self.min_walk_len = min_walk_len
+        self.max_walk_len = max_walk_len
+
+    def sample(self, sul, model):
+        input_al = list({el for s in model.states for el in s.transitions.keys()})
+        samples = []
+
+        for _ in range(self.num_walks):
+            walk_len = randint(self.min_walk_len, self.max_walk_len)
+            random_walk = tuple(choice(input_al) for _ in range(walk_len))
+
+            outputs = sul.query(random_walk)
+            samples.append(list(zip(random_walk, outputs)))
+
+        samples = convert_i_o_traces_for_RPNI(samples)
+        return samples
+
+
+def run_active_RPNI(data, sul, sampler, n_iter, automaton_type, print_info=True):
+    model = None
+    for i in range(n_iter):
+        if print_info:
+            print(f'-------------Active RPNI Iteration: {i}-------------')
+        model = run_RPNI(data, automaton_type=automaton_type, print_info=print_info)
+
+        new_samples = sampler.sample(sul, model)
+        data.extend(new_samples)
+
+    return model
```

## aalpy/learning_algs/deterministic_passive/rpni_helper_functions.py

```diff
@@ -1,266 +1,265 @@
-import pickle
-import queue
-from typing import Set
-
-
-class RpniNode:
-    __slots__ = ['output', 'children', 'prefix', "type"]
-
-    def __init__(self, output=None, children=None, automaton_type='moore'):
-        if output is None and automaton_type == 'mealy':
-            output = dict()
-        if children is None:
-            children = dict()
-        self.output = output
-        self.children = children
-        self.prefix = ()
-        self.type = automaton_type
-
-    def shallow_copy(self):
-        output = self.output if self.type != 'mealy' else dict(self.output)
-        return RpniNode(output, dict(self.children), self.type)
-
-    def copy(self):
-        return pickle.loads(pickle.dumps(self, -1))
-
-    def __lt__(self, other):
-        return len(self.prefix) < len(other.prefix)
-
-    def __le__(self, other):
-        return len(self.prefix) <= len(other.prefix)
-
-    def __eq__(self, other):
-        return self.prefix == other.prefix
-
-    def __hash__(self):
-        return id(self)  # TODO This is a hack
-
-    def get_all_nodes(self) -> Set['RpniNode']:
-        qu = queue.Queue()
-        qu.put(self)
-        nodes = set()
-        while not qu.empty():
-            state = qu.get()
-            nodes.add(state)
-            for child in state.children.values():
-                if child not in nodes:
-                    qu.put(child)
-        return nodes
-
-    def to_automaton(self):
-        nodes = self.get_all_nodes()
-        nodes.remove(self)  # dunno whether order is preserved?
-        nodes = [self] + list(nodes)
-        return to_automaton(nodes, self.type)
-
-    def compatible_outputs(self, other):
-        so, oo = [self.output, other.output]
-        cmp = lambda x, y: x is None or y is None or x == y
-        if self.type == 'moore':
-            return cmp(so, oo)
-        else:
-            return all(cmp(so[key], oo[key]) for key in filter(lambda k: k in oo, so))
-
-    def get_child_by_prefix(self, prefix):
-        node = self
-        for symbol in prefix:
-            node = node.children[symbol]
-        return node
-
-
-class StateMerging:
-    def __init__(self, data, automaton_type, print_info=True):
-        self.data = data
-        self.automaton_type = automaton_type
-        self.print_info = print_info
-
-        self.root = createPTA(data, automaton_type)
-        self.merges = []
-
-    def merge(self, red_node, lex_min_blue, copy_nodes=False):
-        """
-        Merge two states and return the root node of resulting model.
-        """
-
-        if self.automaton_type == 'mealy':
-            raise NotImplementedError()
-
-        if not copy_nodes:
-            self.merges.append((red_node, lex_min_blue))
-
-        root_node = self.root.copy() if copy_nodes else self.root
-        lex_min_blue = lex_min_blue.copy() if copy_nodes else lex_min_blue
-
-        red_node_in_tree = root_node
-        for p in red_node.prefix:
-            red_node_in_tree = red_node_in_tree.children[p]
-
-        to_update = root_node
-        for p in lex_min_blue.prefix[:-1]:
-            to_update = to_update.children[p]
-
-        to_update.children[lex_min_blue.prefix[-1]] = red_node_in_tree
-
-        if not self._fold(red_node_in_tree, lex_min_blue, not copy_nodes):
-            return None
-
-        return root_node
-
-    def _fold(self, red_node, blue_node, report):
-        # Change the output of red only to concrete output, ignore None
-        if report and not RpniNode.compatible_outputs(red_node, blue_node):
-            print(f"conflict {red_node.prefix} ({red_node.output}) {blue_node.prefix} ({blue_node.output})")
-            return False
-        red_node.output = blue_node.output if blue_node.output is not None else red_node.output
-
-        for i in blue_node.children.keys():
-            if i in red_node.children.keys():
-                self._fold(red_node.children[i], blue_node.children[i], report)
-            else:
-                red_node.children[i] = blue_node.children[i]
-        return True
-
-    def to_automaton(self):
-        return self.root.to_automaton()
-
-    def replay_log(self, commands: list):
-        for command, args in commands:
-            if command == "merge":
-                self.merge(self.root.get_child_by_prefix(args[0]), self.root.get_child_by_prefix(args[1]))
-            elif command == "promote":
-                pass
-
-    @staticmethod
-    def replay_log_on_pta(data, commands: list, automaton_type):
-        sm = StateMerging(data, automaton_type)
-        sm.replay_log(commands)
-        return sm.to_automaton()
-
-
-def check_sequence(root_node, seq, automaton_type):
-    """
-    Checks whether each sequence in the dataset is valid in the current automaton.
-    """
-    curr_node = root_node
-    for i, o in seq:
-        if automaton_type == 'mealy':
-            input_outputs = {i: o for i, o in curr_node.children.keys()}
-            if i[0] not in input_outputs.keys() or o is not None and input_outputs[i[0]] != o:
-                return False
-            curr_node = curr_node.children[(i[0], input_outputs[i[0]])]
-        else:
-            # For dfa and moore, check if outputs are the same, iff output in test data is concrete (not None)
-            curr_node = curr_node.children[i]
-            if o is not None and curr_node.output != o:
-                return False
-    return True
-
-
-def createPTA(data, automaton_type):
-    data.sort(key=lambda x: len(x[0]))
-
-    root_node = RpniNode(automaton_type=automaton_type)
-    for seq, label in data:
-        curr_node = root_node
-        for idx, symbol in enumerate(seq):
-            if symbol not in curr_node.children.keys():
-                node = RpniNode(automaton_type=automaton_type)
-                node.prefix = curr_node.prefix + (symbol,)
-                curr_node.children[symbol] = node
-
-            if automaton_type == 'mealy' and idx == len(seq) - 1:
-                if symbol not in curr_node.output:
-                    curr_node.output[symbol] = label
-                if curr_node.output[symbol] != label:
-                    return None
-            curr_node = curr_node.children[symbol]
-        if automaton_type == 'moore' or automaton_type == 'dfa':
-            if curr_node.output is None:
-                curr_node.output = label
-            if curr_node.output != label:
-                return None
-
-    return root_node
-
-
-def extract_unique_sequences(root_node):
-    def get_leaf_nodes(root):
-        leaves = []
-
-        def _get_leaf_nodes(node):
-            if node is not None:
-                if len(node.children.keys()) == 0:
-                    leaves.append(node)
-                for n in node.children.values():
-                    _get_leaf_nodes(n)
-
-        _get_leaf_nodes(root)
-        return leaves
-
-    leaf_nodes = get_leaf_nodes(root_node)
-    paths = []
-    for node in leaf_nodes:
-        seq = []
-        curr_node = root_node
-        for i in node.prefix:
-            curr_node = curr_node.children[i]
-            seq.append((i, curr_node.output))
-        paths.append(seq)
-
-    return paths
-
-
-def to_automaton(red, automaton_type):
-    from aalpy.automata import DfaState, Dfa, MooreMachine, MooreState, MealyMachine, MealyState
-
-    if automaton_type == 'dfa':
-        state, automaton = DfaState, Dfa
-    elif automaton_type == 'moore':
-        state, automaton = MooreState, MooreMachine
-    else:
-        state, automaton = MealyState, MealyMachine
-
-    initial_state = None
-    prefix_state_map = {}
-    for i, r in enumerate(red):
-        if automaton_type == 'moore' or automaton_type == 'dfa':
-            prefix_state_map[r.prefix] = state(f's{i}', r.output)
-        else:
-            prefix_state_map[r.prefix] = state(f's{i}')
-        if i == 0:
-            initial_state = prefix_state_map[r.prefix]
-
-    for r in red:
-        for i, c in r.children.items():
-            if automaton_type == 'moore' or automaton_type == 'dfa':
-                prefix_state_map[r.prefix].transitions[i] = prefix_state_map[c.prefix]
-            else:
-                prefix_state_map[r.prefix].transitions[i] = prefix_state_map[c.prefix]
-                prefix_state_map[r.prefix].output_fun[i] = r.output[i] if i in r.output else None
-
-    return automaton(initial_state, list(prefix_state_map.values()))
-
-
-def visualize_pta(root_node, path='pta.pdf'):
-    from pydot import Dot, Node, Edge
-    graph = Dot('fpta', graph_type='digraph')
-
-    graph.add_node(Node(str(root_node.prefix), label=f'{root_node.output}'))
-
-    queue = [root_node]
-    visited = set()
-    visited.add(root_node.prefix)
-    while queue:
-        curr = queue.pop(0)
-        for i, c in curr.children.items():
-            if c.prefix not in visited:
-                graph.add_node(Node(str(c.prefix), label=f'{c.output}'))
-            graph.add_edge(Edge(str(curr.prefix), str(c.prefix), label=f'{i}'))
-            if c.prefix not in visited:
-                queue.append(c)
-            visited.add(c.prefix)
-
-    graph.add_node(Node('__start0', shape='none', label=''))
-    graph.add_edge(Edge('__start0', str(root_node.prefix), label=''))
-
-    graph.write(path=path, format='pdf')
+import pickle
+import queue
+from functools import total_ordering
+from typing import Set
+
+
+@total_ordering
+class RpniNode:
+    __slots__ = ['output', 'children', 'prefix', "type"]
+
+    def __init__(self, output=None, children=None, automaton_type='moore'):
+        if output is None and automaton_type == 'mealy':
+            output = dict()
+        if children is None:
+            children = dict()
+        self.output = output
+        self.children = children
+        self.prefix = ()
+        self.type = automaton_type
+
+    def shallow_copy(self):
+        output = self.output if self.type != 'mealy' else dict(self.output)
+        return RpniNode(output, dict(self.children), self.type)
+
+    def copy(self):
+        return pickle.loads(pickle.dumps(self, -1))
+
+    def __lt__(self, other):
+        return (len(self.prefix), self.prefix) < (len(other.prefix), other.prefix)
+
+    def __eq__(self, other):
+        return self.prefix == other.prefix
+
+    def __hash__(self):
+        return id(self)  # TODO This is a hack
+
+    def get_all_nodes(self) -> Set['RpniNode']:
+        qu = queue.Queue()
+        qu.put(self)
+        nodes = set()
+        while not qu.empty():
+            state = qu.get()
+            nodes.add(state)
+            for child in state.children.values():
+                if child not in nodes:
+                    qu.put(child)
+        return nodes
+
+    def to_automaton(self):
+        nodes = self.get_all_nodes()
+        nodes.remove(self)  # dunno whether order is preserved?
+        nodes = [self] + list(nodes)
+        return to_automaton(nodes, self.type)
+
+    def compatible_outputs(self, other):
+        so, oo = [self.output, other.output]
+        cmp = lambda x, y: x is None or y is None or x == y
+        if self.type == 'moore':
+            return cmp(so, oo)
+        else:
+            return all(cmp(so[key], oo[key]) for key in filter(lambda k: k in oo, so))
+
+    def get_child_by_prefix(self, prefix):
+        node = self
+        for symbol in prefix:
+            node = node.children[symbol]
+        return node
+
+
+class StateMerging:
+    def __init__(self, data, automaton_type, print_info=True):
+        self.data = data
+        self.automaton_type = automaton_type
+        self.print_info = print_info
+
+        self.root = createPTA(data, automaton_type)
+        self.merges = []
+
+    def merge(self, red_node, lex_min_blue, copy_nodes=False):
+        """
+        Merge two states and return the root node of resulting model.
+        """
+
+        if self.automaton_type == 'mealy':
+            raise NotImplementedError()
+
+        if not copy_nodes:
+            self.merges.append((red_node, lex_min_blue))
+
+        root_node = self.root.copy() if copy_nodes else self.root
+        lex_min_blue = lex_min_blue.copy() if copy_nodes else lex_min_blue
+
+        red_node_in_tree = root_node
+        for p in red_node.prefix:
+            red_node_in_tree = red_node_in_tree.children[p]
+
+        to_update = root_node
+        for p in lex_min_blue.prefix[:-1]:
+            to_update = to_update.children[p]
+
+        to_update.children[lex_min_blue.prefix[-1]] = red_node_in_tree
+
+        if not self._fold(red_node_in_tree, lex_min_blue, not copy_nodes):
+            return None
+
+        return root_node
+
+    def _fold(self, red_node, blue_node, report):
+        # Change the output of red only to concrete output, ignore None
+        if report and not RpniNode.compatible_outputs(red_node, blue_node):
+            print(f"conflict {red_node.prefix} ({red_node.output}) {blue_node.prefix} ({blue_node.output})")
+            return False
+        red_node.output = blue_node.output if blue_node.output is not None else red_node.output
+
+        for i in blue_node.children.keys():
+            if i in red_node.children.keys():
+                self._fold(red_node.children[i], blue_node.children[i], report)
+            else:
+                red_node.children[i] = blue_node.children[i]
+        return True
+
+    def to_automaton(self):
+        return self.root.to_automaton()
+
+    def replay_log(self, commands: list):
+        for command, args in commands:
+            if command == "merge":
+                self.merge(self.root.get_child_by_prefix(args[0]), self.root.get_child_by_prefix(args[1]))
+            elif command == "promote":
+                pass
+
+    @staticmethod
+    def replay_log_on_pta(data, commands: list, automaton_type):
+        sm = StateMerging(data, automaton_type)
+        sm.replay_log(commands)
+        return sm.to_automaton()
+
+
+def check_sequence(root_node, seq, automaton_type):
+    """
+    Checks whether each sequence in the dataset is valid in the current automaton.
+    """
+    curr_node = root_node
+    for i, o in seq:
+        if automaton_type == 'mealy':
+            input_outputs = {i: o for i, o in curr_node.children.keys()}
+            if i[0] not in input_outputs.keys() or o is not None and input_outputs[i[0]] != o:
+                return False
+            curr_node = curr_node.children[(i[0], input_outputs[i[0]])]
+        else:
+            # For dfa and moore, check if outputs are the same, iff output in test data is concrete (not None)
+            curr_node = curr_node.children[i]
+            if o is not None and curr_node.output != o:
+                return False
+    return True
+
+
+def createPTA(data, automaton_type):
+    data.sort(key=lambda x: len(x[0]))
+
+    root_node = RpniNode(automaton_type=automaton_type)
+    for seq, label in data:
+        curr_node = root_node
+        for idx, symbol in enumerate(seq):
+            if symbol not in curr_node.children.keys():
+                node = RpniNode(automaton_type=automaton_type)
+                node.prefix = curr_node.prefix + (symbol,)
+                curr_node.children[symbol] = node
+
+            if automaton_type == 'mealy' and idx == len(seq) - 1:
+                if symbol not in curr_node.output:
+                    curr_node.output[symbol] = label
+                if curr_node.output[symbol] != label:
+                    return None
+            curr_node = curr_node.children[symbol]
+        if automaton_type == 'moore' or automaton_type == 'dfa':
+            if curr_node.output is None:
+                curr_node.output = label
+            if curr_node.output != label:
+                return None
+
+    return root_node
+
+
+def extract_unique_sequences(root_node):
+    def get_leaf_nodes(root):
+        leaves = []
+
+        def _get_leaf_nodes(node):
+            if node is not None:
+                if len(node.children.keys()) == 0:
+                    leaves.append(node)
+                for n in node.children.values():
+                    _get_leaf_nodes(n)
+
+        _get_leaf_nodes(root)
+        return leaves
+
+    leaf_nodes = get_leaf_nodes(root_node)
+    paths = []
+    for node in leaf_nodes:
+        seq = []
+        curr_node = root_node
+        for i in node.prefix:
+            curr_node = curr_node.children[i]
+            seq.append((i, curr_node.output))
+        paths.append(seq)
+
+    return paths
+
+
+def to_automaton(red, automaton_type):
+    from aalpy.automata import DfaState, Dfa, MooreMachine, MooreState, MealyMachine, MealyState
+
+    if automaton_type == 'dfa':
+        state, automaton = DfaState, Dfa
+    elif automaton_type == 'moore':
+        state, automaton = MooreState, MooreMachine
+    else:
+        state, automaton = MealyState, MealyMachine
+
+    initial_state = None
+    prefix_state_map = {}
+    for i, r in enumerate(red):
+        if automaton_type == 'moore' or automaton_type == 'dfa':
+            prefix_state_map[r.prefix] = state(f's{i}', r.output)
+        else:
+            prefix_state_map[r.prefix] = state(f's{i}')
+        if i == 0:
+            initial_state = prefix_state_map[r.prefix]
+
+    for r in red:
+        for i, c in r.children.items():
+            if automaton_type == 'moore' or automaton_type == 'dfa':
+                prefix_state_map[r.prefix].transitions[i] = prefix_state_map[c.prefix]
+            else:
+                prefix_state_map[r.prefix].transitions[i] = prefix_state_map[c.prefix]
+                prefix_state_map[r.prefix].output_fun[i] = r.output[i] if i in r.output else None
+
+    return automaton(initial_state, list(prefix_state_map.values()))
+
+
+def visualize_pta(root_node, path='pta.pdf'):
+    from pydot import Dot, Node, Edge
+    graph = Dot('fpta', graph_type='digraph')
+
+    graph.add_node(Node(str(root_node.prefix), label=f'{root_node.output}'))
+
+    queue = [root_node]
+    visited = set()
+    visited.add(root_node.prefix)
+    while queue:
+        curr = queue.pop(0)
+        for i, c in curr.children.items():
+            if c.prefix not in visited:
+                graph.add_node(Node(str(c.prefix), label=f'{c.output}'))
+            graph.add_edge(Edge(str(curr.prefix), str(c.prefix), label=f'{i}'))
+            if c.prefix not in visited:
+                queue.append(c)
+            visited.add(c.prefix)
+
+    graph.add_node(Node('__start0', shape='none', label=''))
+    graph.add_edge(Edge('__start0', str(root_node.prefix), label=''))
+
+    graph.write(path=path, format='pdf')
```

## aalpy/learning_algs/non_deterministic/AbstractedOnfsmLstar.py

 * *Ordering differences only*

```diff
@@ -1,146 +1,146 @@
-import time
-
-from aalpy.base import SUL, Oracle
-from aalpy.learning_algs.non_deterministic.AbstractedOnfsmObservationTable import AbstractedNonDetObservationTable
-from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
-from aalpy.utils.HelperFunctions import print_learning_info, print_observation_table
-
-print_options = [0, 1, 2, 3]
-
-
-def run_abstracted_ONFSM_Lstar(alphabet: list, sul: SUL, eq_oracle: Oracle, abstraction_mapping: dict, n_sampling=100,
-                               max_learning_rounds=None, return_data=False, print_level=2):
-    """
-    Based on ''Learning Abstracted Non-deterministic Finite State Machines'' from Pferscher and Aichernig.
-    The algorithm learns an abstracted onfsm of a non-deterministic system. For the additional abstraction,
-    equivalence classes for outputs are used.
-    Learning ONFSM relies on all-weather assumption. If this assumption is not satisfied by sampling,
-    learning might not converge to the minimal model and runtime could increase substantially.
-    Note that this is the inherent flaw of the all-weather assumption. (All outputs will be seen)
-    AALpy v.2.0 will try to solve that problem with a novel approach.
-
-    Args:
-
-        alphabet: input alphabet
-
-        sul: system under learning
-
-        eq_oracle: equivalence oracle
-
-        abstraction_mapping: dictionary containing mappings from abstracted to concrete values (equivalence classes)
-
-        n_sampling: number of times that membership/input queries will be asked for each cell in the observation
-            (Default value = 100)
-
-        max_learning_rounds: if max_learning_rounds is reached, learning will stop (Default value = None)
-
-        return_data: if True, map containing all information like number of queries... will be returned
-            (Default value = False)
-
-        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
-            (Default value = 2)
-
-    Returns:
-        learned abstracted ONFSM
-
-    """
-    start_time = time.time()
-    eq_query_time = 0
-    learning_rounds = 0
-    hypothesis = None
-
-    sul = NonDeterministicSULWrapper(sul)
-    eq_oracle.sul = sul
-
-    abstracted_observation_table = AbstractedNonDetObservationTable(alphabet, sul, abstraction_mapping, n_sampling)
-
-    # We fist query the initial row. Then based on output in its cells, we generate new rows in S.A,
-    # and then we perform membership/input queries for them.
-    abstracted_observation_table.update_obs_table()
-    new_rows = abstracted_observation_table.update_extended_S()
-    abstracted_observation_table.update_obs_table(s_set=new_rows)
-
-    while True:
-        learning_rounds += 1
-        if max_learning_rounds and learning_rounds - 1 == max_learning_rounds:
-            break
-
-        closed_complete_consistent = False
-        while not closed_complete_consistent:
-            closed_complete_consistent = True
-
-            row_to_close = abstracted_observation_table.get_row_to_close()
-            while row_to_close is not None:
-                # First we add new rows to the S.A. They are added based on the values in the cells of the
-                # rows that is to be closed. Once those rows are created, they are populated and closedness is checked
-                # once again.
-                closed_complete_consistent = False
-                extended_rows = abstracted_observation_table.update_extended_S(row_to_close)
-                abstracted_observation_table.update_obs_table(s_set=extended_rows)
-                row_to_close = abstracted_observation_table.get_row_to_close()
-
-            row_to_complete = abstracted_observation_table.get_row_to_complete()
-            while row_to_complete is not None:
-                closed_complete_consistent = False
-                abstracted_observation_table.extend_S_dot_A([row_to_complete])
-                abstracted_observation_table.update_obs_table(s_set=[row_to_complete])
-                row_to_complete = abstracted_observation_table.get_row_to_complete()
-
-            e_column_for_consistency = abstracted_observation_table.get_row_to_make_consistent()
-            while e_column_for_consistency is not None:
-                closed_complete_consistent = False
-                extended_col = abstracted_observation_table.update_E(e_column_for_consistency)
-                abstracted_observation_table.update_obs_table(e_set=extended_col)
-                e_column_for_consistency = abstracted_observation_table.get_row_to_make_consistent()
-
-        abstracted_observation_table.clean_tables()
-        hypothesis = abstracted_observation_table.gen_hypothesis()
-
-        if print_level == 3:
-            print('Observation Table')
-            print_observation_table(abstracted_observation_table.observation_table, 'non-det')
-            print()
-            print('Abstracted Observation Table')
-            # CHANGED, but not important to alg
-            print_observation_table(abstracted_observation_table, 'abstracted-non-det')
-
-        if print_level > 1:
-            print(f'Hypothesis {learning_rounds} has {len(hypothesis.states)} states.')
-
-        # Find counterexample
-        eq_query_start = time.time()
-        cex = eq_oracle.find_cex(hypothesis)
-        eq_query_time += time.time() - eq_query_start
-
-        if cex is None:
-            break
-
-        if print_level >= 2:
-            print('Counterexample', cex)
-
-        # Process counterexample -> add cex to S.A or E
-        abstracted_observation_table.cex_processing(cex, hypothesis)
-
-    total_time = round(time.time() - start_time, 2)
-    eq_query_time = round(eq_query_time, 2)
-    learning_time = round(total_time - eq_query_time, 2)
-
-    info = {
-        'learning_rounds': learning_rounds,
-        'automaton_size': len(hypothesis.states),
-        'queries_learning': sul.num_queries,
-        'steps_learning': sul.num_steps,
-        'queries_eq_oracle': eq_oracle.num_queries,
-        'steps_eq_oracle': eq_oracle.num_steps,
-        'learning_time': learning_time,
-        'eq_oracle_time': eq_query_time,
-        'total_time': total_time
-    }
-
-    if print_level > 0:
-        print_learning_info(info)
-
-    if return_data:
-        return hypothesis, info
-
-    return hypothesis
+import time
+
+from aalpy.base import SUL, Oracle
+from aalpy.learning_algs.non_deterministic.AbstractedOnfsmObservationTable import AbstractedNonDetObservationTable
+from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
+from aalpy.utils.HelperFunctions import print_learning_info, print_observation_table
+
+print_options = [0, 1, 2, 3]
+
+
+def run_abstracted_ONFSM_Lstar(alphabet: list, sul: SUL, eq_oracle: Oracle, abstraction_mapping: dict, n_sampling=100,
+                               max_learning_rounds=None, return_data=False, print_level=2):
+    """
+    Based on ''Learning Abstracted Non-deterministic Finite State Machines'' from Pferscher and Aichernig.
+    The algorithm learns an abstracted onfsm of a non-deterministic system. For the additional abstraction,
+    equivalence classes for outputs are used.
+    Learning ONFSM relies on all-weather assumption. If this assumption is not satisfied by sampling,
+    learning might not converge to the minimal model and runtime could increase substantially.
+    Note that this is the inherent flaw of the all-weather assumption. (All outputs will be seen)
+    AALpy v.2.0 will try to solve that problem with a novel approach.
+
+    Args:
+
+        alphabet: input alphabet
+
+        sul: system under learning
+
+        eq_oracle: equivalence oracle
+
+        abstraction_mapping: dictionary containing mappings from abstracted to concrete values (equivalence classes)
+
+        n_sampling: number of times that membership/input queries will be asked for each cell in the observation
+            (Default value = 100)
+
+        max_learning_rounds: if max_learning_rounds is reached, learning will stop (Default value = None)
+
+        return_data: if True, map containing all information like number of queries... will be returned
+            (Default value = False)
+
+        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
+            (Default value = 2)
+
+    Returns:
+        learned abstracted ONFSM
+
+    """
+    start_time = time.time()
+    eq_query_time = 0
+    learning_rounds = 0
+    hypothesis = None
+
+    sul = NonDeterministicSULWrapper(sul)
+    eq_oracle.sul = sul
+
+    abstracted_observation_table = AbstractedNonDetObservationTable(alphabet, sul, abstraction_mapping, n_sampling)
+
+    # We fist query the initial row. Then based on output in its cells, we generate new rows in S.A,
+    # and then we perform membership/input queries for them.
+    abstracted_observation_table.update_obs_table()
+    new_rows = abstracted_observation_table.update_extended_S()
+    abstracted_observation_table.update_obs_table(s_set=new_rows)
+
+    while True:
+        learning_rounds += 1
+        if max_learning_rounds and learning_rounds - 1 == max_learning_rounds:
+            break
+
+        closed_complete_consistent = False
+        while not closed_complete_consistent:
+            closed_complete_consistent = True
+
+            row_to_close = abstracted_observation_table.get_row_to_close()
+            while row_to_close is not None:
+                # First we add new rows to the S.A. They are added based on the values in the cells of the
+                # rows that is to be closed. Once those rows are created, they are populated and closedness is checked
+                # once again.
+                closed_complete_consistent = False
+                extended_rows = abstracted_observation_table.update_extended_S(row_to_close)
+                abstracted_observation_table.update_obs_table(s_set=extended_rows)
+                row_to_close = abstracted_observation_table.get_row_to_close()
+
+            row_to_complete = abstracted_observation_table.get_row_to_complete()
+            while row_to_complete is not None:
+                closed_complete_consistent = False
+                abstracted_observation_table.extend_S_dot_A([row_to_complete])
+                abstracted_observation_table.update_obs_table(s_set=[row_to_complete])
+                row_to_complete = abstracted_observation_table.get_row_to_complete()
+
+            e_column_for_consistency = abstracted_observation_table.get_row_to_make_consistent()
+            while e_column_for_consistency is not None:
+                closed_complete_consistent = False
+                extended_col = abstracted_observation_table.update_E(e_column_for_consistency)
+                abstracted_observation_table.update_obs_table(e_set=extended_col)
+                e_column_for_consistency = abstracted_observation_table.get_row_to_make_consistent()
+
+        abstracted_observation_table.clean_tables()
+        hypothesis = abstracted_observation_table.gen_hypothesis()
+
+        if print_level == 3:
+            print('Observation Table')
+            print_observation_table(abstracted_observation_table.observation_table, 'non-det')
+            print()
+            print('Abstracted Observation Table')
+            # CHANGED, but not important to alg
+            print_observation_table(abstracted_observation_table, 'abstracted-non-det')
+
+        if print_level > 1:
+            print(f'Hypothesis {learning_rounds} has {len(hypothesis.states)} states.')
+
+        # Find counterexample
+        eq_query_start = time.time()
+        cex = eq_oracle.find_cex(hypothesis)
+        eq_query_time += time.time() - eq_query_start
+
+        if cex is None:
+            break
+
+        if print_level >= 2:
+            print('Counterexample', cex)
+
+        # Process counterexample -> add cex to S.A or E
+        abstracted_observation_table.cex_processing(cex, hypothesis)
+
+    total_time = round(time.time() - start_time, 2)
+    eq_query_time = round(eq_query_time, 2)
+    learning_time = round(total_time - eq_query_time, 2)
+
+    info = {
+        'learning_rounds': learning_rounds,
+        'automaton_size': len(hypothesis.states),
+        'queries_learning': sul.num_queries,
+        'steps_learning': sul.num_steps,
+        'queries_eq_oracle': eq_oracle.num_queries,
+        'steps_eq_oracle': eq_oracle.num_steps,
+        'learning_time': learning_time,
+        'eq_oracle_time': eq_query_time,
+        'total_time': total_time
+    }
+
+    if print_level > 0:
+        print_learning_info(info)
+
+    if return_data:
+        return hypothesis, info
+
+    return hypothesis
```

## aalpy/learning_algs/non_deterministic/AbstractedOnfsmObservationTable.py

 * *Ordering differences only*

```diff
@@ -1,442 +1,442 @@
-from collections import defaultdict
-
-from aalpy.automata import Onfsm, OnfsmState
-from aalpy.learning_algs.non_deterministic.OnfsmObservationTable import NonDetObservationTable
-from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
-from aalpy.utils.HelperFunctions import all_suffixes, extend_set
-
-
-class AbstractedNonDetObservationTable:
-    def __init__(self, alphabet: list, sul: NonDeterministicSULWrapper, abstraction_mapping: dict, n_sampling=100):
-        """
-        Construction of the abstracted non-deterministic observation table.
-
-        Args:
-
-            alphabet: input alphabet
-            sul: system under learning
-            abstraction_mapping: map that translates outputs to abstracted outputs
-            n_sampling: number of samples to be performed for each cell
-        """
-
-        assert alphabet is not None and sul is not None
-
-        self.observation_table = NonDetObservationTable(alphabet, sul, n_sampling)
-
-        self.S = list()
-        self.S_dot_A = []
-        self.E = []
-        self.T = defaultdict(dict)
-        self.A = [tuple([a]) for a in alphabet]
-
-        self.abstraction_mapping = abstraction_mapping
-        self.sul = sul
-
-        empty_word = tuple()
-        self.S.append((empty_word, empty_word))
-
-    def update_obs_table(self, s_set=None, e_set: list = None):
-        """
-        Perform the membership queries and abstraction on observation table
-        With  the  all-weather  assumption,  each  output  query  is  tried  a  number  of  times  on  the  system,
-        and  the  driver  reports  the  set  of  all  possible  outputs.
-
-        Args:
-
-            s_set: Prefixes of S set on which to preform membership queries (Default value = None)
-            e_set: Suffixes of E set on which to perform membership queries
-
-
-        """
-
-        self.observation_table.query_missing_observations(s_set, e_set)
-        self.abstract_obs_table()
-        self.clean_obs_table()
-
-    def abstract_obs_table(self):
-        """
-        Creation of abstracted observation table. The provided abstraction mapping is used to
-        replace outputs by abstracted outputs.
-        """
-
-        self.S = self.observation_table.S
-        self.S_dot_A = list(set(self.observation_table.get_extended_S()).union(set(self.S_dot_A) - set(self.S)))
-        self.E = self.observation_table.E
-
-        update_S = self.S + self.S_dot_A
-        update_E = self.E
-
-        for s in update_S:
-            for e in update_E:
-                for o_tup in self.get_all_outputs(s, e):
-                    abstracted_outputs = []
-                    o_tup = tuple([o_tup])
-                    for outputs in o_tup:
-                        for o in outputs:
-                            abstract_output = self.get_abstraction(o)
-                            abstracted_outputs.append(abstract_output)
-                    self.add_to_T(s, e, tuple(abstracted_outputs))
-
-    def add_to_T(self, s, e, value):
-        """
-        Add values to the cell at T[s][e].
-
-        Args:
-
-            s: prefix
-            e: element of S
-            value: value to be added to the cell
-
-
-        """
-        if e not in self.T[s]:
-            self.T[s][e] = set()
-        self.T[s][e].add(value)
-
-    # CHANGED
-    # helper function
-    def get_all_outputs(self, s, e):
-        cell_outputs = set()
-        cell_outputs.update(self.sul.cache.get_all_traces(s, e))
-        return cell_outputs
-
-    def update_extended_S(self, row_prefix=None):
-        """
-        Helper generator function that returns extended S, or S.A set.
-        For all values in the cell, create a new row where inputs is parent input plus element of alphabet, and
-        output is parent output plus value in cell.
-
-        Returns:
-
-            New rows of extended S set.
-        """
-        return self.observation_table.get_extended_S(row_prefix=row_prefix)
-
-    def get_row_to_close(self):
-        """
-        Get row for that needs to be closed.
-
-        Returns:
-
-            row that will be moved to S set and closed
-        """
-        s_rows = set()
-        for s in self.S:
-            s_rows.add(self.row_to_hashable(s))
-
-        for t in self.S_dot_A:
-            row_t = self.row_to_hashable(t)
-
-            if row_t not in s_rows:
-                self.S.append(t)
-                self.S_dot_A.remove(t)
-                return t
-
-        return None
-
-    def get_row_to_complete(self):
-        """
-        Get row for that needs to be completed.
-
-        Returns:
-
-            row that will be added to S.A
-        """
-
-        s_rows = set()
-        for s in self.S:
-            s_rows.add(tuple((s, self.row_to_hashable(s))))
-
-        for s_row in s_rows:
-            similar_s_dot_a_rows = []
-            for t in self.S_dot_A:
-                row_t = self.row_to_hashable(t)
-                if row_t == s_row[1]:
-                    similar_s_dot_a_rows.append(t)
-            similar_s_dot_a_rows.sort(key=lambda row: len(row[0]))
-            for a in self.A:
-                complete_outputs = self.get_all_outputs(s_row[0], a)
-                for similar_s_dot_a_row in similar_s_dot_a_rows:
-                    t_row_outputs = self.get_all_outputs(similar_s_dot_a_row, a)
-                    output_difference = t_row_outputs.difference(complete_outputs)
-                    if len(output_difference) > 0:
-                        for o in output_difference:
-                            extension = (similar_s_dot_a_row[0] + a, similar_s_dot_a_row[1] + tuple([o[0]]))
-                            if extension not in self.S and extension not in self.S_dot_A:
-                                return extension
-                            else:
-                                complete_outputs = complete_outputs.union(output_difference)
-
-        return None
-
-    def get_row_to_make_consistent(self):
-        """
-        Get row that violates consistency.
-        """
-        unified_S = self.S + self.S_dot_A
-        s_rows = set()
-        for s in self.S:
-            s_rows.add(tuple((s, self.row_to_hashable(s))))
-
-        for s_row in s_rows:
-            similar_s_dot_a_rows = []
-            for t in self.S_dot_A:
-                row_t = self.row_to_hashable(t)
-                if row_t == s_row[1]:
-                    similar_s_dot_a_rows.append(t)
-
-            similar_s_dot_a_rows.sort(key=lambda row: len(row[0]))
-
-            for a in self.A:
-                # CHANGED
-                #                 outputs = self.observation_table.T[s_row[0]][a]
-                outputs = self.get_all_outputs(s_row[0], a)
-                for o in outputs:
-                    extended_s_sequence = (s_row[0][0] + a, s_row[0][1] + tuple([o]))
-                    if extended_s_sequence in unified_S:
-                        extended_s_sequence_row = self.row_to_hashable(extended_s_sequence)
-                        for similar_s_dot_a_row in similar_s_dot_a_rows:
-                            extended_s_dot_a_sequence = (
-                                similar_s_dot_a_row[0] + a, similar_s_dot_a_row[1] + tuple([o]))
-                            if extended_s_dot_a_sequence in unified_S:
-                                extended_s_dot_a_sequence_row = self.row_to_hashable(extended_s_dot_a_sequence)
-                                if extended_s_sequence_row is not extended_s_dot_a_sequence_row:
-                                    return self.get_distinctive_input_sequence(extended_s_sequence,
-                                                                               extended_s_dot_a_sequence, a)
-
-        return None
-
-    def get_distinctive_input_sequence(self, first_row, second_row, inp):
-        """
-        get input sequence that leads to a different output sequence for two given input/output sequences
-
-        Args:
-
-            first_row: row to be compared
-            second_row: row to be compared
-            inp: appended input to first_row and second_row that leads to different state 
-
-        Returns:
-
-            input sequence that leads to different outputs
-
-        """
-        for e in self.E:
-            if len(self.T[first_row][e].difference(self.T[second_row][e])) > 0:
-                return tuple([inp]) + e
-
-        return None
-
-    def update_E(self, seq):
-        if seq not in self.E:
-            self.E.append(seq)
-
-    def clean_obs_table(self):
-        """
-        Moves duplicates from S to S_dot_A. The entries in S_dot_A which are based on the moved row get deleted.
-        The table will be smaller and more efficient.
-
-        """
-        # just for testing without cleaning
-        # return False
-
-        tmp_S = self.S.copy()
-        tmp_both_S = self.S + self.S_dot_A
-        hashed_rows_from_s = set()
-
-        tmp_S.sort(key=lambda t: len(t[0]))
-
-        for s in tmp_S:
-            hashed_s_row = self.row_to_hashable(s)
-            if hashed_s_row in hashed_rows_from_s:
-                if s in self.S:
-                    self.S.remove(s)
-                    self.observation_table.S.remove(s)
-                size = len(s[0])
-                for row_prefix in tmp_both_S:
-                    s_both_row = (row_prefix[0][:size], row_prefix[1][:size])
-                    if s != row_prefix and s == s_both_row:
-                        if row_prefix in self.S:
-                            self.S.remove(row_prefix)
-                            self.observation_table.S.remove(s)
-            else:
-                hashed_rows_from_s.add(hashed_s_row)
-
-    def row_to_hashable(self, row_prefix):
-        """
-        Creates the hashable representation of the row. Frozenset is used as the order of element in each cell does not
-        matter
-
-        Args:
-
-            row_prefix: prefix of the row in the observation table
-
-        Returns:
-
-            hashable representation of the row
-
-        """
-        row_repr = tuple()
-        for e in self.E:
-            # if e in self.T[row_prefix].keys():
-            row_repr += (frozenset(self.T[row_prefix][e]),)
-        return row_repr
-
-    def gen_hypothesis(self) -> Onfsm:
-        """
-        Generate automaton based on the values found in the abstracted observation table.
-
-        Returns:
-
-            Current abstracted hypothesis
-
-        """
-        state_distinguish = dict()
-        states_dict = dict()
-        initial = None
-
-        unified_S = self.S + self.S_dot_A
-
-        stateCounter = 0
-        for prefix in self.S:
-            state_id = f's{stateCounter}'
-            states_dict[prefix] = OnfsmState(state_id)
-
-            states_dict[prefix].prefix = prefix
-            state_distinguish[self.row_to_hashable(prefix)] = states_dict[prefix]
-
-            if prefix == self.S[0]:
-                initial = states_dict[prefix]
-            stateCounter += 1
-
-        for prefix in self.S:
-            similar_rows = []
-            for row in unified_S:
-                if self.row_to_hashable(row) == self.row_to_hashable(prefix):
-                    similar_rows.append(row)
-            for row in similar_rows:
-                for a in self.A:
-                    for t in self.get_all_outputs(row, a):
-                        s_entry = (row[0] + a, row[1] + t)
-                        if s_entry in unified_S:
-                            state_in_S = state_distinguish[self.row_to_hashable(s_entry)]
-
-                            if (t[0], state_in_S) not in states_dict[prefix].transitions[a[0]]:
-                                states_dict[prefix].transitions[a[0]].append((t[0], state_in_S))
-
-        assert initial
-        automaton = Onfsm(initial, [s for s in states_dict.values()])
-        automaton.characterization_set = self.E
-
-        return automaton
-
-    def extend_S_dot_A(self, cex_prefixes: list):
-        """
-        Extends S.A based on counterexample prefixes.
-
-        Args:
-
-        cex_prefixes: input/output sequences that are added to S.A
-
-        Returns:
-
-        input/output sequences that have been added to the S.A
-        """
-        prefixes = self.S + self.S_dot_A
-        prefixes_to_extend = []
-        for cex_prefix in cex_prefixes:
-            if cex_prefix not in prefixes:
-                prefixes_to_extend.append(cex_prefix)
-                self.S_dot_A.append(cex_prefix)
-        return prefixes_to_extend
-
-    def get_abstraction(self, out):
-        """
-        Get an abstraction for a concrete output. If such abstraction is not defined, return output.
-
-        Args:
-
-            out: output to be abstracted if possible
-
-        Returns:
-
-            abstracted output or output itself
-        """
-        return self.abstraction_mapping[out] if out in self.abstraction_mapping.keys() else out
-
-    def cex_processing(self, cex: tuple, hypothesis: Onfsm):
-        """
-        Add counterexample to the observation table. If the counterexample leads to a state where an output of the
-        same equivalence class already exists, the prefixes of the counterexample are added to S.A.
-        Otherwise, the postfixes of counterexample are added to E.
-
-
-        Args:
-
-            cex: counterexample that should be added to the observation table
-            hypothesis: onfsm that implements the counterexample
-        """
-
-        cex_len = len(cex[0])
-        hypothesis.reset_to_initial()
-
-        for step in range(0, cex_len - 1):
-            hypothesis.step_to(cex[0][step], cex[1][step])
-
-        possible_outputs = hypothesis.outputs_on_input(cex[0][cex_len - 1])
-
-        equivalent_output = False
-
-        for out in possible_outputs:
-            abstracted_out = self.get_abstraction(out)
-            abstracted_out_cex = self.get_abstraction(cex[1][cex_len - 1])
-            if abstracted_out_cex == abstracted_out:
-                equivalent_output = True
-                break
-
-        if equivalent_output:
-            # add prefixes of cex to S_dot_A
-            cex_prefixes = [(tuple(cex[0][0:i + 1]), tuple(cex[1][0:i + 1])) for i in range(0, len(cex[0]))]
-            prefixes_to_extend = self.extend_S_dot_A(cex_prefixes)
-
-            # CHANGED: REMOVED
-            # self.observation_table.S_dot_A.extend(prefixes_to_extend)
-            self.update_obs_table(s_set=prefixes_to_extend)
-        else:
-            # add distinguishing suffixes of cex to E
-            # CHANGED CEX PROX
-            # TODO: this will now not work as cex processing was changed
-            # cex_suffixes = non_det_longest_prefix_cex_processing(self.observation_table, cex)
-            cex_suffixes = all_suffixes(cex[0])
-
-            added_suffixes = extend_set(self.observation_table.E, cex_suffixes)
-            self.update_obs_table(e_set=added_suffixes)
-
-    def clean_tables(self):
-
-        self.observation_table.clean_obs_table()
-        self.abstract_obs_table()
-
-        update_S = self.S.copy()
-        whole_S = self.S + self.S_dot_A
-
-        update_S.sort()
-        update_S.sort(key=lambda t: len(t[0]))
-
-        s_rows = set()
-        for s in update_S:
-            hashed_s_row = self.row_to_hashable(s)
-            if hashed_s_row not in s_rows:
-                s_rows.add(hashed_s_row)
-            else:
-                size = len(s[0])
-                for row in whole_S:
-                    cmp_row = (row[0][:size], row[1][:size])
-                    if s == cmp_row:
-                        if row in self.S_dot_A:
-                            self.S_dot_A.remove(row)
-                        elif row in self.S:
-                            self.S.remove(row)
-
-                self.S_dot_A.append(s)
-                self.S.remove(s)
+from collections import defaultdict
+
+from aalpy.automata import Onfsm, OnfsmState
+from aalpy.learning_algs.non_deterministic.OnfsmObservationTable import NonDetObservationTable
+from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
+from aalpy.utils.HelperFunctions import all_suffixes, extend_set
+
+
+class AbstractedNonDetObservationTable:
+    def __init__(self, alphabet: list, sul: NonDeterministicSULWrapper, abstraction_mapping: dict, n_sampling=100):
+        """
+        Construction of the abstracted non-deterministic observation table.
+
+        Args:
+
+            alphabet: input alphabet
+            sul: system under learning
+            abstraction_mapping: map that translates outputs to abstracted outputs
+            n_sampling: number of samples to be performed for each cell
+        """
+
+        assert alphabet is not None and sul is not None
+
+        self.observation_table = NonDetObservationTable(alphabet, sul, n_sampling)
+
+        self.S = list()
+        self.S_dot_A = []
+        self.E = []
+        self.T = defaultdict(dict)
+        self.A = [tuple([a]) for a in alphabet]
+
+        self.abstraction_mapping = abstraction_mapping
+        self.sul = sul
+
+        empty_word = tuple()
+        self.S.append((empty_word, empty_word))
+
+    def update_obs_table(self, s_set=None, e_set: list = None):
+        """
+        Perform the membership queries and abstraction on observation table
+        With  the  all-weather  assumption,  each  output  query  is  tried  a  number  of  times  on  the  system,
+        and  the  driver  reports  the  set  of  all  possible  outputs.
+
+        Args:
+
+            s_set: Prefixes of S set on which to preform membership queries (Default value = None)
+            e_set: Suffixes of E set on which to perform membership queries
+
+
+        """
+
+        self.observation_table.query_missing_observations(s_set, e_set)
+        self.abstract_obs_table()
+        self.clean_obs_table()
+
+    def abstract_obs_table(self):
+        """
+        Creation of abstracted observation table. The provided abstraction mapping is used to
+        replace outputs by abstracted outputs.
+        """
+
+        self.S = self.observation_table.S
+        self.S_dot_A = list(set(self.observation_table.get_extended_S()).union(set(self.S_dot_A) - set(self.S)))
+        self.E = self.observation_table.E
+
+        update_S = self.S + self.S_dot_A
+        update_E = self.E
+
+        for s in update_S:
+            for e in update_E:
+                for o_tup in self.get_all_outputs(s, e):
+                    abstracted_outputs = []
+                    o_tup = tuple([o_tup])
+                    for outputs in o_tup:
+                        for o in outputs:
+                            abstract_output = self.get_abstraction(o)
+                            abstracted_outputs.append(abstract_output)
+                    self.add_to_T(s, e, tuple(abstracted_outputs))
+
+    def add_to_T(self, s, e, value):
+        """
+        Add values to the cell at T[s][e].
+
+        Args:
+
+            s: prefix
+            e: element of S
+            value: value to be added to the cell
+
+
+        """
+        if e not in self.T[s]:
+            self.T[s][e] = set()
+        self.T[s][e].add(value)
+
+    # CHANGED
+    # helper function
+    def get_all_outputs(self, s, e):
+        cell_outputs = set()
+        cell_outputs.update(self.sul.cache.get_all_traces(s, e))
+        return cell_outputs
+
+    def update_extended_S(self, row_prefix=None):
+        """
+        Helper generator function that returns extended S, or S.A set.
+        For all values in the cell, create a new row where inputs is parent input plus element of alphabet, and
+        output is parent output plus value in cell.
+
+        Returns:
+
+            New rows of extended S set.
+        """
+        return self.observation_table.get_extended_S(row_prefix=row_prefix)
+
+    def get_row_to_close(self):
+        """
+        Get row for that needs to be closed.
+
+        Returns:
+
+            row that will be moved to S set and closed
+        """
+        s_rows = set()
+        for s in self.S:
+            s_rows.add(self.row_to_hashable(s))
+
+        for t in self.S_dot_A:
+            row_t = self.row_to_hashable(t)
+
+            if row_t not in s_rows:
+                self.S.append(t)
+                self.S_dot_A.remove(t)
+                return t
+
+        return None
+
+    def get_row_to_complete(self):
+        """
+        Get row for that needs to be completed.
+
+        Returns:
+
+            row that will be added to S.A
+        """
+
+        s_rows = set()
+        for s in self.S:
+            s_rows.add(tuple((s, self.row_to_hashable(s))))
+
+        for s_row in s_rows:
+            similar_s_dot_a_rows = []
+            for t in self.S_dot_A:
+                row_t = self.row_to_hashable(t)
+                if row_t == s_row[1]:
+                    similar_s_dot_a_rows.append(t)
+            similar_s_dot_a_rows.sort(key=lambda row: len(row[0]))
+            for a in self.A:
+                complete_outputs = self.get_all_outputs(s_row[0], a)
+                for similar_s_dot_a_row in similar_s_dot_a_rows:
+                    t_row_outputs = self.get_all_outputs(similar_s_dot_a_row, a)
+                    output_difference = t_row_outputs.difference(complete_outputs)
+                    if len(output_difference) > 0:
+                        for o in output_difference:
+                            extension = (similar_s_dot_a_row[0] + a, similar_s_dot_a_row[1] + tuple([o[0]]))
+                            if extension not in self.S and extension not in self.S_dot_A:
+                                return extension
+                            else:
+                                complete_outputs = complete_outputs.union(output_difference)
+
+        return None
+
+    def get_row_to_make_consistent(self):
+        """
+        Get row that violates consistency.
+        """
+        unified_S = self.S + self.S_dot_A
+        s_rows = set()
+        for s in self.S:
+            s_rows.add(tuple((s, self.row_to_hashable(s))))
+
+        for s_row in s_rows:
+            similar_s_dot_a_rows = []
+            for t in self.S_dot_A:
+                row_t = self.row_to_hashable(t)
+                if row_t == s_row[1]:
+                    similar_s_dot_a_rows.append(t)
+
+            similar_s_dot_a_rows.sort(key=lambda row: len(row[0]))
+
+            for a in self.A:
+                # CHANGED
+                #                 outputs = self.observation_table.T[s_row[0]][a]
+                outputs = self.get_all_outputs(s_row[0], a)
+                for o in outputs:
+                    extended_s_sequence = (s_row[0][0] + a, s_row[0][1] + tuple([o]))
+                    if extended_s_sequence in unified_S:
+                        extended_s_sequence_row = self.row_to_hashable(extended_s_sequence)
+                        for similar_s_dot_a_row in similar_s_dot_a_rows:
+                            extended_s_dot_a_sequence = (
+                                similar_s_dot_a_row[0] + a, similar_s_dot_a_row[1] + tuple([o]))
+                            if extended_s_dot_a_sequence in unified_S:
+                                extended_s_dot_a_sequence_row = self.row_to_hashable(extended_s_dot_a_sequence)
+                                if extended_s_sequence_row is not extended_s_dot_a_sequence_row:
+                                    return self.get_distinctive_input_sequence(extended_s_sequence,
+                                                                               extended_s_dot_a_sequence, a)
+
+        return None
+
+    def get_distinctive_input_sequence(self, first_row, second_row, inp):
+        """
+        get input sequence that leads to a different output sequence for two given input/output sequences
+
+        Args:
+
+            first_row: row to be compared
+            second_row: row to be compared
+            inp: appended input to first_row and second_row that leads to different state 
+
+        Returns:
+
+            input sequence that leads to different outputs
+
+        """
+        for e in self.E:
+            if len(self.T[first_row][e].difference(self.T[second_row][e])) > 0:
+                return tuple([inp]) + e
+
+        return None
+
+    def update_E(self, seq):
+        if seq not in self.E:
+            self.E.append(seq)
+
+    def clean_obs_table(self):
+        """
+        Moves duplicates from S to S_dot_A. The entries in S_dot_A which are based on the moved row get deleted.
+        The table will be smaller and more efficient.
+
+        """
+        # just for testing without cleaning
+        # return False
+
+        tmp_S = self.S.copy()
+        tmp_both_S = self.S + self.S_dot_A
+        hashed_rows_from_s = set()
+
+        tmp_S.sort(key=lambda t: len(t[0]))
+
+        for s in tmp_S:
+            hashed_s_row = self.row_to_hashable(s)
+            if hashed_s_row in hashed_rows_from_s:
+                if s in self.S:
+                    self.S.remove(s)
+                    self.observation_table.S.remove(s)
+                size = len(s[0])
+                for row_prefix in tmp_both_S:
+                    s_both_row = (row_prefix[0][:size], row_prefix[1][:size])
+                    if s != row_prefix and s == s_both_row:
+                        if row_prefix in self.S:
+                            self.S.remove(row_prefix)
+                            self.observation_table.S.remove(s)
+            else:
+                hashed_rows_from_s.add(hashed_s_row)
+
+    def row_to_hashable(self, row_prefix):
+        """
+        Creates the hashable representation of the row. Frozenset is used as the order of element in each cell does not
+        matter
+
+        Args:
+
+            row_prefix: prefix of the row in the observation table
+
+        Returns:
+
+            hashable representation of the row
+
+        """
+        row_repr = tuple()
+        for e in self.E:
+            # if e in self.T[row_prefix].keys():
+            row_repr += (frozenset(self.T[row_prefix][e]),)
+        return row_repr
+
+    def gen_hypothesis(self) -> Onfsm:
+        """
+        Generate automaton based on the values found in the abstracted observation table.
+
+        Returns:
+
+            Current abstracted hypothesis
+
+        """
+        state_distinguish = dict()
+        states_dict = dict()
+        initial = None
+
+        unified_S = self.S + self.S_dot_A
+
+        stateCounter = 0
+        for prefix in self.S:
+            state_id = f's{stateCounter}'
+            states_dict[prefix] = OnfsmState(state_id)
+
+            states_dict[prefix].prefix = prefix
+            state_distinguish[self.row_to_hashable(prefix)] = states_dict[prefix]
+
+            if prefix == self.S[0]:
+                initial = states_dict[prefix]
+            stateCounter += 1
+
+        for prefix in self.S:
+            similar_rows = []
+            for row in unified_S:
+                if self.row_to_hashable(row) == self.row_to_hashable(prefix):
+                    similar_rows.append(row)
+            for row in similar_rows:
+                for a in self.A:
+                    for t in self.get_all_outputs(row, a):
+                        s_entry = (row[0] + a, row[1] + t)
+                        if s_entry in unified_S:
+                            state_in_S = state_distinguish[self.row_to_hashable(s_entry)]
+
+                            if (t[0], state_in_S) not in states_dict[prefix].transitions[a[0]]:
+                                states_dict[prefix].transitions[a[0]].append((t[0], state_in_S))
+
+        assert initial
+        automaton = Onfsm(initial, [s for s in states_dict.values()])
+        automaton.characterization_set = self.E
+
+        return automaton
+
+    def extend_S_dot_A(self, cex_prefixes: list):
+        """
+        Extends S.A based on counterexample prefixes.
+
+        Args:
+
+        cex_prefixes: input/output sequences that are added to S.A
+
+        Returns:
+
+        input/output sequences that have been added to the S.A
+        """
+        prefixes = self.S + self.S_dot_A
+        prefixes_to_extend = []
+        for cex_prefix in cex_prefixes:
+            if cex_prefix not in prefixes:
+                prefixes_to_extend.append(cex_prefix)
+                self.S_dot_A.append(cex_prefix)
+        return prefixes_to_extend
+
+    def get_abstraction(self, out):
+        """
+        Get an abstraction for a concrete output. If such abstraction is not defined, return output.
+
+        Args:
+
+            out: output to be abstracted if possible
+
+        Returns:
+
+            abstracted output or output itself
+        """
+        return self.abstraction_mapping[out] if out in self.abstraction_mapping.keys() else out
+
+    def cex_processing(self, cex: tuple, hypothesis: Onfsm):
+        """
+        Add counterexample to the observation table. If the counterexample leads to a state where an output of the
+        same equivalence class already exists, the prefixes of the counterexample are added to S.A.
+        Otherwise, the postfixes of counterexample are added to E.
+
+
+        Args:
+
+            cex: counterexample that should be added to the observation table
+            hypothesis: onfsm that implements the counterexample
+        """
+
+        cex_len = len(cex[0])
+        hypothesis.reset_to_initial()
+
+        for step in range(0, cex_len - 1):
+            hypothesis.step_to(cex[0][step], cex[1][step])
+
+        possible_outputs = hypothesis.outputs_on_input(cex[0][cex_len - 1])
+
+        equivalent_output = False
+
+        for out in possible_outputs:
+            abstracted_out = self.get_abstraction(out)
+            abstracted_out_cex = self.get_abstraction(cex[1][cex_len - 1])
+            if abstracted_out_cex == abstracted_out:
+                equivalent_output = True
+                break
+
+        if equivalent_output:
+            # add prefixes of cex to S_dot_A
+            cex_prefixes = [(tuple(cex[0][0:i + 1]), tuple(cex[1][0:i + 1])) for i in range(0, len(cex[0]))]
+            prefixes_to_extend = self.extend_S_dot_A(cex_prefixes)
+
+            # CHANGED: REMOVED
+            # self.observation_table.S_dot_A.extend(prefixes_to_extend)
+            self.update_obs_table(s_set=prefixes_to_extend)
+        else:
+            # add distinguishing suffixes of cex to E
+            # CHANGED CEX PROX
+            # TODO: this will now not work as cex processing was changed
+            # cex_suffixes = non_det_longest_prefix_cex_processing(self.observation_table, cex)
+            cex_suffixes = all_suffixes(cex[0])
+
+            added_suffixes = extend_set(self.observation_table.E, cex_suffixes)
+            self.update_obs_table(e_set=added_suffixes)
+
+    def clean_tables(self):
+
+        self.observation_table.clean_obs_table()
+        self.abstract_obs_table()
+
+        update_S = self.S.copy()
+        whole_S = self.S + self.S_dot_A
+
+        update_S.sort()
+        update_S.sort(key=lambda t: len(t[0]))
+
+        s_rows = set()
+        for s in update_S:
+            hashed_s_row = self.row_to_hashable(s)
+            if hashed_s_row not in s_rows:
+                s_rows.add(hashed_s_row)
+            else:
+                size = len(s[0])
+                for row in whole_S:
+                    cmp_row = (row[0][:size], row[1][:size])
+                    if s == cmp_row:
+                        if row in self.S_dot_A:
+                            self.S_dot_A.remove(row)
+                        elif row in self.S:
+                            self.S.remove(row)
+
+                self.S_dot_A.append(s)
+                self.S.remove(s)
```

## aalpy/learning_algs/non_deterministic/NonDeterministicSULWrapper.py

 * *Ordering differences only*

```diff
@@ -1,25 +1,25 @@
-from aalpy.base import SUL
-from aalpy.learning_algs.non_deterministic.TraceTree import TraceTree
-
-
-class NonDeterministicSULWrapper(SUL):
-    """
-    Wrapper for non-deterministic SUL. After every step, input/output pair is added to the tree containing all traces.
-    """
-
-    def __init__(self, sul: SUL):
-        super().__init__()
-        self.sul = sul
-        self.cache = TraceTree()
-
-    def pre(self):
-        self.cache.reset()
-        self.sul.pre()
-
-    def post(self):
-        self.sul.post()
-
-    def step(self, letter):
-        out = self.sul.step(letter)
-        self.cache.add_to_tree(letter, out)
-        return out
+from aalpy.base import SUL
+from aalpy.learning_algs.non_deterministic.TraceTree import TraceTree
+
+
+class NonDeterministicSULWrapper(SUL):
+    """
+    Wrapper for non-deterministic SUL. After every step, input/output pair is added to the tree containing all traces.
+    """
+
+    def __init__(self, sul: SUL):
+        super().__init__()
+        self.sul = sul
+        self.cache = TraceTree()
+
+    def pre(self):
+        self.cache.reset()
+        self.sul.pre()
+
+    def post(self):
+        self.sul.post()
+
+    def step(self, letter):
+        out = self.sul.step(letter)
+        self.cache.add_to_tree(letter, out)
+        return out
```

## aalpy/learning_algs/non_deterministic/OnfsmLstar.py

 * *Ordering differences only*

```diff
@@ -1,150 +1,150 @@
-import time
-
-from aalpy.base import SUL, Oracle
-from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
-from aalpy.learning_algs.non_deterministic.OnfsmObservationTable import NonDetObservationTable
-from aalpy.utils.HelperFunctions import print_learning_info, print_observation_table, \
-    get_available_oracles_and_err_msg, all_suffixes
-
-print_options = [0, 1, 2, 3]
-
-available_oracles, available_oracles_error_msg = get_available_oracles_and_err_msg()
-
-
-def run_non_det_Lstar(alphabet: list, sul: SUL, eq_oracle: Oracle, n_sampling=5, samples=None, stochastic=False,
-                      max_learning_rounds=None, return_data=False, print_level=2):
-    """
-    A ONFSM learning algorithm that does not rely on all weather assumption (once an input is queried, all possible
-    outputs are observed).
-
-    Args:
-
-        alphabet: input alphabet
-
-        sul: system under learning
-
-        eq_oracle: equivalence oracle
-
-        n_sampling: number of times that each cell has to be updated. If this number is to low, all-weather condition
-            will not hold and learning will not converge to the correct model. (Default value = 50)
-
-        samples: input output sequances provided to learning algorithm. List of ((input sequence), (output sequence)).
-
-        stochastic: if True, non deterministic learning will be performed but probabilities will be added to the
-        returned model, making it a stochastic Mealy machine
-
-        max_learning_rounds: if max_learning_rounds is reached, learning will stop (Default value = None)
-
-        return_data: if True, map containing all information like number of queries... will be returned
-            (Default value = False)
-
-        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
-            (Default value = 2)
-
-    Returns:
-        learned ONFSM
-
-    """
-
-    start_time = time.time()
-    eq_query_time = 0
-    learning_rounds = 0
-
-    sul = NonDeterministicSULWrapper(sul)
-
-    if samples:
-        for inputs, outputs in samples:
-            sul.cache.add_trace(inputs, outputs)
-
-    eq_oracle.sul = sul
-
-    ot = NonDetObservationTable(alphabet, sul, n_sampling)
-
-    # Keep track of last counterexample and last hypothesis size
-    # With this data we can check if the extension of the E set lead to state increase
-    last_cex = None
-
-    hypothesis = None
-
-    while True:
-        if max_learning_rounds and learning_rounds - 1 == max_learning_rounds:
-            break
-
-        ot.S = list()
-        ot.S.append((tuple(), tuple()))
-        ot.query_missing_observations()
-
-        row_to_close = ot.get_row_to_close()
-        while row_to_close is not None:
-            ot.query_missing_observations()
-            row_to_close = ot.get_row_to_close()
-            ot.clean_obs_table()
-
-        hypothesis = ot.gen_hypothesis()
-
-        if counterexample_not_valid(hypothesis, last_cex):
-            cex = sul.cache.find_cex_in_cache(hypothesis)
-
-            if cex is None:
-                learning_rounds += 1
-                # Find counterexample
-                if print_level > 1:
-                    print(f'Hypothesis {learning_rounds}: {len(hypothesis.states)} states.')
-
-                if print_level == 3:
-                    print_observation_table(ot, 'non-det')
-
-                eq_query_start = time.time()
-                cex = eq_oracle.find_cex(hypothesis)
-                eq_query_time += time.time() - eq_query_start
-
-            last_cex = cex
-        else:
-            cex = last_cex
-
-        if cex is None:
-            break
-        else:
-            cex_suffixes = all_suffixes(cex[0])
-            for suffix in cex_suffixes:
-                if suffix not in ot.E:
-                    ot.E.append(suffix)
-                    break
-
-    if stochastic:
-        hypothesis = ot.gen_hypothesis(stochastic=True)
-
-    total_time = round(time.time() - start_time, 2)
-    eq_query_time = round(eq_query_time, 2)
-    learning_time = round(total_time - eq_query_time, 2)
-
-    info = {
-        'learning_rounds': learning_rounds,
-        'automaton_size': len(hypothesis.states),
-        'queries_learning': sul.num_queries,
-        'steps_learning': sul.num_steps,
-        'queries_eq_oracle': eq_oracle.num_queries,
-        'steps_eq_oracle': eq_oracle.num_steps,
-        'learning_time': learning_time,
-        'eq_oracle_time': eq_query_time,
-        'total_time': total_time
-    }
-
-    if print_level > 0:
-        print_learning_info(info)
-
-    if return_data:
-        return hypothesis, info
-
-    return hypothesis
-
-
-def counterexample_not_valid(hypothesis, cex):
-    if cex is None:
-        return True
-    hypothesis.reset_to_initial()
-    for i, o in zip(cex[0], cex[1]):
-        out = hypothesis.step_to(i, o)
-        if out is None:
-            return False
-    return True
+import time
+
+from aalpy.base import SUL, Oracle
+from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
+from aalpy.learning_algs.non_deterministic.OnfsmObservationTable import NonDetObservationTable
+from aalpy.utils.HelperFunctions import print_learning_info, print_observation_table, \
+    get_available_oracles_and_err_msg, all_suffixes
+
+print_options = [0, 1, 2, 3]
+
+available_oracles, available_oracles_error_msg = get_available_oracles_and_err_msg()
+
+
+def run_non_det_Lstar(alphabet: list, sul: SUL, eq_oracle: Oracle, n_sampling=5, samples=None, stochastic=False,
+                      max_learning_rounds=None, return_data=False, print_level=2):
+    """
+    A ONFSM learning algorithm that does not rely on all weather assumption (once an input is queried, all possible
+    outputs are observed).
+
+    Args:
+
+        alphabet: input alphabet
+
+        sul: system under learning
+
+        eq_oracle: equivalence oracle
+
+        n_sampling: number of times that each cell has to be updated. If this number is to low, all-weather condition
+            will not hold and learning will not converge to the correct model. (Default value = 50)
+
+        samples: input output sequances provided to learning algorithm. List of ((input sequence), (output sequence)).
+
+        stochastic: if True, non deterministic learning will be performed but probabilities will be added to the
+        returned model, making it a stochastic Mealy machine
+
+        max_learning_rounds: if max_learning_rounds is reached, learning will stop (Default value = None)
+
+        return_data: if True, map containing all information like number of queries... will be returned
+            (Default value = False)
+
+        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
+            (Default value = 2)
+
+    Returns:
+        learned ONFSM
+
+    """
+
+    start_time = time.time()
+    eq_query_time = 0
+    learning_rounds = 0
+
+    sul = NonDeterministicSULWrapper(sul)
+
+    if samples:
+        for inputs, outputs in samples:
+            sul.cache.add_trace(inputs, outputs)
+
+    eq_oracle.sul = sul
+
+    ot = NonDetObservationTable(alphabet, sul, n_sampling)
+
+    # Keep track of last counterexample and last hypothesis size
+    # With this data we can check if the extension of the E set lead to state increase
+    last_cex = None
+
+    hypothesis = None
+
+    while True:
+        if max_learning_rounds and learning_rounds - 1 == max_learning_rounds:
+            break
+
+        ot.S = list()
+        ot.S.append((tuple(), tuple()))
+        ot.query_missing_observations()
+
+        row_to_close = ot.get_row_to_close()
+        while row_to_close is not None:
+            ot.query_missing_observations()
+            row_to_close = ot.get_row_to_close()
+            ot.clean_obs_table()
+
+        hypothesis = ot.gen_hypothesis()
+
+        if counterexample_not_valid(hypothesis, last_cex):
+            cex = sul.cache.find_cex_in_cache(hypothesis)
+
+            if cex is None:
+                learning_rounds += 1
+                # Find counterexample
+                if print_level > 1:
+                    print(f'Hypothesis {learning_rounds}: {len(hypothesis.states)} states.')
+
+                if print_level == 3:
+                    print_observation_table(ot, 'non-det')
+
+                eq_query_start = time.time()
+                cex = eq_oracle.find_cex(hypothesis)
+                eq_query_time += time.time() - eq_query_start
+
+            last_cex = cex
+        else:
+            cex = last_cex
+
+        if cex is None:
+            break
+        else:
+            cex_suffixes = all_suffixes(cex[0])
+            for suffix in cex_suffixes:
+                if suffix not in ot.E:
+                    ot.E.append(suffix)
+                    break
+
+    if stochastic:
+        hypothesis = ot.gen_hypothesis(stochastic=True)
+
+    total_time = round(time.time() - start_time, 2)
+    eq_query_time = round(eq_query_time, 2)
+    learning_time = round(total_time - eq_query_time, 2)
+
+    info = {
+        'learning_rounds': learning_rounds,
+        'automaton_size': len(hypothesis.states),
+        'queries_learning': sul.num_queries,
+        'steps_learning': sul.num_steps,
+        'queries_eq_oracle': eq_oracle.num_queries,
+        'steps_eq_oracle': eq_oracle.num_steps,
+        'learning_time': learning_time,
+        'eq_oracle_time': eq_query_time,
+        'total_time': total_time
+    }
+
+    if print_level > 0:
+        print_learning_info(info)
+
+    if return_data:
+        return hypothesis, info
+
+    return hypothesis
+
+
+def counterexample_not_valid(hypothesis, cex):
+    if cex is None:
+        return True
+    hypothesis.reset_to_initial()
+    for i, o in zip(cex[0], cex[1]):
+        out = hypothesis.step_to(i, o)
+        if out is None:
+            return False
+    return True
```

## aalpy/learning_algs/non_deterministic/OnfsmObservationTable.py

 * *Ordering differences only*

```diff
@@ -1,204 +1,204 @@
-from collections import Counter
-
-from aalpy.automata import Onfsm, OnfsmState, StochasticMealyState, StochasticMealyMachine
-from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
-
-
-class NonDetObservationTable:
-
-    def __init__(self, alphabet: list, sul: NonDeterministicSULWrapper, n_sampling):
-        """
-        Construction of the non-deterministic observation table.
-
-        Args:
-
-            alphabet: input alphabet
-            sul: system under learning
-            n_sampling: number of samples to be performed for each cell
-        """
-        assert alphabet is not None and sul is not None
-
-        self.alphabet = alphabet
-        self.A = [tuple([a]) for a in alphabet]
-        self.S = list()  # prefixes of S
-
-        self.E = [tuple([a]) for a in alphabet]
-
-        self.n_samples = n_sampling
-        self.closing_counter = 0
-
-        self.sul = sul
-
-        self.sampling_counter = Counter()
-
-        empty_word = tuple()
-
-        # Elements of S are in form that is presented in 'Learning Finite State Models of Observable Nondeterministic
-        # Systems in a Testing Context'. Each element of S is a (inputs, outputs) tuple, where first element of the
-        # tuple are inputs and second element of the tuple are outputs associated with inputs.
-        self.S.append((empty_word, empty_word))
-
-        self.pruned_nodes = set()
-
-    def get_row_to_close(self):
-        """
-        Get row for that need to be closed.
-
-        Returns:
-
-            row that will be moved to S set and closed
-        """
-
-        s_rows = set()
-        update_S_dot_A = self.get_extended_S()
-
-        for s in self.S.copy():
-            s_rows.add(self.row_to_hashable(s))
-
-        for t in update_S_dot_A:
-            row_t = self.row_to_hashable(t)
-            if row_t not in s_rows:
-                self.closing_counter += 1
-                self.S.append(t)
-                return t
-
-        self.closing_counter = 0
-        return None
-
-    def get_extended_S(self, row_prefix=None):
-        """
-        Helper generator function that returns extended S, or S.A set.
-        For all values in the cell, create a new row where inputs is parent input plus element of alphabet, and
-        output is parent output plus value in cell.
-
-        Returns:
-
-            extended S set.
-        """
-
-        rows = self.S if row_prefix is None else [row_prefix]
-
-        S_dot_A = []
-        for row in rows:
-            for a in self.A:
-                trace = self.sul.cache.get_all_traces(row, a)
-
-                for t in trace:
-                    new_row = (row[0] + a, row[1] + (t[-1],))
-                    if new_row not in self.S:
-                        S_dot_A.append(new_row)
-        return S_dot_A
-
-    def query_missing_observations(self, s=None, e=None):
-        s_set = s if s is not None else self.S + self.get_extended_S()
-        e_set = e if e is not None else self.E
-
-        for s in s_set:
-            for e in e_set:
-                while self.sul.cache.get_s_e_sampling_frequency(s, e) < self.n_samples:
-                    self.sul.query(s[0] + e)
-
-    def row_to_hashable(self, row_prefix):
-        """
-        Creates the hashable representation of the row. Frozenset is used as the order of element in each cell does not
-        matter
-
-        Args:
-
-            row_prefix: prefix of the row in the observation table
-
-        Returns:
-
-            hashable representation of the row
-
-        """
-        row_repr = tuple()
-
-        for e in self.E:
-            cell = self.sul.cache.get_all_traces(row_prefix, e)
-            while cell is None:
-                self.query_missing_observations([row_prefix], [e])
-                cell = self.sul.cache.get_all_traces(row_prefix, e)
-
-            row_repr += (frozenset(cell),)
-
-        return row_repr
-
-    def clean_obs_table(self):
-        """
-        Moves duplicates from S to S_dot_A. The entries in S_dot_A which are based on the moved row get deleted.
-        The table will be smaller and more efficient.
-
-        """
-
-        tmp_S = self.S.copy()
-        tmp_both_S = self.S + self.get_extended_S()
-        hashed_rows_from_s = set()
-
-        tmp_S.sort(key=lambda t: len(t[0]))
-
-        for s in tmp_S:
-            hashed_s_row = self.row_to_hashable(s)
-            if hashed_s_row in hashed_rows_from_s:
-                if s in self.S:
-                    self.S.remove(s)
-                size = len(s[0])
-                for row_prefix in tmp_both_S:
-                    s_both_row = (row_prefix[0][:size], row_prefix[1][:size])
-                    if s != row_prefix and s == s_both_row:
-                        if row_prefix in self.S:
-                            self.S.remove(row_prefix)
-            else:
-                hashed_rows_from_s.add(hashed_s_row)
-
-    def gen_hypothesis(self, stochastic=False):
-        """
-        Generate automaton based on the values found in the observation table.
-        If stochastic is set to True, returns a Stochastic Mealy Machine.
-
-        Returns:
-
-            Current hypothesis
-        """
-
-        state_distinguish = dict()
-        states_dict = dict()
-        initial = None
-
-        stateCounter = 0
-
-        state_class = OnfsmState if not stochastic else StochasticMealyState
-        model_class = Onfsm if not stochastic else StochasticMealyMachine
-
-        for prefix in self.S:
-            state_id = f's{stateCounter}'
-            states_dict[prefix] = state_class(state_id)
-
-            states_dict[prefix].prefix = prefix
-            state_distinguish[self.row_to_hashable(prefix)] = states_dict[prefix]
-
-            if prefix == self.S[0]:
-                initial = states_dict[prefix]
-            stateCounter += 1
-
-        for prefix in self.S:
-            for a in self.A:
-                observations_in_cell = self.sul.cache.get_all_traces(prefix, a)
-                probability_distribution = None
-                if stochastic:
-                    probability_distribution = self.sul.cache.get_sampling_distributions(prefix, a[0])
-                for obs in observations_in_cell:
-                    reached_row = (prefix[0] + a, prefix[1] + (obs[-1],))
-                    destination = state_distinguish[self.row_to_hashable(reached_row)]
-                    assert destination
-                    if not stochastic:
-                        states_dict[prefix].transitions[a[0]].append((obs[-1], destination))
-                    else:
-                        states_dict[prefix].transitions[a[0]].append((destination, obs[-1],
-                                                                      probability_distribution[obs[-1]]))
-
-        assert initial
-        automaton = model_class(initial, [s for s in states_dict.values()])
-        automaton.characterization_set = self.E
-
-        return automaton
+from collections import Counter
+
+from aalpy.automata import Onfsm, OnfsmState, StochasticMealyState, StochasticMealyMachine
+from aalpy.learning_algs.non_deterministic.NonDeterministicSULWrapper import NonDeterministicSULWrapper
+
+
+class NonDetObservationTable:
+
+    def __init__(self, alphabet: list, sul: NonDeterministicSULWrapper, n_sampling):
+        """
+        Construction of the non-deterministic observation table.
+
+        Args:
+
+            alphabet: input alphabet
+            sul: system under learning
+            n_sampling: number of samples to be performed for each cell
+        """
+        assert alphabet is not None and sul is not None
+
+        self.alphabet = alphabet
+        self.A = [tuple([a]) for a in alphabet]
+        self.S = list()  # prefixes of S
+
+        self.E = [tuple([a]) for a in alphabet]
+
+        self.n_samples = n_sampling
+        self.closing_counter = 0
+
+        self.sul = sul
+
+        self.sampling_counter = Counter()
+
+        empty_word = tuple()
+
+        # Elements of S are in form that is presented in 'Learning Finite State Models of Observable Nondeterministic
+        # Systems in a Testing Context'. Each element of S is a (inputs, outputs) tuple, where first element of the
+        # tuple are inputs and second element of the tuple are outputs associated with inputs.
+        self.S.append((empty_word, empty_word))
+
+        self.pruned_nodes = set()
+
+    def get_row_to_close(self):
+        """
+        Get row for that need to be closed.
+
+        Returns:
+
+            row that will be moved to S set and closed
+        """
+
+        s_rows = set()
+        update_S_dot_A = self.get_extended_S()
+
+        for s in self.S.copy():
+            s_rows.add(self.row_to_hashable(s))
+
+        for t in update_S_dot_A:
+            row_t = self.row_to_hashable(t)
+            if row_t not in s_rows:
+                self.closing_counter += 1
+                self.S.append(t)
+                return t
+
+        self.closing_counter = 0
+        return None
+
+    def get_extended_S(self, row_prefix=None):
+        """
+        Helper generator function that returns extended S, or S.A set.
+        For all values in the cell, create a new row where inputs is parent input plus element of alphabet, and
+        output is parent output plus value in cell.
+
+        Returns:
+
+            extended S set.
+        """
+
+        rows = self.S if row_prefix is None else [row_prefix]
+
+        S_dot_A = []
+        for row in rows:
+            for a in self.A:
+                trace = self.sul.cache.get_all_traces(row, a)
+
+                for t in trace:
+                    new_row = (row[0] + a, row[1] + (t[-1],))
+                    if new_row not in self.S:
+                        S_dot_A.append(new_row)
+        return S_dot_A
+
+    def query_missing_observations(self, s=None, e=None):
+        s_set = s if s is not None else self.S + self.get_extended_S()
+        e_set = e if e is not None else self.E
+
+        for s in s_set:
+            for e in e_set:
+                while self.sul.cache.get_s_e_sampling_frequency(s, e) < self.n_samples:
+                    self.sul.query(s[0] + e)
+
+    def row_to_hashable(self, row_prefix):
+        """
+        Creates the hashable representation of the row. Frozenset is used as the order of element in each cell does not
+        matter
+
+        Args:
+
+            row_prefix: prefix of the row in the observation table
+
+        Returns:
+
+            hashable representation of the row
+
+        """
+        row_repr = tuple()
+
+        for e in self.E:
+            cell = self.sul.cache.get_all_traces(row_prefix, e)
+            while cell is None:
+                self.query_missing_observations([row_prefix], [e])
+                cell = self.sul.cache.get_all_traces(row_prefix, e)
+
+            row_repr += (frozenset(cell),)
+
+        return row_repr
+
+    def clean_obs_table(self):
+        """
+        Moves duplicates from S to S_dot_A. The entries in S_dot_A which are based on the moved row get deleted.
+        The table will be smaller and more efficient.
+
+        """
+
+        tmp_S = self.S.copy()
+        tmp_both_S = self.S + self.get_extended_S()
+        hashed_rows_from_s = set()
+
+        tmp_S.sort(key=lambda t: len(t[0]))
+
+        for s in tmp_S:
+            hashed_s_row = self.row_to_hashable(s)
+            if hashed_s_row in hashed_rows_from_s:
+                if s in self.S:
+                    self.S.remove(s)
+                size = len(s[0])
+                for row_prefix in tmp_both_S:
+                    s_both_row = (row_prefix[0][:size], row_prefix[1][:size])
+                    if s != row_prefix and s == s_both_row:
+                        if row_prefix in self.S:
+                            self.S.remove(row_prefix)
+            else:
+                hashed_rows_from_s.add(hashed_s_row)
+
+    def gen_hypothesis(self, stochastic=False):
+        """
+        Generate automaton based on the values found in the observation table.
+        If stochastic is set to True, returns a Stochastic Mealy Machine.
+
+        Returns:
+
+            Current hypothesis
+        """
+
+        state_distinguish = dict()
+        states_dict = dict()
+        initial = None
+
+        stateCounter = 0
+
+        state_class = OnfsmState if not stochastic else StochasticMealyState
+        model_class = Onfsm if not stochastic else StochasticMealyMachine
+
+        for prefix in self.S:
+            state_id = f's{stateCounter}'
+            states_dict[prefix] = state_class(state_id)
+
+            states_dict[prefix].prefix = prefix
+            state_distinguish[self.row_to_hashable(prefix)] = states_dict[prefix]
+
+            if prefix == self.S[0]:
+                initial = states_dict[prefix]
+            stateCounter += 1
+
+        for prefix in self.S:
+            for a in self.A:
+                observations_in_cell = self.sul.cache.get_all_traces(prefix, a)
+                probability_distribution = None
+                if stochastic:
+                    probability_distribution = self.sul.cache.get_sampling_distributions(prefix, a[0])
+                for obs in observations_in_cell:
+                    reached_row = (prefix[0] + a, prefix[1] + (obs[-1],))
+                    destination = state_distinguish[self.row_to_hashable(reached_row)]
+                    assert destination
+                    if not stochastic:
+                        states_dict[prefix].transitions[a[0]].append((obs[-1], destination))
+                    else:
+                        states_dict[prefix].transitions[a[0]].append((destination, obs[-1],
+                                                                      probability_distribution[obs[-1]]))
+
+        assert initial
+        automaton = model_class(initial, [s for s in states_dict.values()])
+        automaton.characterization_set = self.E
+
+        return automaton
```

## aalpy/learning_algs/non_deterministic/TraceTree.py

 * *Ordering differences only*

```diff
@@ -1,203 +1,203 @@
-from collections import defaultdict
-
-
-class Node:
-    __slots__ = ['output', 'children', 'parent', 'frequency_counter']
-
-    def __init__(self, output):
-        self.output = output
-        self.children = defaultdict(list)
-        self.parent = None
-
-        # frq counter
-        self.frequency_counter = 0
-
-    def get_child(self, inp, out):
-        """
-        Args:
-          inp:
-          out:
-
-        Returns:
-
-        """
-        return next((child for child in self.children[inp] if child.output == out), None)
-
-    def get_prefix(self):
-        prefix = ()
-        curr_node = self
-        while curr_node.parent is not None:
-            prefix = (curr_node.output,) + prefix
-            curr_node = curr_node.parent
-        return prefix
-
-
-class TraceTree:
-    """
-    Tree used for keeping track of seen observations.
-    """
-
-    def __init__(self):
-        self.root_node = Node(None)
-        self.curr_node = None
-
-    def reset(self):
-        self.curr_node = self.root_node
-
-    def add_to_tree(self, inp, out):
-        """
-        Adds new element to tree and makes it the current node
-
-        Args:
-
-          inp: Input
-          out: Output
-
-        """
-        if inp not in self.curr_node.children.keys() or \
-                out not in {child.output for child in self.curr_node.children[inp]}:
-            node = Node(out)
-            self.curr_node.children[inp].append(node)
-            node.parent = self.curr_node
-
-        self.curr_node = self.curr_node.get_child(inp, out)
-        self.curr_node.frequency_counter += 1
-
-    def add_trace(self, inputs, outputs):
-        self.reset()
-        for i, o in zip(inputs, outputs):
-            self.add_to_tree(i, o)
-
-    def get_to_node(self, inputs, outputs):
-        """
-        Follows the path described by inp and out and returns the node which is reached
-
-        Args:
-          inputs: Inputs
-          outputs: Outputs
-
-        Returns:
-
-          Node that is reached when following the given input and output through the tree
-        """
-        curr_node = self.root_node
-        for i, o in zip(inputs, outputs):
-            node = curr_node.get_child(i, o)
-            if node is None:
-                return None
-            curr_node = node
-
-        return curr_node
-
-    def get_all_traces(self, prefix, e=None):
-        """
-
-        Args:
-
-          prefix: prefix
-          e: List of inputs
-
-        Returns:
-
-          Traces of outputs corresponding to the input-sequence given by e
-        """
-
-        if not prefix or not e:
-            return []
-
-        curr_node = self.root_node
-        for i, o in zip(prefix[0], prefix[1]):
-            curr_node = curr_node.get_child(i, o)
-            if curr_node is None:
-                return []
-
-        queue = [(curr_node, 0)]
-        reached_nodes = []
-        while queue:
-            node, depth = queue.pop(0)
-            if depth == len(e):
-                reached_nodes.append(node)
-            else:
-                children_with_same_input = node.children[e[depth]]
-                for c in children_with_same_input:
-                    queue.append((c, depth + 1))
-
-        cell = [node.get_prefix()[-len(e):] for node in reached_nodes]
-        return cell
-
-    def get_table(self, s, e):
-        """
-        Generates a table from the tree
-
-        Args:
-          s: rows from S, S_dot_A, or both which should be presented in the table.
-          e: E
-
-        Returns:
-          a table in a format that can be used for printing.
-        """
-        result = {}
-        for prefix in s:
-            result[prefix] = {}
-
-            for inp in e:
-                result[prefix][inp] = self.get_all_traces(prefix, inp)
-
-        return result
-
-    def find_cex_in_cache(self, hypothesis):
-
-        queue = [(self.root_node, tuple())]
-        while queue:
-            curr_node, path = queue.pop(0)
-
-            if path:
-                hypothesis.reset_to_initial()
-                inputs, outputs = [], []
-                for i, o in zip(path[0::2], path[1::2]):
-                    inputs.append(i)
-                    outputs.append(o)
-                    out = hypothesis.step_to(i, o)
-                    if out is None:
-                        return inputs, outputs
-            for inp in curr_node.children.keys():
-                children = curr_node.children[inp]
-                for child in children:
-                    # if curr_node.frequency_counter[(inp, child_out)] >= threshold:
-                    queue.append((child, path + (inp, child.output)))
-
-        return None
-
-    def get_s_e_sampling_frequency(self, prefix, suffix):
-        sampling_frequency = 0
-        curr_node = self.root_node
-        for i, o in zip(prefix[0], prefix[1]):
-            curr_node = curr_node.get_child(i, o)
-            if curr_node is None:
-                return 0
-
-        queue = [(curr_node, 0)]
-        while queue:
-            node, depth = queue.pop(0)
-            children_with_same_input = node.children[suffix[depth]]
-            if depth == len(suffix) - 1:
-                for c in children_with_same_input:
-                    sampling_frequency += c.frequency_counter
-            else:
-                for c in children_with_same_input:
-                    queue.append((c, depth + 1))
-
-        return sampling_frequency
-
-    def get_sampling_distributions(self, prefix, input_from_alphabet):
-        sampling_distribution = {}
-        curr_node = self.root_node
-        for i, o in zip(prefix[0], prefix[1]):
-            curr_node = curr_node.get_child(i, o)
-
-        children = curr_node.children[input_from_alphabet]
-        sampling_sum = sum(c.frequency_counter for c in children)
-        for c in children:
-            sampling_distribution[c.output] = c.frequency_counter / sampling_sum
-
-        return sampling_distribution
+from collections import defaultdict
+
+
+class Node:
+    __slots__ = ['output', 'children', 'parent', 'frequency_counter']
+
+    def __init__(self, output):
+        self.output = output
+        self.children = defaultdict(list)
+        self.parent = None
+
+        # frq counter
+        self.frequency_counter = 0
+
+    def get_child(self, inp, out):
+        """
+        Args:
+          inp:
+          out:
+
+        Returns:
+
+        """
+        return next((child for child in self.children[inp] if child.output == out), None)
+
+    def get_prefix(self):
+        prefix = ()
+        curr_node = self
+        while curr_node.parent is not None:
+            prefix = (curr_node.output,) + prefix
+            curr_node = curr_node.parent
+        return prefix
+
+
+class TraceTree:
+    """
+    Tree used for keeping track of seen observations.
+    """
+
+    def __init__(self):
+        self.root_node = Node(None)
+        self.curr_node = None
+
+    def reset(self):
+        self.curr_node = self.root_node
+
+    def add_to_tree(self, inp, out):
+        """
+        Adds new element to tree and makes it the current node
+
+        Args:
+
+          inp: Input
+          out: Output
+
+        """
+        if inp not in self.curr_node.children.keys() or \
+                out not in {child.output for child in self.curr_node.children[inp]}:
+            node = Node(out)
+            self.curr_node.children[inp].append(node)
+            node.parent = self.curr_node
+
+        self.curr_node = self.curr_node.get_child(inp, out)
+        self.curr_node.frequency_counter += 1
+
+    def add_trace(self, inputs, outputs):
+        self.reset()
+        for i, o in zip(inputs, outputs):
+            self.add_to_tree(i, o)
+
+    def get_to_node(self, inputs, outputs):
+        """
+        Follows the path described by inp and out and returns the node which is reached
+
+        Args:
+          inputs: Inputs
+          outputs: Outputs
+
+        Returns:
+
+          Node that is reached when following the given input and output through the tree
+        """
+        curr_node = self.root_node
+        for i, o in zip(inputs, outputs):
+            node = curr_node.get_child(i, o)
+            if node is None:
+                return None
+            curr_node = node
+
+        return curr_node
+
+    def get_all_traces(self, prefix, e=None):
+        """
+
+        Args:
+
+          prefix: prefix
+          e: List of inputs
+
+        Returns:
+
+          Traces of outputs corresponding to the input-sequence given by e
+        """
+
+        if not prefix or not e:
+            return []
+
+        curr_node = self.root_node
+        for i, o in zip(prefix[0], prefix[1]):
+            curr_node = curr_node.get_child(i, o)
+            if curr_node is None:
+                return []
+
+        queue = [(curr_node, 0)]
+        reached_nodes = []
+        while queue:
+            node, depth = queue.pop(0)
+            if depth == len(e):
+                reached_nodes.append(node)
+            else:
+                children_with_same_input = node.children[e[depth]]
+                for c in children_with_same_input:
+                    queue.append((c, depth + 1))
+
+        cell = [node.get_prefix()[-len(e):] for node in reached_nodes]
+        return cell
+
+    def get_table(self, s, e):
+        """
+        Generates a table from the tree
+
+        Args:
+          s: rows from S, S_dot_A, or both which should be presented in the table.
+          e: E
+
+        Returns:
+          a table in a format that can be used for printing.
+        """
+        result = {}
+        for prefix in s:
+            result[prefix] = {}
+
+            for inp in e:
+                result[prefix][inp] = self.get_all_traces(prefix, inp)
+
+        return result
+
+    def find_cex_in_cache(self, hypothesis):
+
+        queue = [(self.root_node, tuple())]
+        while queue:
+            curr_node, path = queue.pop(0)
+
+            if path:
+                hypothesis.reset_to_initial()
+                inputs, outputs = [], []
+                for i, o in zip(path[0::2], path[1::2]):
+                    inputs.append(i)
+                    outputs.append(o)
+                    out = hypothesis.step_to(i, o)
+                    if out is None:
+                        return inputs, outputs
+            for inp in curr_node.children.keys():
+                children = curr_node.children[inp]
+                for child in children:
+                    # if curr_node.frequency_counter[(inp, child_out)] >= threshold:
+                    queue.append((child, path + (inp, child.output)))
+
+        return None
+
+    def get_s_e_sampling_frequency(self, prefix, suffix):
+        sampling_frequency = 0
+        curr_node = self.root_node
+        for i, o in zip(prefix[0], prefix[1]):
+            curr_node = curr_node.get_child(i, o)
+            if curr_node is None:
+                return 0
+
+        queue = [(curr_node, 0)]
+        while queue:
+            node, depth = queue.pop(0)
+            children_with_same_input = node.children[suffix[depth]]
+            if depth == len(suffix) - 1:
+                for c in children_with_same_input:
+                    sampling_frequency += c.frequency_counter
+            else:
+                for c in children_with_same_input:
+                    queue.append((c, depth + 1))
+
+        return sampling_frequency
+
+    def get_sampling_distributions(self, prefix, input_from_alphabet):
+        sampling_distribution = {}
+        curr_node = self.root_node
+        for i, o in zip(prefix[0], prefix[1]):
+            curr_node = curr_node.get_child(i, o)
+
+        children = curr_node.children[input_from_alphabet]
+        sampling_sum = sum(c.frequency_counter for c in children)
+        for c in children:
+            sampling_distribution[c.output] = c.frequency_counter / sampling_sum
+
+        return sampling_distribution
```

## aalpy/learning_algs/stochastic/DifferenceChecker.py

 * *Ordering differences only*

```diff
@@ -1,177 +1,177 @@
-from abc import ABC, abstractmethod
-from math import sqrt, log
-
-chi2_table = dict()
-
-chi2_table[0.95] = \
-    dict([(1, 3.841458820694124), (2, 5.991464547107979), (3, 7.814727903251179), (4, 9.487729036781154),
-          (5, 11.070497693516351), (6, 12.591587243743977), (7, 14.067140449340169), (8, 15.50731305586545),
-          (9, 16.918977604620448), (10, 18.307038053275146), (11, 19.67513757268249), (12, 21.02606981748307),
-          (13, 22.362032494826934), (14, 23.684791304840576), (15, 24.995790139728616), (16, 26.29622760486423),
-          (17, 27.58711163827534), (18, 28.869299430392623), (19, 30.14352720564616), (20, 31.410432844230918)])
-chi2_table[0.99] = \
-    dict([(1, 6.6348966010212145), (2, 9.21034037197618), (3, 11.344866730144373), (4, 13.276704135987622),
-          (5, 15.08627246938899), (6, 16.811893829770927), (7, 18.475306906582357), (8, 20.090235029663233),
-          (9, 21.665994333461924), (10, 23.209251158954356), (11, 24.724970311318277), (12, 26.216967305535853),
-          (13, 27.68824961045705), (14, 29.141237740672796), (15, 30.57791416689249), (16, 31.999926908815176),
-          (17, 33.40866360500461), (18, 34.805305734705065), (19, 36.19086912927004), (20, 37.56623478662507)])
-
-chi2_table[0.999] = \
-    dict([(1, 10.827566170662733), (2, 13.815510557964274), (3, 16.26623619623813), (4, 18.46682695290317),
-          (5, 20.515005652432873), (6, 22.457744484825323), (7, 24.321886347856854), (8, 26.12448155837614),
-          (9, 27.877164871256568), (10, 29.58829844507442), (11, 31.264133620239985), (12, 32.90949040736021),
-          (13, 34.52817897487089), (14, 36.12327368039813), (15, 37.69729821835383), (16, 39.252354790768464),
-          (17, 40.79021670690253), (18, 42.31239633167996), (19, 43.82019596451753), (20, 45.31474661812586)])
-
-
-class DifferenceChecker(ABC):
-
-    @abstractmethod
-    def are_cells_different(self, c1: dict, c2: dict, **kwargs) -> bool:
-        pass
-
-    def difference_value(self, c1: dict, c2: dict):
-        return None
-
-    def use_diff_value(self):
-        return False
-
-
-class HoeffdingChecker(DifferenceChecker):
-
-    def __init__(self, alpha=0.05):
-        self.alpha = alpha
-
-    def are_cells_different(self, c1: dict, c2: dict, **kwargs) -> bool:
-        if c1.keys() != c2.keys():
-            return True
-
-        n1 = sum(c1.values())
-        n2 = sum(c2.values())
-
-        if n1 > 0 and n2 > 0:
-            for o in c1.keys():
-                if abs(c1[o] / n1 - c2[o] / n2) > \
-                        ((sqrt(1 / n1) + sqrt(1 / n2)) * sqrt(0.5 * log(2 / self.alpha))):
-                    return True
-        return False
-
-
-def compute_epsilon(alpha1, n1):
-    epsilon1 = sqrt((1. / (2 * n1)) * log(2. / alpha1))
-    return epsilon1
-
-
-class AdvancedHoeffdingChecker(DifferenceChecker):
-    def __init__(self, alpha=0.05, use_diff=False):
-        self.alpha = alpha
-        self.use_diff = use_diff
-
-    def are_cells_different(self, c1: dict, c2: dict, **kwargs) -> bool:
-        n1 = sum(c1.values())
-        n2 = sum(c2.values())
-
-        if n1 > 0 and n2 > 0:
-            for o in set(c1.keys()).union(c2.keys()):
-                c1o = c1[o] if o in c1.keys() else 0
-                c2o = c2[o] if o in c2.keys() else 0
-                alpha1 = self.alpha
-                alpha2 = self.alpha
-                epsilon1 = compute_epsilon(alpha1, n1)
-                epsilon2 = compute_epsilon(alpha2, n2)
-
-                if abs(c1o / n1 - c2o / n2) > epsilon1 + epsilon2:
-                    return True
-        return False
-
-    def use_diff_value(self):
-        return self.use_diff
-
-    def difference_value(self, c1_out_freq: dict, c2_out_freq: dict):
-        n1 = 0 if not c1_out_freq else sum(c1_out_freq.values())
-        n2 = 0 if not c2_out_freq else sum(c2_out_freq.values())
-
-        if n1 > 0 and n2 > 0:
-            dist = 0
-            for o in set(c1_out_freq.keys()).union(c2_out_freq.keys()):
-                c1o = c1_out_freq[o] if o in c1_out_freq.keys() else 0
-                c2o = c2_out_freq[o] if o in c2_out_freq.keys() else 0
-                dist += abs(c1o / n1 - c2o / n2)
-            return dist
-        elif n1 > 0 or n2 > 0:
-            alpha1 = self.alpha
-            alpha2 = self.alpha
-            epsilon1 = compute_epsilon(alpha1, max(n1, n2))
-            epsilon2 = compute_epsilon(alpha2, max(n1, n2))
-            return epsilon1 + epsilon2
-        else:
-            return 0
-
-
-class ChiSquareChecker(DifferenceChecker):
-
-    def __init__(self, alpha=0.001, use_diff_value=False):
-        self.alpha = alpha
-        self.chi2_cache = dict()
-        if 1 - self.alpha not in chi2_table.keys():
-            raise ValueError("alpha must be in [0.01,0.001,0.05]")
-        self.chi2_values = chi2_table[1 - self.alpha]
-        self.use_diff = use_diff_value
-
-    def are_cells_different(self, c1_out_freq: dict, c2_out_freq: dict, **kwargs) -> bool:
-        # chi square test for homogeneity (see, for instance: https://online.stat.psu.edu/stat415/lesson/17/17.1)
-        if not c1_out_freq or not c2_out_freq:
-            return False
-        keys = list(set(c1_out_freq.keys()).union(c2_out_freq.keys()))
-        dof = len(keys) - 1
-        if dof == 0:
-            return False
-        shared_keys = set(c1_out_freq.keys()).intersection(c2_out_freq.keys())
-        if len(shared_keys) == 0:
-            # if the supports of the tested frequencies are completely then chi2 makes no sense, use the Hoeffding test
-            # to determine if there are enough observations for a difference
-            hoeffding_checker = AdvancedHoeffdingChecker()
-            return hoeffding_checker.are_cells_different(c1_out_freq, c2_out_freq)
-
-        Q = self.compute_Q(c1_out_freq, c2_out_freq, keys)
-        if dof not in self.chi2_values.keys():
-            raise ValueError("Too many possible outputs, chi2 table needs to be extended.")
-        else:
-            chi2_val = self.chi2_values[dof]
-
-        return Q >= chi2_val
-
-    def use_diff_value(self):
-        return self.use_diff
-
-    def difference_value(self, c1_out_freq: dict, c2_out_freq: dict):
-        if not c1_out_freq or not c2_out_freq:
-            # return a value on the threshold if we don't have information
-            c1_outs = set(c1_out_freq.keys()) if c1_out_freq else set()
-            c2_outs = set(c2_out_freq.keys()) if c2_out_freq else set()
-            nr_outs = len(c1_outs.union(c2_outs))
-            return self.chi2_values[max(1, nr_outs)]
-        keys = list(set(c1_out_freq.keys()).union(c2_out_freq.keys()))
-        shared_keys = set(c1_out_freq.keys()).intersection(c2_out_freq.keys())
-        dof = len(keys) - 1
-        if dof == 0:
-            return 0
-        Q = self.compute_Q(c1_out_freq, c2_out_freq, keys)
-        return Q
-
-    def compute_Q(self, c1_out_freq, c2_out_freq, keys):
-        n_1 = sum(c1_out_freq.values())
-        n_2 = sum(c2_out_freq.values())
-
-        Q = 0
-        default_val = 0
-        yates_correction = -0.5 if len(keys) == 2 and \
-                                   any(c1_out_freq.get(k, 0) < 5 or c2_out_freq.get(k, 0) < 5 for k in keys) else 0
-        for k in keys:
-            p_hat_k = float(c1_out_freq.get(k, default_val) + c2_out_freq.get(k, default_val)) / (n_1 + n_2)
-            q_1_k = float(((abs(c1_out_freq.get(k, default_val) - n_1 * p_hat_k)) + yates_correction) ** 2) / (
-                    n_1 * p_hat_k)
-            q_2_k = float(((abs(c2_out_freq.get(k, default_val) - n_2 * p_hat_k)) + yates_correction) ** 2) / (
-                    n_2 * p_hat_k)
-            Q = Q + q_1_k + q_2_k
-        return Q
+from abc import ABC, abstractmethod
+from math import sqrt, log
+
+chi2_table = dict()
+
+chi2_table[0.95] = \
+    dict([(1, 3.841458820694124), (2, 5.991464547107979), (3, 7.814727903251179), (4, 9.487729036781154),
+          (5, 11.070497693516351), (6, 12.591587243743977), (7, 14.067140449340169), (8, 15.50731305586545),
+          (9, 16.918977604620448), (10, 18.307038053275146), (11, 19.67513757268249), (12, 21.02606981748307),
+          (13, 22.362032494826934), (14, 23.684791304840576), (15, 24.995790139728616), (16, 26.29622760486423),
+          (17, 27.58711163827534), (18, 28.869299430392623), (19, 30.14352720564616), (20, 31.410432844230918)])
+chi2_table[0.99] = \
+    dict([(1, 6.6348966010212145), (2, 9.21034037197618), (3, 11.344866730144373), (4, 13.276704135987622),
+          (5, 15.08627246938899), (6, 16.811893829770927), (7, 18.475306906582357), (8, 20.090235029663233),
+          (9, 21.665994333461924), (10, 23.209251158954356), (11, 24.724970311318277), (12, 26.216967305535853),
+          (13, 27.68824961045705), (14, 29.141237740672796), (15, 30.57791416689249), (16, 31.999926908815176),
+          (17, 33.40866360500461), (18, 34.805305734705065), (19, 36.19086912927004), (20, 37.56623478662507)])
+
+chi2_table[0.999] = \
+    dict([(1, 10.827566170662733), (2, 13.815510557964274), (3, 16.26623619623813), (4, 18.46682695290317),
+          (5, 20.515005652432873), (6, 22.457744484825323), (7, 24.321886347856854), (8, 26.12448155837614),
+          (9, 27.877164871256568), (10, 29.58829844507442), (11, 31.264133620239985), (12, 32.90949040736021),
+          (13, 34.52817897487089), (14, 36.12327368039813), (15, 37.69729821835383), (16, 39.252354790768464),
+          (17, 40.79021670690253), (18, 42.31239633167996), (19, 43.82019596451753), (20, 45.31474661812586)])
+
+
+class DifferenceChecker(ABC):
+
+    @abstractmethod
+    def are_cells_different(self, c1: dict, c2: dict, **kwargs) -> bool:
+        pass
+
+    def difference_value(self, c1: dict, c2: dict):
+        return None
+
+    def use_diff_value(self):
+        return False
+
+
+class HoeffdingChecker(DifferenceChecker):
+
+    def __init__(self, alpha=0.05):
+        self.alpha = alpha
+
+    def are_cells_different(self, c1: dict, c2: dict, **kwargs) -> bool:
+        if c1.keys() != c2.keys():
+            return True
+
+        n1 = sum(c1.values())
+        n2 = sum(c2.values())
+
+        if n1 > 0 and n2 > 0:
+            for o in c1.keys():
+                if abs(c1[o] / n1 - c2[o] / n2) > \
+                        ((sqrt(1 / n1) + sqrt(1 / n2)) * sqrt(0.5 * log(2 / self.alpha))):
+                    return True
+        return False
+
+
+def compute_epsilon(alpha1, n1):
+    epsilon1 = sqrt((1. / (2 * n1)) * log(2. / alpha1))
+    return epsilon1
+
+
+class AdvancedHoeffdingChecker(DifferenceChecker):
+    def __init__(self, alpha=0.05, use_diff=False):
+        self.alpha = alpha
+        self.use_diff = use_diff
+
+    def are_cells_different(self, c1: dict, c2: dict, **kwargs) -> bool:
+        n1 = sum(c1.values())
+        n2 = sum(c2.values())
+
+        if n1 > 0 and n2 > 0:
+            for o in set(c1.keys()).union(c2.keys()):
+                c1o = c1[o] if o in c1.keys() else 0
+                c2o = c2[o] if o in c2.keys() else 0
+                alpha1 = self.alpha
+                alpha2 = self.alpha
+                epsilon1 = compute_epsilon(alpha1, n1)
+                epsilon2 = compute_epsilon(alpha2, n2)
+
+                if abs(c1o / n1 - c2o / n2) > epsilon1 + epsilon2:
+                    return True
+        return False
+
+    def use_diff_value(self):
+        return self.use_diff
+
+    def difference_value(self, c1_out_freq: dict, c2_out_freq: dict):
+        n1 = 0 if not c1_out_freq else sum(c1_out_freq.values())
+        n2 = 0 if not c2_out_freq else sum(c2_out_freq.values())
+
+        if n1 > 0 and n2 > 0:
+            dist = 0
+            for o in set(c1_out_freq.keys()).union(c2_out_freq.keys()):
+                c1o = c1_out_freq[o] if o in c1_out_freq.keys() else 0
+                c2o = c2_out_freq[o] if o in c2_out_freq.keys() else 0
+                dist += abs(c1o / n1 - c2o / n2)
+            return dist
+        elif n1 > 0 or n2 > 0:
+            alpha1 = self.alpha
+            alpha2 = self.alpha
+            epsilon1 = compute_epsilon(alpha1, max(n1, n2))
+            epsilon2 = compute_epsilon(alpha2, max(n1, n2))
+            return epsilon1 + epsilon2
+        else:
+            return 0
+
+
+class ChiSquareChecker(DifferenceChecker):
+
+    def __init__(self, alpha=0.001, use_diff_value=False):
+        self.alpha = alpha
+        self.chi2_cache = dict()
+        if 1 - self.alpha not in chi2_table.keys():
+            raise ValueError("alpha must be in [0.01,0.001,0.05]")
+        self.chi2_values = chi2_table[1 - self.alpha]
+        self.use_diff = use_diff_value
+
+    def are_cells_different(self, c1_out_freq: dict, c2_out_freq: dict, **kwargs) -> bool:
+        # chi square test for homogeneity (see, for instance: https://online.stat.psu.edu/stat415/lesson/17/17.1)
+        if not c1_out_freq or not c2_out_freq:
+            return False
+        keys = list(set(c1_out_freq.keys()).union(c2_out_freq.keys()))
+        dof = len(keys) - 1
+        if dof == 0:
+            return False
+        shared_keys = set(c1_out_freq.keys()).intersection(c2_out_freq.keys())
+        if len(shared_keys) == 0:
+            # if the supports of the tested frequencies are completely then chi2 makes no sense, use the Hoeffding test
+            # to determine if there are enough observations for a difference
+            hoeffding_checker = AdvancedHoeffdingChecker()
+            return hoeffding_checker.are_cells_different(c1_out_freq, c2_out_freq)
+
+        Q = self.compute_Q(c1_out_freq, c2_out_freq, keys)
+        if dof not in self.chi2_values.keys():
+            raise ValueError("Too many possible outputs, chi2 table needs to be extended.")
+        else:
+            chi2_val = self.chi2_values[dof]
+
+        return Q >= chi2_val
+
+    def use_diff_value(self):
+        return self.use_diff
+
+    def difference_value(self, c1_out_freq: dict, c2_out_freq: dict):
+        if not c1_out_freq or not c2_out_freq:
+            # return a value on the threshold if we don't have information
+            c1_outs = set(c1_out_freq.keys()) if c1_out_freq else set()
+            c2_outs = set(c2_out_freq.keys()) if c2_out_freq else set()
+            nr_outs = len(c1_outs.union(c2_outs))
+            return self.chi2_values[max(1, nr_outs)]
+        keys = list(set(c1_out_freq.keys()).union(c2_out_freq.keys()))
+        shared_keys = set(c1_out_freq.keys()).intersection(c2_out_freq.keys())
+        dof = len(keys) - 1
+        if dof == 0:
+            return 0
+        Q = self.compute_Q(c1_out_freq, c2_out_freq, keys)
+        return Q
+
+    def compute_Q(self, c1_out_freq, c2_out_freq, keys):
+        n_1 = sum(c1_out_freq.values())
+        n_2 = sum(c2_out_freq.values())
+
+        Q = 0
+        default_val = 0
+        yates_correction = -0.5 if len(keys) == 2 and \
+                                   any(c1_out_freq.get(k, 0) < 5 or c2_out_freq.get(k, 0) < 5 for k in keys) else 0
+        for k in keys:
+            p_hat_k = float(c1_out_freq.get(k, default_val) + c2_out_freq.get(k, default_val)) / (n_1 + n_2)
+            q_1_k = float(((abs(c1_out_freq.get(k, default_val) - n_1 * p_hat_k)) + yates_correction) ** 2) / (
+                    n_1 * p_hat_k)
+            q_2_k = float(((abs(c2_out_freq.get(k, default_val) - n_2 * p_hat_k)) + yates_correction) ** 2) / (
+                    n_2 * p_hat_k)
+            Q = Q + q_1_k + q_2_k
+        return Q
```

## aalpy/learning_algs/stochastic/SamplingBasedObservationTable.py

 * *Ordering differences only*

```diff
@@ -1,642 +1,642 @@
-from collections import defaultdict
-
-from aalpy.automata import Mdp, MdpState, StochasticMealyState, StochasticMealyMachine
-from .DifferenceChecker import DifferenceChecker
-from .StochasticTeacher import StochasticTeacher, Node
-from ...utils.HelperFunctions import is_suffix_of
-
-
-class SamplingBasedObservationTable:
-    def __init__(self, input_alphabet: list, automaton_type, teacher: StochasticTeacher,
-                 compatibility_checker: DifferenceChecker,
-                 alpha=0.05, strategy='normal',
-                 cex_processing=None):
-        """Constructor of the observation table. Initial queries are asked in the constructor.
-
-        Args:
-
-          input_alphabet: input alphabet
-          teacher: stochastic teacher
-          alpha: constant used in Hoeffding bound
-
-        """
-        self.compatibility_checker = compatibility_checker
-        assert input_alphabet is not None and teacher is not None
-        self.automaton_type = automaton_type
-
-        self.input_alphabet = [tuple([a]) for a in input_alphabet]
-
-        self.S = list()  # prefixes of S
-        self.E = [tuple([a]) for a in input_alphabet]
-        self.T = defaultdict(dict)
-
-        self.teacher = teacher
-        self.empty_word = tuple()
-        self.alpha = alpha
-        self.strategy = strategy
-        self.cex_processing = cex_processing
-
-        # initial output
-        if automaton_type == 'mdp':
-            self.initial_output = tuple(teacher.initial_value)
-            self.S.append(self.initial_output)
-        else:
-            self.S.append(tuple())
-
-        # Cache
-        self.compatibility_classes_representatives = None
-        self.compatibility_class = dict()
-        self.freq_query_cache = dict()
-
-        self.unambiguity_values = []
-
-    def refine_not_completed_cells(self, n_resample, uniform=False):
-        """
-        Firstly a prefix-tree acceptor is constructed for all non-completed cells and then that tree is used
-        for online testing/sampling.
-
-        Args:
-
-          uniform: if true, all cells will be uniformly sampled (Default value = False)
-          n_resample: Number of resamples
-
-        Returns:
-
-            False if no cells are to be refined, True if refining happened
-        """
-        if self.automaton_type == 'mdp':
-            pta_root = Node(self.initial_output[0])
-        else:
-            pta_root = Node(None)
-
-        dynamic = 0
-        if self.strategy == 'classic':
-            to_refine = []
-            for s in self.S + list(self.get_extended_s()):
-                for e in self.E:
-                    if not self.teacher.complete_query(s, e):
-                        to_refine.append(s + e)
-
-            if not to_refine:
-                return False
-
-            to_refine.sort(key=len, reverse=True)
-
-            for trace in to_refine:
-                self.add_to_PTA(pta_root, trace)
-
-        else:
-            for s in self.S + list(self.get_extended_s()):
-                if uniform:
-                    for e in self.E:
-                        self.add_to_PTA(pta_root, s + e, 1)
-                else:
-                    for e in self.E:
-                        longest_row_trace_prefix = (s + e)[:-1]
-                        while longest_row_trace_prefix not in self.T.keys():
-                            longest_row_trace_prefix = longest_row_trace_prefix[:-1]
-                        row_repr = 0
-                        for r in self.compatibility_classes_representatives:
-                            if self.are_rows_compatible(longest_row_trace_prefix, r):
-                                row_repr += 1
-                        # row_repr can be zero for non-closed
-                        # (int(row_repr - 1 * 2))
-                        uncertainty_value = max((row_repr - 1) * 2, 1)
-                        dynamic += uncertainty_value
-                        self.add_to_PTA(pta_root, s + e, uncertainty_value)
-
-        resample_value = n_resample if self.strategy == 'classic' else max(dynamic // 2, 500)
-
-        for i in range(resample_value):
-            self.teacher.tree_query(pta_root)
-        return True
-
-    def update_obs_table_with_freq_obs(self, element_of_s=None):
-        """
-        Updates cells in the observation table with frequency data. If the row in S has no extension yet, it is
-        generated and its cells populated.
-
-        Args:
-          element_of_s: if not None, selected row and its extensions will be updated (Default value = None)
-
-        Returns:
-
-        """
-        if element_of_s:
-            s_set = element_of_s + list(self.get_extended_s(element_of_s=element_of_s))
-        else:
-            s_set = self.S + list(self.get_extended_s())
-        # s_set = element_of_s if  else self.S + list(self.get_extended_s())
-
-        for s in s_set:
-            for e in self.E:
-                self.T[s][e] = self.teacher.frequency_query(s, e)
-                self.freq_query_cache[s + e] = self.T[s][e]
-
-    def get_extended_s(self, element_of_s=None):
-        """Generator returning all elements of the extended S set.
-
-        Args:
-          element_of_s:  (Default value = None)
-
-        Returns:
-
-        """
-        s_set = element_of_s if element_of_s else self.S
-        for s in s_set:
-            for i in self.input_alphabet:
-                if s + i in self.freq_query_cache.keys():
-                    freq_dict = self.freq_query_cache[s + i]
-                else:
-                    freq_dict = self.teacher.frequency_query(s, i)
-                for out, freq in freq_dict.items():
-                    new_pref = s + i + tuple([out])
-                    if freq > 0 and new_pref not in self.S:
-                        yield new_pref
-
-    def make_closed_and_consistent(self):
-        """
-        Observation table is updated until it is closed and consistent. Note that due the updated notion of row
-        equivalence no sampling is needed.
-        """
-        self.update_compatibility_classes()
-
-        while True:
-            closed, consistent = False, False
-            row_to_close = self.get_row_to_close()
-            if not row_to_close:
-                closed = True
-            if row_to_close:
-                self.S.append(row_to_close)
-                self.update_obs_table_with_freq_obs(element_of_s=[row_to_close])
-                self.update_compatibility_classes()
-
-            consistency_violation = self.get_consistency_violation()
-            if not consistency_violation:
-                consistent = True
-            if consistency_violation:
-                if consistency_violation not in self.E:
-                    self.E.append(consistency_violation)
-                self.update_obs_table_with_freq_obs()
-                self.update_compatibility_classes()
-
-            if closed and consistent:
-                break
-
-    def get_row_to_close(self):
-        """
-        Returns a row that is not closed.
-
-        Returns:
-
-            row that needs to be closed
-        """
-        for lt in self.get_extended_s():
-            row_is_closed = False
-            for r in self.compatibility_classes_representatives:
-                if self.are_rows_compatible(r, lt):
-                    row_is_closed = True
-                    break
-            if not row_is_closed:
-                return lt
-        return None
-
-    def get_consistency_violation(self, ignore=None):
-        """Find and return cause of consistency violation. Only computed on the compatibility class representatives.
-        :return: element of input + element of output + element of e that lead to the inconsistency
-
-        Args:
-          ignore:  (Default value = None)
-
-        Returns:
-
-            i + o + e that violate consistency
-        """
-        if self.cex_processing is not None:
-            return None
-
-        for ind, s1 in enumerate(self.S):
-            for s2 in self.S[ind + 1:]:
-                if self.are_rows_compatible(s1, s2, ignore):
-                    i_o_pairs = [(i, tuple([o])) for i in self.input_alphabet for o in self.T[s1][i].keys()]
-                    for i, o in i_o_pairs:
-                        s1_keys = self.T[s1 + i + o].keys()
-                        s2_keys = self.T[s2 + i + o].keys()
-                        if not s1_keys or not s2_keys:
-                            continue
-
-                        for e in self.E:
-                            if e == ignore:
-                                continue
-                            if self.are_cells_incompatible(s1 + i + o, s2 + i + o, e):
-                                return i + o + e
-        return None
-
-    def get_representative(self, target):
-        """
-
-        Args:
-          target: row in the observation table
-
-        Returns:
-          a representative compatible with the target
-
-        """
-        if self.compatibility_checker.use_diff_value():
-            smallest_diff_value = 2 ** 32
-            best_rep = None
-            if target in self.compatibility_classes_representatives:
-                return target
-            for r in self.compatibility_classes_representatives:
-                if self.automaton_type == "mdp" and r[-1] != target[-1]:
-                    continue
-                if not self.are_rows_compatible(r, target):
-                    continue
-                diff_value = 0
-                row_target = self.T[target]
-                row_r = self.T[r]
-                for e in self.E:
-                    diff_value += self.compatibility_checker.difference_value(row_r.get(e, None),
-                                                                              row_target.get(e, None))
-                if diff_value < smallest_diff_value:
-                    # if smallest_diff_value != 2**32:
-                    #    print("Found a better rep")
-                    smallest_diff_value = diff_value
-                    best_rep = r
-            return best_rep
-        else:
-            if target in self.S:
-                for r in self.compatibility_classes_representatives:
-                    if target == r or target in self.compatibility_class[r]:
-                        return r
-            else:
-                for r in self.compatibility_classes_representatives:
-                    if self.are_rows_compatible(r, target):
-                        return r
-        assert False
-
-    def trim_columns(self):
-        """ """
-        reverse_sorted_E = list(self.E)
-        reverse_sorted_E.sort(key=len, reverse=True)
-        to_remove = []
-        to_keep = []
-        self.update_obs_table_with_freq_obs()
-        self.make_closed_and_consistent()  # need a closed observation table
-
-        for e in reverse_sorted_E:
-            if e in self.input_alphabet:
-                continue
-            contains_dependent = False
-            for other_e in to_keep:
-                if is_suffix_of(e, other_e):
-                    contains_dependent = True
-            if contains_dependent:
-                to_keep.append(e)
-            elif self.get_consistency_violation(e):
-                to_keep.append(e)
-            else:
-                self.E.remove(e)  # need to remove here for get_consistency_violation to work
-                to_remove.append(e)
-
-        for e in to_remove:
-            for s in self.T.keys():
-                if e in self.T[s]:
-                    self.T[s].pop(e)
-
-    def trim(self, hypothesis):
-        """
-        Removes unnecessary rows from the observation table.
-
-        Args:
-          hypothesis: 
-
-        """
-
-        prefix_to_state_dict = {state.prefix: state for state in hypothesis.states}
-
-        to_remove = []
-        for s in self.S:
-            if s in self.compatibility_classes_representatives or s in to_remove:
-                continue
-
-            rep = self.get_representative(s)
-            if self.automaton_type == 'mdp':
-                if 'chaos' in {t[0].output for transitions in prefix_to_state_dict[rep].transitions.values()
-                               for t in transitions}:
-                    continue
-            else:
-                if 'chaos' in {t[1] for transitions in prefix_to_state_dict[rep].transitions.values()
-                               for t in transitions}:
-                    continue
-
-            num_compatible_repr = 0
-            row_is_prefix = False
-            for r in self.compatibility_classes_representatives:
-                if self.are_rows_compatible(r, s):
-                    num_compatible_repr += 1
-                if len(s) < len(r) and s == r[:len(s)]:
-                    row_is_prefix = True
-                    continue
-            if num_compatible_repr != 1 or row_is_prefix:
-                continue
-
-            to_remove.append(s)
-            for otherS in self.S:
-                if s == otherS[:len(s)] and otherS not in to_remove:
-                    to_remove.append(otherS)
-
-        for s in to_remove:
-            self.S.remove(s)
-            self.T.pop(s, None)
-            for i in self.input_alphabet:
-                for o in self.T[s + i]:
-                    self.T.pop(s + i + o, None)
-
-        if not self.cex_processing:
-            self.trim_columns()
-        else:
-            self.update_obs_table_with_freq_obs()
-
-    def stop(self, learning_round, chaos_cex_present, cex, stopping_range_dict, min_rounds=10, max_rounds=None,
-             target_unambiguity=0.99, print_unambiguity=False):
-        """
-        Decide if learning should terminate.
-
-        Args:
-
-          learning_round: current learning round
-          chaos_cex_present: is chaos counterexample present in the hypothesis
-          cex: counterexample found by the eq oracle
-          stopping_range_dict: dictionary where keys are number of last unambiguity values and value is
-          maximum differance allowed between them
-          min_rounds: minimum number of learning rounds (Default value = 5)
-          max_rounds: maximum number of learning rounds (Default value = None)
-          target_unambiguity: percentage of rows with unambiguous representatives (Default value = 0.99)
-          print_unambiguity: if true, current unambiguity rate will be printed (Default value = False)
-
-        Returns:
-
-          True if stopping condition satisfied, false otherwise
-        """
-        if max_rounds:
-            assert min_rounds <= max_rounds
-        if max_rounds and learning_round == max_rounds:
-            return True
-        if chaos_cex_present or cex is not None:
-            return False
-
-        extended_s = list(self.get_extended_s())
-        self.update_compatibility_classes()
-        numerator = 0
-        for row in self.S + extended_s:
-            row_repr = 0
-            for r in self.compatibility_classes_representatives:
-                if self.are_rows_compatible(row, r):
-                    row_repr += 1
-            numerator += 1 if row_repr == 1 else 0
-
-        unambiguous_rows_percentage = numerator / len(self.S + extended_s)
-
-        self.unambiguity_values.append(unambiguous_rows_percentage)
-        if self.strategy != 'classic' and learning_round >= min_rounds:
-            # keys are number of last unambiguity values and value is maximum differance allowed between them
-
-            for num_last, diff in stopping_range_dict.items():
-                if len(self.unambiguity_values) < num_last:
-                    continue
-                last_n_unamb = self.unambiguity_values[-num_last:]
-                if abs(max(last_n_unamb) - min(last_n_unamb) <= diff):
-                    return True
-
-        if print_unambiguity and learning_round % 5 == 0:
-            print(f'Unambiguous rows: {round(unambiguous_rows_percentage * 100, 2)}%;'
-                  f' {numerator} out of {len(self.S + extended_s)}')
-        if learning_round >= min_rounds and unambiguous_rows_percentage >= target_unambiguity:
-            return True
-
-        return False
-
-    def get_unamb_percentage(self):
-        extended_s = list(self.get_extended_s())
-        self.update_compatibility_classes()
-        numerator = 0
-        for row in self.S + extended_s:
-            row_repr = 0
-            for r in self.compatibility_classes_representatives:
-                if self.are_rows_compatible(row, r):
-                    row_repr += 1
-            numerator += 1 if row_repr == 1 else 0
-
-        unambiguous_rows_percentage = numerator / len(self.S + extended_s)
-        return round(unambiguous_rows_percentage * 100, 2)
-
-    def are_cells_incompatible(self, s1, s2, e):
-        """
-        Checks if 2 cells are considered different.
-
-        Args:
-
-          s1: prefix of row s1
-          s2: prefix of row s2
-          e: element of E
-
-        Returns:
-
-          True if cells are different, false otherwise
-
-        """
-        if self.strategy == 'classic':
-            if self.teacher.complete_query(s1, e) and self.teacher.complete_query(s2, e):
-                return self.compatibility_checker.are_cells_different(self.T[s1][e], self.T[s2][e])
-        elif self.strategy == 'normal' or self.strategy == 'chi2':
-            if e in self.T[s1] and e in self.T[s2]:
-                return self.compatibility_checker.are_cells_different(self.T[s1][e], self.T[s2][e])
-        else:
-            if e in self.T[s1] and e in self.T[s2]:
-                return self.compatibility_checker.are_cells_different(self.T[s1][e], self.T[s2][e], s1=s1, s2=s2, e=e)
-        return False
-
-    def are_rows_compatible(self, s1, s2, e_ignore=None):
-        """
-        Check if the rows are compatible.
-        Rows are compatible if all cells are compatible(not different) and their prefixes
-        end in the same output element.
-
-        Args:
-          s1: prefix of row s1
-          s2: prefix of row s2
-          e_ignore: e not considered for the computation of row compatibility (Default value = None)
-
-        Returns:
-          True if rows are compatible, False otherwise
-
-        """
-        if self.automaton_type == 'mdp' and s1[-1] != s2[-1]:
-            return False
-
-        for e in self.E:
-            if e == e_ignore:
-                continue
-            if self.are_cells_incompatible(s1, s2, e):
-                return False
-        return True
-
-    def update_compatibility_classes(self):
-        """Updates the compatibility classes and stores their representatives."""
-        self.compatibility_class.clear()
-
-        class_rank_pair = []
-        for s in self.S:
-            rank = sum([sum(self.T[s][i].values()) for i in self.input_alphabet])
-            class_rank_pair.append((s, rank))
-
-        # sort according to frequency
-        class_rank_pair.sort(key=lambda x: x[1], reverse=True)
-
-        # # sort according to prefix length, and elements of same length sort by value
-        # class_rank_pair = [(s, -rank) for (s, rank) in class_rank_pair]
-        # class_rank_pair.sort(key=lambda x: (len(x[0]), x[1]))
-        # class_rank_pair = [(s, -rank) for (s, rank) in class_rank_pair]
-
-        compatibility_classes = [c[0] for c in class_rank_pair]
-
-        tmp_classes = list(compatibility_classes)
-        not_partitioned = list(self.S)
-
-        representatives = []
-        while not_partitioned:
-            r = tmp_classes.pop(0)
-            not_partitioned.remove(r)
-
-            cg_r = [s for s in not_partitioned if self.are_rows_compatible(r, s)]
-
-            self.compatibility_class[r] = cg_r
-
-            representatives.append(r)
-            for sp in cg_r:
-                not_partitioned.remove(sp)
-                tmp_classes.remove(sp)
-
-        self.compatibility_classes_representatives = representatives
-
-    def chaos_counterexample(self, hypothesis):
-        """ Check whether the chaos state is reachable.
-
-        Args:
-          hypothesis: current hypothesis
-
-        Returns:
-          True if chaos state is reachable, False otherwise
-
-        """
-        for state in hypothesis.states:
-            if self.automaton_type == "mdp" and state.output == "chaos" \
-                    or self.automaton_type == "smm" and state.state_id == "chaos":
-                # we are not interested in chaos state, but in prefix to chaos
-                continue
-            for i in self.input_alphabet:
-                output_states = state.transitions[i[0]]
-                if self.automaton_type == 'mdp':
-                    for (s, _) in output_states:
-                        if s.output == 'chaos':
-                            return True
-                            # return state.prefix + i
-                else:
-                    for (_, o, _) in output_states:
-                        if o == 'chaos':
-                            return True
-                            # return state.prefix
-        return False
-        # return None
-
-    def add_to_PTA(self, pta_root, trace, uncertainty_value=None):
-        """Adds a trace to the PTA. PTA is later used for online sampling. The uncertainty value is added to inputs as
-        frequencies, which specify how often a particular input should be sampled.
-
-        Args:
-          pta_root: root of the prefix tree acceptor
-          trace: trace to add to the PTA
-          uncertainty_value: uncertainty value (Default value = None)
-
-        Returns:
-
-        """
-        curr_node = pta_root
-        start = 1 if self.automaton_type == 'mdp' else 0
-        for index in range(start, len(trace), 2):
-            inp = trace[index]
-            if uncertainty_value:
-                # use frequencies for uncertainties
-                curr_node.input_frequencies[inp] += uncertainty_value
-            # need to add a dummy output in the leaves
-            output = trace[index + 1] if index + 1 < len(trace) else "dummy"
-            child = curr_node.get_child(inp, output)
-            if child:
-                curr_node = child
-            else:
-                new_node = Node(output)
-                curr_node.children[inp][output] = new_node
-                curr_node = new_node
-
-    def generate_hypothesis(self):
-        """Generates the hypothesis from the observation table.
-        :return: current hypothesis
-
-        Args:
-
-        Returns:
-
-        """
-        r_state_map = dict()
-        state_counter = 0
-        for r in self.compatibility_classes_representatives:
-            if self.automaton_type == 'mdp':
-                r_state_map[r] = MdpState(state_id=f's{state_counter}', output=r[-1])
-            else:
-                r_state_map[r] = StochasticMealyState(state_id=f's{state_counter}')
-            r_state_map[r].prefix = r
-
-            state_counter += 1
-        if self.automaton_type == 'mdp':
-            r_state_map['chaos'] = MdpState(state_id=f's{state_counter}', output='chaos')
-            for i in self.input_alphabet:
-                r_state_map['chaos'].transitions[i[0]].append((r_state_map['chaos'], 1.))
-        else:
-            r_state_map['chaos'] = StochasticMealyState(state_id=f'chaos')
-            for i in self.input_alphabet:
-                r_state_map['chaos'].transitions[i[0]].append((r_state_map['chaos'], 'chaos', 1.))
-
-        for s in self.compatibility_classes_representatives:
-            for i in self.input_alphabet:
-                freq_dict = self.T[s][i]
-
-                total_sum = sum(freq_dict.values())
-
-                origin_state = s
-                if self.strategy == 'classic' and not self.teacher.complete_query(s, i) \
-                        or self.strategy != 'classic' and i not in self.T[s]:
-                    if self.automaton_type == 'mdp':
-                        r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 1.))
-                    else:
-                        r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 'chaos', 1.))
-                else:
-                    if len(freq_dict.items()) == 0:
-                        if self.automaton_type == 'mdp':
-                            r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 1.))
-                        else:
-                            r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 'chaos', 1.))
-                    else:
-                        for output, frequency in freq_dict.items():
-                            new_state = self.get_representative(s + i + tuple([output]))
-                            if self.automaton_type == 'mdp':
-                                r_state_map[origin_state].transitions[i[0]].append(
-                                    (r_state_map[new_state], frequency / total_sum))
-                            else:
-                                r_state_map[origin_state].transitions[i[0]].append(
-                                    (r_state_map[new_state], output, frequency / total_sum))
-
-        if self.automaton_type == 'mdp':
-            return Mdp(r_state_map[self.get_representative(self.initial_output)], list(r_state_map.values()))
-        else:
-            return StochasticMealyMachine(r_state_map[tuple()], list(r_state_map.values()))
+from collections import defaultdict
+
+from aalpy.automata import Mdp, MdpState, StochasticMealyState, StochasticMealyMachine
+from .DifferenceChecker import DifferenceChecker
+from .StochasticTeacher import StochasticTeacher, Node
+from ...utils.HelperFunctions import is_suffix_of
+
+
+class SamplingBasedObservationTable:
+    def __init__(self, input_alphabet: list, automaton_type, teacher: StochasticTeacher,
+                 compatibility_checker: DifferenceChecker,
+                 alpha=0.05, strategy='normal',
+                 cex_processing=None):
+        """Constructor of the observation table. Initial queries are asked in the constructor.
+
+        Args:
+
+          input_alphabet: input alphabet
+          teacher: stochastic teacher
+          alpha: constant used in Hoeffding bound
+
+        """
+        self.compatibility_checker = compatibility_checker
+        assert input_alphabet is not None and teacher is not None
+        self.automaton_type = automaton_type
+
+        self.input_alphabet = [tuple([a]) for a in input_alphabet]
+
+        self.S = list()  # prefixes of S
+        self.E = [tuple([a]) for a in input_alphabet]
+        self.T = defaultdict(dict)
+
+        self.teacher = teacher
+        self.empty_word = tuple()
+        self.alpha = alpha
+        self.strategy = strategy
+        self.cex_processing = cex_processing
+
+        # initial output
+        if automaton_type == 'mdp':
+            self.initial_output = tuple(teacher.initial_value)
+            self.S.append(self.initial_output)
+        else:
+            self.S.append(tuple())
+
+        # Cache
+        self.compatibility_classes_representatives = None
+        self.compatibility_class = dict()
+        self.freq_query_cache = dict()
+
+        self.unambiguity_values = []
+
+    def refine_not_completed_cells(self, n_resample, uniform=False):
+        """
+        Firstly a prefix-tree acceptor is constructed for all non-completed cells and then that tree is used
+        for online testing/sampling.
+
+        Args:
+
+          uniform: if true, all cells will be uniformly sampled (Default value = False)
+          n_resample: Number of resamples
+
+        Returns:
+
+            False if no cells are to be refined, True if refining happened
+        """
+        if self.automaton_type == 'mdp':
+            pta_root = Node(self.initial_output[0])
+        else:
+            pta_root = Node(None)
+
+        dynamic = 0
+        if self.strategy == 'classic':
+            to_refine = []
+            for s in self.S + list(self.get_extended_s()):
+                for e in self.E:
+                    if not self.teacher.complete_query(s, e):
+                        to_refine.append(s + e)
+
+            if not to_refine:
+                return False
+
+            to_refine.sort(key=len, reverse=True)
+
+            for trace in to_refine:
+                self.add_to_PTA(pta_root, trace)
+
+        else:
+            for s in self.S + list(self.get_extended_s()):
+                if uniform:
+                    for e in self.E:
+                        self.add_to_PTA(pta_root, s + e, 1)
+                else:
+                    for e in self.E:
+                        longest_row_trace_prefix = (s + e)[:-1]
+                        while longest_row_trace_prefix not in self.T.keys():
+                            longest_row_trace_prefix = longest_row_trace_prefix[:-1]
+                        row_repr = 0
+                        for r in self.compatibility_classes_representatives:
+                            if self.are_rows_compatible(longest_row_trace_prefix, r):
+                                row_repr += 1
+                        # row_repr can be zero for non-closed
+                        # (int(row_repr - 1 * 2))
+                        uncertainty_value = max((row_repr - 1) * 2, 1)
+                        dynamic += uncertainty_value
+                        self.add_to_PTA(pta_root, s + e, uncertainty_value)
+
+        resample_value = n_resample if self.strategy == 'classic' else max(dynamic // 2, 500)
+
+        for i in range(resample_value):
+            self.teacher.tree_query(pta_root)
+        return True
+
+    def update_obs_table_with_freq_obs(self, element_of_s=None):
+        """
+        Updates cells in the observation table with frequency data. If the row in S has no extension yet, it is
+        generated and its cells populated.
+
+        Args:
+          element_of_s: if not None, selected row and its extensions will be updated (Default value = None)
+
+        Returns:
+
+        """
+        if element_of_s:
+            s_set = element_of_s + list(self.get_extended_s(element_of_s=element_of_s))
+        else:
+            s_set = self.S + list(self.get_extended_s())
+        # s_set = element_of_s if  else self.S + list(self.get_extended_s())
+
+        for s in s_set:
+            for e in self.E:
+                self.T[s][e] = self.teacher.frequency_query(s, e)
+                self.freq_query_cache[s + e] = self.T[s][e]
+
+    def get_extended_s(self, element_of_s=None):
+        """Generator returning all elements of the extended S set.
+
+        Args:
+          element_of_s:  (Default value = None)
+
+        Returns:
+
+        """
+        s_set = element_of_s if element_of_s else self.S
+        for s in s_set:
+            for i in self.input_alphabet:
+                if s + i in self.freq_query_cache.keys():
+                    freq_dict = self.freq_query_cache[s + i]
+                else:
+                    freq_dict = self.teacher.frequency_query(s, i)
+                for out, freq in freq_dict.items():
+                    new_pref = s + i + tuple([out])
+                    if freq > 0 and new_pref not in self.S:
+                        yield new_pref
+
+    def make_closed_and_consistent(self):
+        """
+        Observation table is updated until it is closed and consistent. Note that due the updated notion of row
+        equivalence no sampling is needed.
+        """
+        self.update_compatibility_classes()
+
+        while True:
+            closed, consistent = False, False
+            row_to_close = self.get_row_to_close()
+            if not row_to_close:
+                closed = True
+            if row_to_close:
+                self.S.append(row_to_close)
+                self.update_obs_table_with_freq_obs(element_of_s=[row_to_close])
+                self.update_compatibility_classes()
+
+            consistency_violation = self.get_consistency_violation()
+            if not consistency_violation:
+                consistent = True
+            if consistency_violation:
+                if consistency_violation not in self.E:
+                    self.E.append(consistency_violation)
+                self.update_obs_table_with_freq_obs()
+                self.update_compatibility_classes()
+
+            if closed and consistent:
+                break
+
+    def get_row_to_close(self):
+        """
+        Returns a row that is not closed.
+
+        Returns:
+
+            row that needs to be closed
+        """
+        for lt in self.get_extended_s():
+            row_is_closed = False
+            for r in self.compatibility_classes_representatives:
+                if self.are_rows_compatible(r, lt):
+                    row_is_closed = True
+                    break
+            if not row_is_closed:
+                return lt
+        return None
+
+    def get_consistency_violation(self, ignore=None):
+        """Find and return cause of consistency violation. Only computed on the compatibility class representatives.
+        :return: element of input + element of output + element of e that lead to the inconsistency
+
+        Args:
+          ignore:  (Default value = None)
+
+        Returns:
+
+            i + o + e that violate consistency
+        """
+        if self.cex_processing is not None:
+            return None
+
+        for ind, s1 in enumerate(self.S):
+            for s2 in self.S[ind + 1:]:
+                if self.are_rows_compatible(s1, s2, ignore):
+                    i_o_pairs = [(i, tuple([o])) for i in self.input_alphabet for o in self.T[s1][i].keys()]
+                    for i, o in i_o_pairs:
+                        s1_keys = self.T[s1 + i + o].keys()
+                        s2_keys = self.T[s2 + i + o].keys()
+                        if not s1_keys or not s2_keys:
+                            continue
+
+                        for e in self.E:
+                            if e == ignore:
+                                continue
+                            if self.are_cells_incompatible(s1 + i + o, s2 + i + o, e):
+                                return i + o + e
+        return None
+
+    def get_representative(self, target):
+        """
+
+        Args:
+          target: row in the observation table
+
+        Returns:
+          a representative compatible with the target
+
+        """
+        if self.compatibility_checker.use_diff_value():
+            smallest_diff_value = 2 ** 32
+            best_rep = None
+            if target in self.compatibility_classes_representatives:
+                return target
+            for r in self.compatibility_classes_representatives:
+                if self.automaton_type == "mdp" and r[-1] != target[-1]:
+                    continue
+                if not self.are_rows_compatible(r, target):
+                    continue
+                diff_value = 0
+                row_target = self.T[target]
+                row_r = self.T[r]
+                for e in self.E:
+                    diff_value += self.compatibility_checker.difference_value(row_r.get(e, None),
+                                                                              row_target.get(e, None))
+                if diff_value < smallest_diff_value:
+                    # if smallest_diff_value != 2**32:
+                    #    print("Found a better rep")
+                    smallest_diff_value = diff_value
+                    best_rep = r
+            return best_rep
+        else:
+            if target in self.S:
+                for r in self.compatibility_classes_representatives:
+                    if target == r or target in self.compatibility_class[r]:
+                        return r
+            else:
+                for r in self.compatibility_classes_representatives:
+                    if self.are_rows_compatible(r, target):
+                        return r
+        assert False
+
+    def trim_columns(self):
+        """ """
+        reverse_sorted_E = list(self.E)
+        reverse_sorted_E.sort(key=len, reverse=True)
+        to_remove = []
+        to_keep = []
+        self.update_obs_table_with_freq_obs()
+        self.make_closed_and_consistent()  # need a closed observation table
+
+        for e in reverse_sorted_E:
+            if e in self.input_alphabet:
+                continue
+            contains_dependent = False
+            for other_e in to_keep:
+                if is_suffix_of(e, other_e):
+                    contains_dependent = True
+            if contains_dependent:
+                to_keep.append(e)
+            elif self.get_consistency_violation(e):
+                to_keep.append(e)
+            else:
+                self.E.remove(e)  # need to remove here for get_consistency_violation to work
+                to_remove.append(e)
+
+        for e in to_remove:
+            for s in self.T.keys():
+                if e in self.T[s]:
+                    self.T[s].pop(e)
+
+    def trim(self, hypothesis):
+        """
+        Removes unnecessary rows from the observation table.
+
+        Args:
+          hypothesis: 
+
+        """
+
+        prefix_to_state_dict = {state.prefix: state for state in hypothesis.states}
+
+        to_remove = []
+        for s in self.S:
+            if s in self.compatibility_classes_representatives or s in to_remove:
+                continue
+
+            rep = self.get_representative(s)
+            if self.automaton_type == 'mdp':
+                if 'chaos' in {t[0].output for transitions in prefix_to_state_dict[rep].transitions.values()
+                               for t in transitions}:
+                    continue
+            else:
+                if 'chaos' in {t[1] for transitions in prefix_to_state_dict[rep].transitions.values()
+                               for t in transitions}:
+                    continue
+
+            num_compatible_repr = 0
+            row_is_prefix = False
+            for r in self.compatibility_classes_representatives:
+                if self.are_rows_compatible(r, s):
+                    num_compatible_repr += 1
+                if len(s) < len(r) and s == r[:len(s)]:
+                    row_is_prefix = True
+                    continue
+            if num_compatible_repr != 1 or row_is_prefix:
+                continue
+
+            to_remove.append(s)
+            for otherS in self.S:
+                if s == otherS[:len(s)] and otherS not in to_remove:
+                    to_remove.append(otherS)
+
+        for s in to_remove:
+            self.S.remove(s)
+            self.T.pop(s, None)
+            for i in self.input_alphabet:
+                for o in self.T[s + i]:
+                    self.T.pop(s + i + o, None)
+
+        if not self.cex_processing:
+            self.trim_columns()
+        else:
+            self.update_obs_table_with_freq_obs()
+
+    def stop(self, learning_round, chaos_cex_present, cex, stopping_range_dict, min_rounds=10, max_rounds=None,
+             target_unambiguity=0.99, print_unambiguity=False):
+        """
+        Decide if learning should terminate.
+
+        Args:
+
+          learning_round: current learning round
+          chaos_cex_present: is chaos counterexample present in the hypothesis
+          cex: counterexample found by the eq oracle
+          stopping_range_dict: dictionary where keys are number of last unambiguity values and value is
+          maximum differance allowed between them
+          min_rounds: minimum number of learning rounds (Default value = 5)
+          max_rounds: maximum number of learning rounds (Default value = None)
+          target_unambiguity: percentage of rows with unambiguous representatives (Default value = 0.99)
+          print_unambiguity: if true, current unambiguity rate will be printed (Default value = False)
+
+        Returns:
+
+          True if stopping condition satisfied, false otherwise
+        """
+        if max_rounds:
+            assert min_rounds <= max_rounds
+        if max_rounds and learning_round == max_rounds:
+            return True
+        if chaos_cex_present or cex is not None:
+            return False
+
+        extended_s = list(self.get_extended_s())
+        self.update_compatibility_classes()
+        numerator = 0
+        for row in self.S + extended_s:
+            row_repr = 0
+            for r in self.compatibility_classes_representatives:
+                if self.are_rows_compatible(row, r):
+                    row_repr += 1
+            numerator += 1 if row_repr == 1 else 0
+
+        unambiguous_rows_percentage = numerator / len(self.S + extended_s)
+
+        self.unambiguity_values.append(unambiguous_rows_percentage)
+        if self.strategy != 'classic' and learning_round >= min_rounds:
+            # keys are number of last unambiguity values and value is maximum differance allowed between them
+
+            for num_last, diff in stopping_range_dict.items():
+                if len(self.unambiguity_values) < num_last:
+                    continue
+                last_n_unamb = self.unambiguity_values[-num_last:]
+                if abs(max(last_n_unamb) - min(last_n_unamb) <= diff):
+                    return True
+
+        if print_unambiguity and learning_round % 5 == 0:
+            print(f'Unambiguous rows: {round(unambiguous_rows_percentage * 100, 2)}%;'
+                  f' {numerator} out of {len(self.S + extended_s)}')
+        if learning_round >= min_rounds and unambiguous_rows_percentage >= target_unambiguity:
+            return True
+
+        return False
+
+    def get_unamb_percentage(self):
+        extended_s = list(self.get_extended_s())
+        self.update_compatibility_classes()
+        numerator = 0
+        for row in self.S + extended_s:
+            row_repr = 0
+            for r in self.compatibility_classes_representatives:
+                if self.are_rows_compatible(row, r):
+                    row_repr += 1
+            numerator += 1 if row_repr == 1 else 0
+
+        unambiguous_rows_percentage = numerator / len(self.S + extended_s)
+        return round(unambiguous_rows_percentage * 100, 2)
+
+    def are_cells_incompatible(self, s1, s2, e):
+        """
+        Checks if 2 cells are considered different.
+
+        Args:
+
+          s1: prefix of row s1
+          s2: prefix of row s2
+          e: element of E
+
+        Returns:
+
+          True if cells are different, false otherwise
+
+        """
+        if self.strategy == 'classic':
+            if self.teacher.complete_query(s1, e) and self.teacher.complete_query(s2, e):
+                return self.compatibility_checker.are_cells_different(self.T[s1][e], self.T[s2][e])
+        elif self.strategy == 'normal' or self.strategy == 'chi2':
+            if e in self.T[s1] and e in self.T[s2]:
+                return self.compatibility_checker.are_cells_different(self.T[s1][e], self.T[s2][e])
+        else:
+            if e in self.T[s1] and e in self.T[s2]:
+                return self.compatibility_checker.are_cells_different(self.T[s1][e], self.T[s2][e], s1=s1, s2=s2, e=e)
+        return False
+
+    def are_rows_compatible(self, s1, s2, e_ignore=None):
+        """
+        Check if the rows are compatible.
+        Rows are compatible if all cells are compatible(not different) and their prefixes
+        end in the same output element.
+
+        Args:
+          s1: prefix of row s1
+          s2: prefix of row s2
+          e_ignore: e not considered for the computation of row compatibility (Default value = None)
+
+        Returns:
+          True if rows are compatible, False otherwise
+
+        """
+        if self.automaton_type == 'mdp' and s1[-1] != s2[-1]:
+            return False
+
+        for e in self.E:
+            if e == e_ignore:
+                continue
+            if self.are_cells_incompatible(s1, s2, e):
+                return False
+        return True
+
+    def update_compatibility_classes(self):
+        """Updates the compatibility classes and stores their representatives."""
+        self.compatibility_class.clear()
+
+        class_rank_pair = []
+        for s in self.S:
+            rank = sum([sum(self.T[s][i].values()) for i in self.input_alphabet])
+            class_rank_pair.append((s, rank))
+
+        # sort according to frequency
+        class_rank_pair.sort(key=lambda x: x[1], reverse=True)
+
+        # # sort according to prefix length, and elements of same length sort by value
+        # class_rank_pair = [(s, -rank) for (s, rank) in class_rank_pair]
+        # class_rank_pair.sort(key=lambda x: (len(x[0]), x[1]))
+        # class_rank_pair = [(s, -rank) for (s, rank) in class_rank_pair]
+
+        compatibility_classes = [c[0] for c in class_rank_pair]
+
+        tmp_classes = list(compatibility_classes)
+        not_partitioned = list(self.S)
+
+        representatives = []
+        while not_partitioned:
+            r = tmp_classes.pop(0)
+            not_partitioned.remove(r)
+
+            cg_r = [s for s in not_partitioned if self.are_rows_compatible(r, s)]
+
+            self.compatibility_class[r] = cg_r
+
+            representatives.append(r)
+            for sp in cg_r:
+                not_partitioned.remove(sp)
+                tmp_classes.remove(sp)
+
+        self.compatibility_classes_representatives = representatives
+
+    def chaos_counterexample(self, hypothesis):
+        """ Check whether the chaos state is reachable.
+
+        Args:
+          hypothesis: current hypothesis
+
+        Returns:
+          True if chaos state is reachable, False otherwise
+
+        """
+        for state in hypothesis.states:
+            if self.automaton_type == "mdp" and state.output == "chaos" \
+                    or self.automaton_type == "smm" and state.state_id == "chaos":
+                # we are not interested in chaos state, but in prefix to chaos
+                continue
+            for i in self.input_alphabet:
+                output_states = state.transitions[i[0]]
+                if self.automaton_type == 'mdp':
+                    for (s, _) in output_states:
+                        if s.output == 'chaos':
+                            return True
+                            # return state.prefix + i
+                else:
+                    for (_, o, _) in output_states:
+                        if o == 'chaos':
+                            return True
+                            # return state.prefix
+        return False
+        # return None
+
+    def add_to_PTA(self, pta_root, trace, uncertainty_value=None):
+        """Adds a trace to the PTA. PTA is later used for online sampling. The uncertainty value is added to inputs as
+        frequencies, which specify how often a particular input should be sampled.
+
+        Args:
+          pta_root: root of the prefix tree acceptor
+          trace: trace to add to the PTA
+          uncertainty_value: uncertainty value (Default value = None)
+
+        Returns:
+
+        """
+        curr_node = pta_root
+        start = 1 if self.automaton_type == 'mdp' else 0
+        for index in range(start, len(trace), 2):
+            inp = trace[index]
+            if uncertainty_value:
+                # use frequencies for uncertainties
+                curr_node.input_frequencies[inp] += uncertainty_value
+            # need to add a dummy output in the leaves
+            output = trace[index + 1] if index + 1 < len(trace) else "dummy"
+            child = curr_node.get_child(inp, output)
+            if child:
+                curr_node = child
+            else:
+                new_node = Node(output)
+                curr_node.children[inp][output] = new_node
+                curr_node = new_node
+
+    def generate_hypothesis(self):
+        """Generates the hypothesis from the observation table.
+        :return: current hypothesis
+
+        Args:
+
+        Returns:
+
+        """
+        r_state_map = dict()
+        state_counter = 0
+        for r in self.compatibility_classes_representatives:
+            if self.automaton_type == 'mdp':
+                r_state_map[r] = MdpState(state_id=f's{state_counter}', output=r[-1])
+            else:
+                r_state_map[r] = StochasticMealyState(state_id=f's{state_counter}')
+            r_state_map[r].prefix = r
+
+            state_counter += 1
+        if self.automaton_type == 'mdp':
+            r_state_map['chaos'] = MdpState(state_id=f's{state_counter}', output='chaos')
+            for i in self.input_alphabet:
+                r_state_map['chaos'].transitions[i[0]].append((r_state_map['chaos'], 1.))
+        else:
+            r_state_map['chaos'] = StochasticMealyState(state_id=f'chaos')
+            for i in self.input_alphabet:
+                r_state_map['chaos'].transitions[i[0]].append((r_state_map['chaos'], 'chaos', 1.))
+
+        for s in self.compatibility_classes_representatives:
+            for i in self.input_alphabet:
+                freq_dict = self.T[s][i]
+
+                total_sum = sum(freq_dict.values())
+
+                origin_state = s
+                if self.strategy == 'classic' and not self.teacher.complete_query(s, i) \
+                        or self.strategy != 'classic' and i not in self.T[s]:
+                    if self.automaton_type == 'mdp':
+                        r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 1.))
+                    else:
+                        r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 'chaos', 1.))
+                else:
+                    if len(freq_dict.items()) == 0:
+                        if self.automaton_type == 'mdp':
+                            r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 1.))
+                        else:
+                            r_state_map[origin_state].transitions[i[0]].append((r_state_map['chaos'], 'chaos', 1.))
+                    else:
+                        for output, frequency in freq_dict.items():
+                            new_state = self.get_representative(s + i + tuple([output]))
+                            if self.automaton_type == 'mdp':
+                                r_state_map[origin_state].transitions[i[0]].append(
+                                    (r_state_map[new_state], frequency / total_sum))
+                            else:
+                                r_state_map[origin_state].transitions[i[0]].append(
+                                    (r_state_map[new_state], output, frequency / total_sum))
+
+        if self.automaton_type == 'mdp':
+            return Mdp(r_state_map[self.get_representative(self.initial_output)], list(r_state_map.values()))
+        else:
+            return StochasticMealyMachine(r_state_map[tuple()], list(r_state_map.values()))
```

## aalpy/learning_algs/stochastic/StochasticCexProcessing.py

 * *Ordering differences only*

```diff
@@ -1,130 +1,130 @@
-from aalpy.automata import Mdp
-from aalpy.base import SUL
-
-
-def stochastic_longest_prefix(cex, prefixes):
-    """
-    Counterexample processing based on Shabaz-Groz cex processing.
-
-    Args:
-
-        cex: counterexample
-        prefixes: all prefixes in the observation table
-    Returns:
-
-        Single suffix.
-    """
-    prefixes = list(prefixes)
-    prefixes.sort(key=len, reverse=True)
-
-    trimmed_cex = None
-    trimmed = False
-    for p in prefixes:
-        if p[1::2] == cex[:len(p)][1::2]:
-            trimmed_cex = cex[len(p):]
-            trimmed = True
-            break
-
-    trimmed_cex = trimmed_cex if trimmed else cex
-    trimmed_cex = list(trimmed_cex)
-
-    if not trimmed_cex:
-        return ()
-
-    # get all suffixes and return
-    suffixes = [tuple(trimmed_cex[len(trimmed_cex) - i - 1:]) for i in range(0, len(trimmed_cex), 2)]
-
-    # prefixes
-    # need to pop 0 for MDP, for SMM remove the line
-    # trimmed_cex.pop(0)
-    # prefixes = [tuple(trimmed_cex[:i + 1]) for i in range(0, len(trimmed_cex), 2)]
-
-    return suffixes
-
-
-def stochastic_rs(sul: SUL, cex: tuple, hypothesis):
-    """Riverst-Schapire counter example processing.
-
-    Args:
-
-        sul: system under learning
-        cex: found counterexample
-        hypothesis: hypothesis on which counterexample was found
-    Returns:
-
-        suffixes to be added to the E set
-
-    """
-    # cex_out = self.sul.query(tuple(cex))
-
-    if isinstance(hypothesis, Mdp):
-        cex = cex[1:]
-
-    inputs = tuple(cex[::2])
-    outputs = tuple(cex[1::2])
-    # cex_out = self.teacher.sul.query(cex)
-
-    lower = 1
-    upper = len(inputs) - 2
-
-    while True:
-        hypothesis.reset_to_initial()
-        mid = (lower + upper) // 2
-
-        # arr[:n] -> first n values
-        # arr[n:] -> last n values
-
-        for i, o in zip(inputs[:mid], outputs[:mid]):
-            hypothesis.step_to(i, o)
-
-        s_bracket = hypothesis.current_state.prefix
-
-        # prefix in hyp is reached
-
-        prefix_inputs = s_bracket[1::2] if isinstance(hypothesis, Mdp) else s_bracket[::2]
-        # prefix_outputs = s_bracket[0::2] if isinstance(hypothesis, Mdp) else s_bracket[1::2]
-
-        not_same = False
-
-        prefix_reached = False
-        while not prefix_reached:
-            hypothesis.reset_to_initial()
-            sul.post()
-            sul.pre()
-
-            repeat = False
-            for inp in prefix_inputs:
-                o_sul = sul.step(inp)
-                o_hyp = hypothesis.step_to(inp, o_sul)
-
-                if o_hyp is None:
-                    repeat = True
-                    break
-
-            prefix_reached = not repeat
-
-        for inp in inputs[mid:]:
-
-            o_sul = sul.step(inp)
-            o_hyp = hypothesis.step_to(inp, o_sul)
-
-            if o_hyp is None:
-                not_same = True
-                break
-
-        if not not_same:
-            lower = mid + 1
-            if upper < lower:
-                suffix = cex[(mid + 1) * 2:]
-                break
-        else:
-            upper = mid - 1
-            if upper < lower:
-                suffix = cex[mid * 2:]
-                break
-
-    suffixes = [tuple(suffix[len(suffix) - i - 1:]) for i in range(0, len(suffix), 2)]
-
-    # suffixes = [suffixes[-1]]
-    # print(len(cex), len(suffixes[-1]))
-    return suffixes
+from aalpy.automata import Mdp
+from aalpy.base import SUL
+
+
+def stochastic_longest_prefix(cex, prefixes):
+    """
+    Counterexample processing based on Shabaz-Groz cex processing.
+
+    Args:
+
+        cex: counterexample
+        prefixes: all prefixes in the observation table
+    Returns:
+
+        Single suffix.
+    """
+    prefixes = list(prefixes)
+    prefixes.sort(key=len, reverse=True)
+
+    trimmed_cex = None
+    trimmed = False
+    for p in prefixes:
+        if p[1::2] == cex[:len(p)][1::2]:
+            trimmed_cex = cex[len(p):]
+            trimmed = True
+            break
+
+    trimmed_cex = trimmed_cex if trimmed else cex
+    trimmed_cex = list(trimmed_cex)
+
+    if not trimmed_cex:
+        return ()
+
+    # get all suffixes and return
+    suffixes = [tuple(trimmed_cex[len(trimmed_cex) - i - 1:]) for i in range(0, len(trimmed_cex), 2)]
+
+    # prefixes
+    # need to pop 0 for MDP, for SMM remove the line
+    # trimmed_cex.pop(0)
+    # prefixes = [tuple(trimmed_cex[:i + 1]) for i in range(0, len(trimmed_cex), 2)]
+
+    return suffixes
+
+
+def stochastic_rs(sul: SUL, cex: tuple, hypothesis):
+    """Riverst-Schapire counter example processing.
+
+    Args:
+
+        sul: system under learning
+        cex: found counterexample
+        hypothesis: hypothesis on which counterexample was found
+    Returns:
+
+        suffixes to be added to the E set
+
+    """
+    # cex_out = self.sul.query(tuple(cex))
+
+    if isinstance(hypothesis, Mdp):
+        cex = cex[1:]
+
+    inputs = tuple(cex[::2])
+    outputs = tuple(cex[1::2])
+    # cex_out = self.teacher.sul.query(cex)
+
+    lower = 1
+    upper = len(inputs) - 2
+
+    while True:
+        hypothesis.reset_to_initial()
+        mid = (lower + upper) // 2
+
+        # arr[:n] -> first n values
+        # arr[n:] -> last n values
+
+        for i, o in zip(inputs[:mid], outputs[:mid]):
+            hypothesis.step_to(i, o)
+
+        s_bracket = hypothesis.current_state.prefix
+
+        # prefix in hyp is reached
+
+        prefix_inputs = s_bracket[1::2] if isinstance(hypothesis, Mdp) else s_bracket[::2]
+        # prefix_outputs = s_bracket[0::2] if isinstance(hypothesis, Mdp) else s_bracket[1::2]
+
+        not_same = False
+
+        prefix_reached = False
+        while not prefix_reached:
+            hypothesis.reset_to_initial()
+            sul.post()
+            sul.pre()
+
+            repeat = False
+            for inp in prefix_inputs:
+                o_sul = sul.step(inp)
+                o_hyp = hypothesis.step_to(inp, o_sul)
+
+                if o_hyp is None:
+                    repeat = True
+                    break
+
+            prefix_reached = not repeat
+
+        for inp in inputs[mid:]:
+
+            o_sul = sul.step(inp)
+            o_hyp = hypothesis.step_to(inp, o_sul)
+
+            if o_hyp is None:
+                not_same = True
+                break
+
+        if not not_same:
+            lower = mid + 1
+            if upper < lower:
+                suffix = cex[(mid + 1) * 2:]
+                break
+        else:
+            upper = mid - 1
+            if upper < lower:
+                suffix = cex[mid * 2:]
+                break
+
+    suffixes = [tuple(suffix[len(suffix) - i - 1:]) for i in range(0, len(suffix), 2)]
+
+    # suffixes = [suffixes[-1]]
+    # print(len(cex), len(suffixes[-1]))
+    return suffixes
```

## aalpy/learning_algs/stochastic/StochasticLStar.py

```diff
@@ -1,220 +1,220 @@
-import time
-
-from aalpy.base import SUL, Oracle
-from aalpy.learning_algs.stochastic.DifferenceChecker import AdvancedHoeffdingChecker, HoeffdingChecker, \
-    ChiSquareChecker, DifferenceChecker
-from aalpy.learning_algs.stochastic.SamplingBasedObservationTable import SamplingBasedObservationTable
-from aalpy.learning_algs.stochastic.StochasticCexProcessing import stochastic_longest_prefix, stochastic_rs
-from aalpy.learning_algs.stochastic.StochasticTeacher import StochasticTeacher
-from aalpy.utils.HelperFunctions import print_learning_info, print_observation_table, get_cex_prefixes, \
-    get_available_oracles_and_err_msg
-
-from aalpy.utils.ModelChecking import stop_based_on_confidence
-
-strategies = ['classic', 'normal', 'chi2']
-cex_sampling_options = [None, 'bfs']
-cex_processing_options = [None, 'longest_prefix', 'rs']
-print_options = [0, 1, 2, 3]
-diff_checker_options = {'classic': HoeffdingChecker(),
-                        'chi2': ChiSquareChecker(),
-                        'normal': AdvancedHoeffdingChecker()}
-available_oracles, available_oracles_error_msg = get_available_oracles_and_err_msg()
-
-
-def run_stochastic_Lstar(input_alphabet, sul: SUL, eq_oracle: Oracle, target_unambiguity=0.99,
-                         min_rounds=10, max_rounds=200, automaton_type='mdp', strategy='normal',
-                         cex_processing=None, samples_cex_strategy=None, stopping_range_dict='strict', custom_oracle=False,
-                         return_data=False, property_based_stopping=None, n_c=20, n_resample=100, print_level=2):
-    """
-    Learning of Markov Decision Processes and Stochastic Mealy machines based on 'L*-Based Learning of Markov Decision
-    Processes' and 'Active Model Learning of Stochastic Reactive Systems' by Tappler et al.
-
-    Args:
-
-        input_alphabet: input alphabet
-
-        sul: system under learning
-
-        eq_oracle: equivalence oracle
-
-        target_unambiguity: target unambiguity value (default 0.99)
-
-        min_rounds: minimum number of learning rounds (Default value = 10)
-
-        max_rounds: if learning_rounds >= max_rounds, learning will stop (Default value = 200)
-
-        automaton_type: either 'mdp' or 'smm' (Default value = 'mdp')
-
-        strategy: either one of ['classic', 'normal', 'chi2'] or a object implementing DifferenceChecker class,
-            default value is 'normal'. Classic strategy is the one presented
-            in the seed paper, 'normal' is the updated version and chi2 is based on chi squared.
-
-        cex_processing: cex processing strategy, None , 'longest_prefix' or 'rs' (rs is experimental)
-
-        samples_cex_strategy: strategy for finding counterexamples in the trace tree. None, 'bfs' or
-            "random:<#traces to check:int>:<stop probability for single trace in [0,1)>" eg. random:200:0.2
-
-        stopping_range_dict: Values in form of a dictionary, or 'strict', 'relaxed' to use predefined stopping
-        criteria. Custom values: Dictionary where keys encode the last n unambiguity values which need to be in range
-        of its value in order to perform early stopping. Eg. {5: 0.001, 10: 0.01} would stop if last 5 hypothesis had
-        unambiguity values when max(last_5_vals) - (last_5_vals) <= 0.001.
-
-        property_based_stopping: A tuple containing (path to the properties file, correct values of each property,
-            allowed error for each property. Recommended one is 0.02 (2%)).
-
-        custom_oracle: if True, warning about oracle type will be removed and custom oracle can be used
-
-        return_data: if True, map containing all information like number of queries... will be returned
-            (Default value = False)
-
-        n_c: cutoff for a cell to be considered complete (Default value = 20), only used with 'classic' strategy
-
-        n_resample: resampling size (Default value = 100), only used with 'classic' strategy
-
-        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
-            (Default value = 2)
-
-
-    Returns:
-
-      learned MDP/SMM
-    """
-
-    assert samples_cex_strategy in cex_sampling_options or samples_cex_strategy.startswith('random')
-    assert cex_processing in cex_processing_options
-    assert automaton_type in {'mdp', 'smm'}
-    if not isinstance(stopping_range_dict, dict):
-        assert stopping_range_dict in {'strict', 'relaxed'}
-    if property_based_stopping:
-        assert len(property_based_stopping) == 3
-
-    if strategy in diff_checker_options:
-        compatibility_checker = diff_checker_options[strategy]
-    else:
-        assert isinstance(strategy, DifferenceChecker)
-        compatibility_checker = strategy
-
-    if not custom_oracle and type(eq_oracle) not in available_oracles:
-        raise SystemExit(available_oracles_error_msg)
-
-    if stopping_range_dict == 'strict':
-        stopping_range_dict = {12: 0.001, 18: 0.002, 25: 0.005, 30: 0.01, 35: 0.02}
-    elif stopping_range_dict == 'relaxed':
-        stopping_range_dict = {7: 0.001, 12: 0.003, 17: 0.005, 22: 0.01, 28: 0.02}
-
-    stochastic_teacher = StochasticTeacher(sul, n_c, eq_oracle, automaton_type, compatibility_checker,
-                                           samples_cex_strategy=samples_cex_strategy)
-
-    # This way all steps from eq. oracle will be added to the tree
-    eq_oracle.sul = stochastic_teacher.sul
-
-    observation_table = SamplingBasedObservationTable(input_alphabet, automaton_type,
-                                                      stochastic_teacher, compatibility_checker=compatibility_checker,
-                                                      strategy=strategy,
-                                                      cex_processing=cex_processing)
-
-    start_time = time.time()
-    eq_query_time = 0
-
-    # Ask queries for non-completed cells and update the observation table
-    observation_table.refine_not_completed_cells(n_resample, uniform=True)
-    observation_table.update_obs_table_with_freq_obs()
-
-    learning_rounds = 0
-
-    while True:
-        learning_rounds += 1
-
-        observation_table.make_closed_and_consistent()
-
-        hypothesis = observation_table.generate_hypothesis()
-
-        observation_table.trim(hypothesis)
-
-        # If there is no chaos state is not reachable, remove it from state set
-        chaos_cex_present = observation_table.chaos_counterexample(hypothesis)
-
-        if not chaos_cex_present:
-            if automaton_type == 'mdp':
-                hypothesis.states.remove(next(state for state in hypothesis.states if state.output == 'chaos'))
-            else:
-                hypothesis.states.remove(next(state for state in hypothesis.states if state.state_id == 'chaos'))
-
-        if print_level > 1:
-            print(f'Hypothesis: {learning_rounds}: {len(hypothesis.states)} states.')
-
-        if print_level == 3:
-            print_observation_table(observation_table, 'stochastic')
-
-        cex = None
-
-        if not chaos_cex_present:
-            eq_query_start = time.time()
-            cex = stochastic_teacher.equivalence_query(hypothesis)
-            eq_query_time += time.time() - eq_query_start
-
-        if cex:
-            if print_level == 3:
-                print('Counterexample', cex)
-            # get all prefixes and add them to the S set
-            if cex_processing is None:
-                for pre in get_cex_prefixes(cex, automaton_type):
-                    if pre not in observation_table.S:
-                        observation_table.S.append(pre)
-            else:
-                suffixes = None
-                if cex_processing == 'longest_prefix':
-                    prefixes = observation_table.S + list(observation_table.get_extended_s())
-                    suffixes = stochastic_longest_prefix(cex, prefixes)
-                elif cex_processing == 'rs':
-                    suffixes = stochastic_rs(sul, cex, hypothesis)
-                for suf in suffixes:
-                    if suf not in observation_table.E:
-                        observation_table.E.append(suf)
-                        break
-
-        # Ask queries for non-completed cells and update the observation table
-        refined = observation_table.refine_not_completed_cells(n_resample)
-        observation_table.update_obs_table_with_freq_obs()
-
-        if property_based_stopping and learning_rounds >= min_rounds:
-            # stop based on maximum allowed error
-            if stop_based_on_confidence(hypothesis, property_based_stopping, print_level):
-                break
-        else:
-            # stop based on number of unambiguous rows
-            stop_based_on_unambiguity = observation_table.stop(learning_rounds, chaos_cex_present, cex,
-                                                               stopping_range_dict,
-                                                               target_unambiguity=target_unambiguity,
-                                                               min_rounds=min_rounds, max_rounds=max_rounds,
-                                                               print_unambiguity=print_level > 1)
-            if stop_based_on_unambiguity:
-                break
-
-        if not refined:
-            break
-
-    total_time = round(time.time() - start_time, 2)
-    eq_query_time = round(eq_query_time, 2)
-    learning_time = round(total_time - eq_query_time, 2)
-
-    info = {
-        'learning_rounds': learning_rounds,
-        'automaton_size': len(hypothesis.states),
-        'queries_learning': stochastic_teacher.sul.num_queries - eq_oracle.num_queries,
-        'steps_learning': stochastic_teacher.sul.num_steps - eq_oracle.num_queries,
-        'queries_eq_oracle': eq_oracle.num_queries,
-        'steps_eq_oracle': eq_oracle.num_steps,
-        'learning_time': learning_time,
-        'eq_oracle_time': eq_query_time,
-        'total_time': total_time
-    }
-
-    if print_level > 0:
-        print_learning_info(info)
-
-    if return_data:
-        return hypothesis, info
-
-    return hypothesis
-
+import time
+
+from aalpy.base import SUL, Oracle
+from aalpy.learning_algs.stochastic.DifferenceChecker import AdvancedHoeffdingChecker, HoeffdingChecker, \
+    ChiSquareChecker, DifferenceChecker
+from aalpy.learning_algs.stochastic.SamplingBasedObservationTable import SamplingBasedObservationTable
+from aalpy.learning_algs.stochastic.StochasticCexProcessing import stochastic_longest_prefix, stochastic_rs
+from aalpy.learning_algs.stochastic.StochasticTeacher import StochasticTeacher
+from aalpy.utils.HelperFunctions import print_learning_info, print_observation_table, get_cex_prefixes, \
+    get_available_oracles_and_err_msg
+
+from aalpy.utils.ModelChecking import stop_based_on_confidence
+
+strategies = ['classic', 'normal', 'chi2']
+cex_sampling_options = [None, 'bfs']
+cex_processing_options = [None, 'longest_prefix', 'rs']
+print_options = [0, 1, 2, 3]
+diff_checker_options = {'classic': HoeffdingChecker(),
+                        'chi2': ChiSquareChecker(),
+                        'normal': AdvancedHoeffdingChecker()}
+available_oracles, available_oracles_error_msg = get_available_oracles_and_err_msg()
+
+
+def run_stochastic_Lstar(input_alphabet, sul: SUL, eq_oracle: Oracle, target_unambiguity=0.99,
+                         min_rounds=10, max_rounds=200, automaton_type='mdp', strategy='normal',
+                         cex_processing=None, samples_cex_strategy=None, stopping_range_dict='strict', custom_oracle=False,
+                         return_data=False, property_based_stopping=None, n_c=20, n_resample=100, print_level=2):
+    """
+    Learning of Markov Decision Processes and Stochastic Mealy machines based on 'L*-Based Learning of Markov Decision
+    Processes' and 'Active Model Learning of Stochastic Reactive Systems' by Tappler et al.
+
+    Args:
+
+        input_alphabet: input alphabet
+
+        sul: system under learning
+
+        eq_oracle: equivalence oracle
+
+        target_unambiguity: target unambiguity value (default 0.99)
+
+        min_rounds: minimum number of learning rounds (Default value = 10)
+
+        max_rounds: if learning_rounds >= max_rounds, learning will stop (Default value = 200)
+
+        automaton_type: either 'mdp' or 'smm' (Default value = 'mdp')
+
+        strategy: either one of ['classic', 'normal', 'chi2'] or a object implementing DifferenceChecker class,
+            default value is 'normal'. Classic strategy is the one presented
+            in the seed paper, 'normal' is the updated version and chi2 is based on chi squared.
+
+        cex_processing: cex processing strategy, None , 'longest_prefix' or 'rs' (rs is experimental)
+
+        samples_cex_strategy: strategy for finding counterexamples in the trace tree. None, 'bfs' or
+            "random:<#traces to check:int>:<stop probability for single trace in [0,1)>" eg. random:200:0.2
+
+        stopping_range_dict: Values in form of a dictionary, or 'strict', 'relaxed' to use predefined stopping
+        criteria. Custom values: Dictionary where keys encode the last n unambiguity values which need to be in range
+        of its value in order to perform early stopping. Eg. {5: 0.001, 10: 0.01} would stop if last 5 hypothesis had
+        unambiguity values when max(last_5_vals) - (last_5_vals) <= 0.001.
+
+        property_based_stopping: A tuple containing (path to the properties file, correct values of each property,
+            allowed error for each property. Recommended one is 0.02 (2%)).
+
+        custom_oracle: if True, warning about oracle type will be removed and custom oracle can be used
+
+        return_data: if True, map containing all information like number of queries... will be returned
+            (Default value = False)
+
+        n_c: cutoff for a cell to be considered complete (Default value = 20), only used with 'classic' strategy
+
+        n_resample: resampling size (Default value = 100), only used with 'classic' strategy
+
+        print_level: 0 - None, 1 - just results, 2 - current round and hypothesis size, 3 - educational/debug
+            (Default value = 2)
+
+
+    Returns:
+
+      learned MDP/SMM
+    """
+
+    assert samples_cex_strategy in cex_sampling_options or samples_cex_strategy.startswith('random')
+    assert cex_processing in cex_processing_options
+    assert automaton_type in {'mdp', 'smm'}
+    if not isinstance(stopping_range_dict, dict):
+        assert stopping_range_dict in {'strict', 'relaxed'}
+    if property_based_stopping:
+        assert len(property_based_stopping) == 3
+
+    if strategy in diff_checker_options:
+        compatibility_checker = diff_checker_options[strategy]
+    else:
+        assert isinstance(strategy, DifferenceChecker)
+        compatibility_checker = strategy
+
+    if not custom_oracle and type(eq_oracle) not in available_oracles:
+        raise SystemExit(available_oracles_error_msg)
+
+    if stopping_range_dict == 'strict':
+        stopping_range_dict = {12: 0.001, 18: 0.002, 25: 0.005, 30: 0.01, 35: 0.02}
+    elif stopping_range_dict == 'relaxed':
+        stopping_range_dict = {7: 0.001, 12: 0.003, 17: 0.005, 22: 0.01, 28: 0.02}
+
+    stochastic_teacher = StochasticTeacher(sul, n_c, eq_oracle, automaton_type, compatibility_checker,
+                                           samples_cex_strategy=samples_cex_strategy)
+
+    # This way all steps from eq. oracle will be added to the tree
+    eq_oracle.sul = stochastic_teacher.sul
+
+    observation_table = SamplingBasedObservationTable(input_alphabet, automaton_type,
+                                                      stochastic_teacher, compatibility_checker=compatibility_checker,
+                                                      strategy=strategy,
+                                                      cex_processing=cex_processing)
+
+    start_time = time.time()
+    eq_query_time = 0
+
+    # Ask queries for non-completed cells and update the observation table
+    observation_table.refine_not_completed_cells(n_resample, uniform=True)
+    observation_table.update_obs_table_with_freq_obs()
+
+    learning_rounds = 0
+
+    while True:
+        learning_rounds += 1
+
+        observation_table.make_closed_and_consistent()
+
+        hypothesis = observation_table.generate_hypothesis()
+
+        observation_table.trim(hypothesis)
+
+        # If there is no chaos state is not reachable, remove it from state set
+        chaos_cex_present = observation_table.chaos_counterexample(hypothesis)
+
+        if not chaos_cex_present:
+            if automaton_type == 'mdp':
+                hypothesis.states.remove(next(state for state in hypothesis.states if state.output == 'chaos'))
+            else:
+                hypothesis.states.remove(next(state for state in hypothesis.states if state.state_id == 'chaos'))
+
+        if print_level > 1:
+            print(f'Hypothesis: {learning_rounds}: {len(hypothesis.states)} states.')
+
+        if print_level == 3:
+            print_observation_table(observation_table, 'stochastic')
+
+        cex = None
+
+        if not chaos_cex_present:
+            eq_query_start = time.time()
+            cex = stochastic_teacher.equivalence_query(hypothesis)
+            eq_query_time += time.time() - eq_query_start
+
+        if cex:
+            if print_level == 3:
+                print('Counterexample', cex)
+            # get all prefixes and add them to the S set
+            if cex_processing is None:
+                for pre in get_cex_prefixes(cex, automaton_type):
+                    if pre not in observation_table.S:
+                        observation_table.S.append(pre)
+            else:
+                suffixes = None
+                if cex_processing == 'longest_prefix':
+                    prefixes = observation_table.S + list(observation_table.get_extended_s())
+                    suffixes = [stochastic_longest_prefix(cex, prefixes)[-1]]
+                elif cex_processing == 'rs':
+                    suffixes = stochastic_rs(sul, cex, hypothesis)
+                for suf in suffixes:
+                    if suf not in observation_table.E:
+                        observation_table.E.append(suf)
+                        break
+
+        # Ask queries for non-completed cells and update the observation table
+        refined = observation_table.refine_not_completed_cells(n_resample)
+        observation_table.update_obs_table_with_freq_obs()
+
+        if property_based_stopping and learning_rounds >= min_rounds:
+            # stop based on maximum allowed error
+            if stop_based_on_confidence(hypothesis, property_based_stopping, print_level):
+                break
+        else:
+            # stop based on number of unambiguous rows
+            stop_based_on_unambiguity = observation_table.stop(learning_rounds, chaos_cex_present, cex,
+                                                               stopping_range_dict,
+                                                               target_unambiguity=target_unambiguity,
+                                                               min_rounds=min_rounds, max_rounds=max_rounds,
+                                                               print_unambiguity=print_level > 1)
+            if stop_based_on_unambiguity:
+                break
+
+        if not refined:
+            break
+
+    total_time = round(time.time() - start_time, 2)
+    eq_query_time = round(eq_query_time, 2)
+    learning_time = round(total_time - eq_query_time, 2)
+
+    info = {
+        'learning_rounds': learning_rounds,
+        'automaton_size': len(hypothesis.states),
+        'queries_learning': stochastic_teacher.sul.num_queries - eq_oracle.num_queries,
+        'steps_learning': stochastic_teacher.sul.num_steps - eq_oracle.num_queries,
+        'queries_eq_oracle': eq_oracle.num_queries,
+        'steps_eq_oracle': eq_oracle.num_steps,
+        'learning_time': learning_time,
+        'eq_oracle_time': eq_query_time,
+        'total_time': total_time
+    }
+
+    if print_level > 0:
+        print_learning_info(info)
+
+    if return_data:
+        return hypothesis, info
+
+    return hypothesis
+
```

## aalpy/learning_algs/stochastic/StochasticTeacher.py

 * *Ordering differences only*

```diff
@@ -1,393 +1,393 @@
-from collections import defaultdict
-from random import choice, random
-
-from aalpy.base import SUL
-from aalpy.learning_algs.stochastic.DifferenceChecker import DifferenceChecker
-
-
-class StochasticSUL(SUL):
-    def __init__(self, sul, teacher):
-        super().__init__()
-        self.sul = sul
-        self.teacher = teacher
-
-    def pre(self):
-        self.num_queries += 1
-        self.teacher.back_to_root()
-        return self.sul.pre()
-
-    def post(self):
-        self.sul.post()
-
-    def step(self, letter):
-        self.num_steps += 1
-        out = self.sul.step(letter)
-        self.teacher.add(letter, out)
-        return out
-
-
-class Node:
-    """
-    Node of the cache/multiset of all traces.
-    """
-
-    def __init__(self, output):
-        self.output = output
-        self.frequency = 0
-        self.children = defaultdict(dict)
-        self.input_frequencies = defaultdict(int)
-
-    def get_child(self, inp, out):
-        """
-
-        Args:
-
-            inp: input
-            out: output
-
-        Returns:
-
-            Child with output that equals to `out` reached when performing `inp`. If such child does not exist,
-            return None.
-        """
-        if inp not in self.children.keys() or out not in self.children[inp].keys():
-            return None
-        return self.children[inp][out]
-
-    def get_frequency_sum(self, input_letter):
-        """
-        Returns:
-
-            number of times input was observed in current state
-        """
-        return self.input_frequencies[input_letter]
-
-    def get_output_frequencies(self, input_letter):
-        """
-        Args:
-
-            input_letter: input
-
-        Returns:
-
-            observed outputs and their frequencies for given `input_letter` in the current state
-
-        """
-        if input_letter not in self.children.keys():
-            return dict()
-        return {child.output: child.frequency for child in self.children[input_letter].values()}
-
-
-class StochasticTeacher:
-    """
-    The sampling-based teacher maintains a multiset of traces S for the estimation of output distributions.
-    Whenever new traces are sampled in the course of learning, they are added to S.
-    """
-
-    def __init__(self, sul: SUL, n_c, eq_oracle, automaton_type, compatibility_checker: DifferenceChecker,
-                 samples_cex_strategy=None):
-        self.automaton_type = automaton_type
-        if automaton_type == 'mdp':
-            self.initial_value = sul.query(tuple())
-            self.root_node = Node(self.initial_value[-1])
-        else:
-            self.root_node = Node(None)
-
-        self.sul = StochasticSUL(sul=sul, teacher=self)
-
-        self.eq_oracle = eq_oracle
-        self.n_c = n_c
-
-        self.curr_node = None
-        # cache
-        self.complete_query_cache = set()
-        self.compatibility_checker = compatibility_checker
-        self.samples_cex_strategy = samples_cex_strategy
-
-        # eq query cache
-        self.last_cex = None
-        self.last_tree_cex = None
-
-    def back_to_root(self):
-        self.curr_node = self.root_node
-
-    def add(self, inp, out):
-        """
-        Adds a input/output to the tree.
-
-        Args:
-
-            inp: input
-            out: output
-
-
-        """
-        self.curr_node.input_frequencies[inp] += 1
-        if inp not in self.curr_node.children.keys() or out not in self.curr_node.children[inp].keys():
-            node = Node(out)
-            self.curr_node.children[inp][out] = node
-
-        self.curr_node = self.curr_node.children[inp][out]
-        self.curr_node.frequency += 1
-
-    def frequency_query(self, s: tuple, e: tuple):
-        """Output frequencies observed after trace s + e.
-
-        Args:
-
-            s: sequence from S set
-            e: sequence from E set
-
-
-        Returns:
-
-            sum of output frequencies
-
-        """
-        if self.automaton_type == 'mdp':
-            s = s[1:]
-
-        input_seq = list(s[0::2] + e[0::2])
-        output_seq = list(s[1::2] + e[1::2])
-
-        last_input = input_seq.pop()
-
-        curr_node = self.root_node
-        for i, o in zip(input_seq, output_seq):
-            curr_node = curr_node.get_child(i, o)
-            if not curr_node:
-                return dict()
-
-        output_freq = curr_node.get_output_frequencies(last_input)
-        if sum(output_freq.values()) >= self.n_c:
-            self.complete_query_cache.add(s + e)
-        return output_freq
-
-    def complete_query(self, s: tuple, e: tuple):
-        """
-        Given a test sequences returns true if sufficient information is available to estimate an output distribution
-        from frequency queries; returns false otherwise.
-
-        Args:
-
-            s: sequence from S set
-            e: sequence from E set
-
-        Returns:
-
-            True if cell is completed, false otherwise
-
-        """
-
-        # extract inputs and outputs
-        if s + e in self.complete_query_cache:
-            return True
-
-        if self.automaton_type == 'mdp':
-            s = s[1:]
-
-        input_seq = list(s[0::2] + e[0::2])
-        output_seq = list(s[1::2] + e[1::2])
-
-        # get last input
-        last_input = input_seq.pop()
-
-        curr_node = self.root_node
-        for i, o in zip(input_seq, output_seq):
-            new_node = curr_node.get_child(i, o)
-            if not new_node:
-                curr_node_complete = curr_node.get_frequency_sum(i) >= self.n_c
-                # if curr_node_complete:
-                #     self.complete_query_cache.add(s + e)
-                return curr_node_complete
-            else:
-                curr_node = new_node
-
-        sum_freq = curr_node.get_frequency_sum(last_input)
-        if sum_freq >= self.n_c:
-            self.complete_query_cache.add(s + e)
-        return sum_freq >= self.n_c
-
-    def tree_query(self, pta_root):
-        """
-        Execute a refine query based on input/output trace. If at some point real outputs differ from expected
-        outputs, trace to that point is added to the tree, otherwise whole trace is executed.
-
-        Args:
-
-            pta_root: root of the PTA
-
-        Returns:
-
-            number of steps taken
-
-        """
-        self.sul.pre()
-        curr_node = pta_root
-
-        inputs = []
-        outputs = []
-
-        while True:
-
-            if curr_node.children:
-                frequency_sum = sum(curr_node.input_frequencies.values())
-                if frequency_sum == 0:
-                    # uniform sampling in case we have no information
-                    inp = choice(list(curr_node.children.keys()))
-                else:
-                    # use float random rather than integers to be able to work with non-integer frequency information
-                    selection_value = random() * frequency_sum
-                    inp = None
-                    for i in curr_node.input_frequencies.keys():
-                        inp = i
-                        selection_value -= curr_node.input_frequencies[i]
-                        if selection_value <= 0:
-                            break
-                    # curr_node.input_frequencies[inp] -= 1
-
-                inputs.append(inp)
-                out = self.sul.step(inp)
-                new_node = curr_node.get_child(inp, out)
-
-                if new_node:
-                    outputs.append(out)
-                    curr_node = new_node
-                else:
-                    self.sul.post()
-                    return
-            else:
-                curr_node = pta_root
-                for i, o in zip(inputs, outputs):
-                    self.curr_node.input_frequencies[i] -= 1
-                    curr_node = curr_node.get_child(i, o)
-                self.sul.post()
-                return
-
-    def single_dfs_for_cex(self, stop_prob, hypothesis):
-        curr_node = self.root_node
-        curr_state = hypothesis.initial_state
-        if self.automaton_type == "mdp":
-            trace = tuple(self.initial_value)
-        else:
-            trace = ()
-
-        while True:
-            rep_trace = curr_state.prefix
-            if trace != rep_trace:
-                for i in curr_node.children.keys():
-                    freq_in_tree = self.frequency_query(trace, (i,))
-                    freq_in_hyp = self.frequency_query(rep_trace, (i,))
-                    if self.compatibility_checker.are_cells_different(freq_in_tree, freq_in_hyp):
-                        return trace + (i,)
-            # choose next node randomly and return None if there is no next node
-            if not curr_node.children:
-                return None
-            i = choice(list(curr_node.children.keys()))
-            if not curr_node.children[i]:
-                return None
-            c = choice(list(curr_node.children[i].values()))
-            o = c.output
-            if self.automaton_type == 'mdp':
-                next_state = next(
-                    (out_state[0] for out_state in curr_state.transitions[i] if out_state[0].output == o), None)
-            else:
-                next_state = next((out_state[0] for out_state in curr_state.transitions[i] if out_state[1] == o),
-                                  None)
-            if not next_state:
-                return trace + (i,)
-            if random() <= stop_prob:
-                return None
-            else:
-                curr_node = c
-                curr_state = next_state
-                trace = trace + (i,) + (o,)
-
-    def dfs_for_cex_in_tree(self, hypothesis, nr_traces, stop_prob):
-        for i in range(nr_traces):
-            cex = self.single_dfs_for_cex(stop_prob, hypothesis)
-            if cex:
-                return cex
-        return None
-
-    def bfs_for_cex_in_tree(self, hypothesis):
-        # BFS for cex
-        if self.automaton_type == "mdp":
-            to_check = [(self.root_node, hypothesis.initial_state, tuple(self.initial_value))]
-        else:
-            to_check = [(self.root_node, hypothesis.initial_state, ())]
-
-        while to_check:
-            (curr_node, curr_state, trace) = to_check.pop(0)
-            rep_trace = curr_state.prefix
-            if trace != rep_trace:
-                for i in curr_node.children.keys():
-                    freq_in_tree = self.frequency_query(trace, (i,))
-                    freq_in_hyp = self.frequency_query(rep_trace, (i,))
-                    if self.compatibility_checker.are_cells_different(freq_in_tree, freq_in_hyp):
-                        return trace + (i,)
-            for i in curr_node.children.keys():
-                for c in curr_node.children[i].values():
-                    o = c.output
-                    if self.automaton_type == 'mdp':
-                        next_state = next((out_state[0] for out_state in curr_state.transitions[i]
-                                           if out_state[0].output == o), None)
-                    else:
-                        next_state = next((out_state[0] for out_state in curr_state.transitions[i]
-                                           if out_state[1] == o), None)
-                    if not next_state:
-                        return trace + (i,)
-                    new_trace = trace + (i,) + (o,)
-                    to_check.append((c, next_state, new_trace))
-        return None
-
-    def equivalence_query(self, hypothesis):
-        """
-        Finds and returns a counterexample
-
-        Args:
-
-            hypothesis: current hypothesis
-
-        Returns:
-
-            counterexample
-
-        """
-        if self.last_cex and not self.is_cex_processed(hypothesis, self.last_cex):
-            return self.last_cex
-
-        if self.samples_cex_strategy:
-            cex = None
-            if self.samples_cex_strategy == 'bfs':
-                cex = self.bfs_for_cex_in_tree(hypothesis)
-            elif self.samples_cex_strategy.startswith('random'):
-                split_strategy = self.samples_cex_strategy.split(":")
-                try:
-                    nr_traces = int(split_strategy[1])
-                    stop_prob = float(split_strategy[2])
-                    cex = self.dfs_for_cex_in_tree(hypothesis, nr_traces, stop_prob)
-                except Exception as e:
-                    print("Problem in random DFS for cex in samples:", e)
-            if cex:
-                self.last_cex = cex
-                self.eq_oracle.reset_counter()
-                return cex
-
-        cex = self.eq_oracle.find_cex(hypothesis)
-        if cex:  # remove last output
-            cex = cex[:-1]
-        self.last_cex = cex
-        return cex
-
-    def is_cex_processed(self, hypothesis, cex):
-        if self.automaton_type == 'mdp':
-            cex = cex[1:]
-        last_inp = cex[-1]
-        hypothesis.reset_to_initial()
-        for i in range(0, len(cex) - 1, 2):
-            o = hypothesis.step_to(cex[i], cex[i + 1])
-            if o is None:
-                return False
-        o = hypothesis.step(last_inp)
-        return o is not None
+from collections import defaultdict
+from random import choice, random
+
+from aalpy.base import SUL
+from aalpy.learning_algs.stochastic.DifferenceChecker import DifferenceChecker
+
+
+class StochasticSUL(SUL):
+    def __init__(self, sul, teacher):
+        super().__init__()
+        self.sul = sul
+        self.teacher = teacher
+
+    def pre(self):
+        self.num_queries += 1
+        self.teacher.back_to_root()
+        return self.sul.pre()
+
+    def post(self):
+        self.sul.post()
+
+    def step(self, letter):
+        self.num_steps += 1
+        out = self.sul.step(letter)
+        self.teacher.add(letter, out)
+        return out
+
+
+class Node:
+    """
+    Node of the cache/multiset of all traces.
+    """
+
+    def __init__(self, output):
+        self.output = output
+        self.frequency = 0
+        self.children = defaultdict(dict)
+        self.input_frequencies = defaultdict(int)
+
+    def get_child(self, inp, out):
+        """
+
+        Args:
+
+            inp: input
+            out: output
+
+        Returns:
+
+            Child with output that equals to `out` reached when performing `inp`. If such child does not exist,
+            return None.
+        """
+        if inp not in self.children.keys() or out not in self.children[inp].keys():
+            return None
+        return self.children[inp][out]
+
+    def get_frequency_sum(self, input_letter):
+        """
+        Returns:
+
+            number of times input was observed in current state
+        """
+        return self.input_frequencies[input_letter]
+
+    def get_output_frequencies(self, input_letter):
+        """
+        Args:
+
+            input_letter: input
+
+        Returns:
+
+            observed outputs and their frequencies for given `input_letter` in the current state
+
+        """
+        if input_letter not in self.children.keys():
+            return dict()
+        return {child.output: child.frequency for child in self.children[input_letter].values()}
+
+
+class StochasticTeacher:
+    """
+    The sampling-based teacher maintains a multiset of traces S for the estimation of output distributions.
+    Whenever new traces are sampled in the course of learning, they are added to S.
+    """
+
+    def __init__(self, sul: SUL, n_c, eq_oracle, automaton_type, compatibility_checker: DifferenceChecker,
+                 samples_cex_strategy=None):
+        self.automaton_type = automaton_type
+        if automaton_type == 'mdp':
+            self.initial_value = sul.query(tuple())
+            self.root_node = Node(self.initial_value[-1])
+        else:
+            self.root_node = Node(None)
+
+        self.sul = StochasticSUL(sul=sul, teacher=self)
+
+        self.eq_oracle = eq_oracle
+        self.n_c = n_c
+
+        self.curr_node = None
+        # cache
+        self.complete_query_cache = set()
+        self.compatibility_checker = compatibility_checker
+        self.samples_cex_strategy = samples_cex_strategy
+
+        # eq query cache
+        self.last_cex = None
+        self.last_tree_cex = None
+
+    def back_to_root(self):
+        self.curr_node = self.root_node
+
+    def add(self, inp, out):
+        """
+        Adds a input/output to the tree.
+
+        Args:
+
+            inp: input
+            out: output
+
+
+        """
+        self.curr_node.input_frequencies[inp] += 1
+        if inp not in self.curr_node.children.keys() or out not in self.curr_node.children[inp].keys():
+            node = Node(out)
+            self.curr_node.children[inp][out] = node
+
+        self.curr_node = self.curr_node.children[inp][out]
+        self.curr_node.frequency += 1
+
+    def frequency_query(self, s: tuple, e: tuple):
+        """Output frequencies observed after trace s + e.
+
+        Args:
+
+            s: sequence from S set
+            e: sequence from E set
+
+
+        Returns:
+
+            sum of output frequencies
+
+        """
+        if self.automaton_type == 'mdp':
+            s = s[1:]
+
+        input_seq = list(s[0::2] + e[0::2])
+        output_seq = list(s[1::2] + e[1::2])
+
+        last_input = input_seq.pop()
+
+        curr_node = self.root_node
+        for i, o in zip(input_seq, output_seq):
+            curr_node = curr_node.get_child(i, o)
+            if not curr_node:
+                return dict()
+
+        output_freq = curr_node.get_output_frequencies(last_input)
+        if sum(output_freq.values()) >= self.n_c:
+            self.complete_query_cache.add(s + e)
+        return output_freq
+
+    def complete_query(self, s: tuple, e: tuple):
+        """
+        Given a test sequences returns true if sufficient information is available to estimate an output distribution
+        from frequency queries; returns false otherwise.
+
+        Args:
+
+            s: sequence from S set
+            e: sequence from E set
+
+        Returns:
+
+            True if cell is completed, false otherwise
+
+        """
+
+        # extract inputs and outputs
+        if s + e in self.complete_query_cache:
+            return True
+
+        if self.automaton_type == 'mdp':
+            s = s[1:]
+
+        input_seq = list(s[0::2] + e[0::2])
+        output_seq = list(s[1::2] + e[1::2])
+
+        # get last input
+        last_input = input_seq.pop()
+
+        curr_node = self.root_node
+        for i, o in zip(input_seq, output_seq):
+            new_node = curr_node.get_child(i, o)
+            if not new_node:
+                curr_node_complete = curr_node.get_frequency_sum(i) >= self.n_c
+                # if curr_node_complete:
+                #     self.complete_query_cache.add(s + e)
+                return curr_node_complete
+            else:
+                curr_node = new_node
+
+        sum_freq = curr_node.get_frequency_sum(last_input)
+        if sum_freq >= self.n_c:
+            self.complete_query_cache.add(s + e)
+        return sum_freq >= self.n_c
+
+    def tree_query(self, pta_root):
+        """
+        Execute a refine query based on input/output trace. If at some point real outputs differ from expected
+        outputs, trace to that point is added to the tree, otherwise whole trace is executed.
+
+        Args:
+
+            pta_root: root of the PTA
+
+        Returns:
+
+            number of steps taken
+
+        """
+        self.sul.pre()
+        curr_node = pta_root
+
+        inputs = []
+        outputs = []
+
+        while True:
+
+            if curr_node.children:
+                frequency_sum = sum(curr_node.input_frequencies.values())
+                if frequency_sum == 0:
+                    # uniform sampling in case we have no information
+                    inp = choice(list(curr_node.children.keys()))
+                else:
+                    # use float random rather than integers to be able to work with non-integer frequency information
+                    selection_value = random() * frequency_sum
+                    inp = None
+                    for i in curr_node.input_frequencies.keys():
+                        inp = i
+                        selection_value -= curr_node.input_frequencies[i]
+                        if selection_value <= 0:
+                            break
+                    # curr_node.input_frequencies[inp] -= 1
+
+                inputs.append(inp)
+                out = self.sul.step(inp)
+                new_node = curr_node.get_child(inp, out)
+
+                if new_node:
+                    outputs.append(out)
+                    curr_node = new_node
+                else:
+                    self.sul.post()
+                    return
+            else:
+                curr_node = pta_root
+                for i, o in zip(inputs, outputs):
+                    self.curr_node.input_frequencies[i] -= 1
+                    curr_node = curr_node.get_child(i, o)
+                self.sul.post()
+                return
+
+    def single_dfs_for_cex(self, stop_prob, hypothesis):
+        curr_node = self.root_node
+        curr_state = hypothesis.initial_state
+        if self.automaton_type == "mdp":
+            trace = tuple(self.initial_value)
+        else:
+            trace = ()
+
+        while True:
+            rep_trace = curr_state.prefix
+            if trace != rep_trace:
+                for i in curr_node.children.keys():
+                    freq_in_tree = self.frequency_query(trace, (i,))
+                    freq_in_hyp = self.frequency_query(rep_trace, (i,))
+                    if self.compatibility_checker.are_cells_different(freq_in_tree, freq_in_hyp):
+                        return trace + (i,)
+            # choose next node randomly and return None if there is no next node
+            if not curr_node.children:
+                return None
+            i = choice(list(curr_node.children.keys()))
+            if not curr_node.children[i]:
+                return None
+            c = choice(list(curr_node.children[i].values()))
+            o = c.output
+            if self.automaton_type == 'mdp':
+                next_state = next(
+                    (out_state[0] for out_state in curr_state.transitions[i] if out_state[0].output == o), None)
+            else:
+                next_state = next((out_state[0] for out_state in curr_state.transitions[i] if out_state[1] == o),
+                                  None)
+            if not next_state:
+                return trace + (i,)
+            if random() <= stop_prob:
+                return None
+            else:
+                curr_node = c
+                curr_state = next_state
+                trace = trace + (i,) + (o,)
+
+    def dfs_for_cex_in_tree(self, hypothesis, nr_traces, stop_prob):
+        for i in range(nr_traces):
+            cex = self.single_dfs_for_cex(stop_prob, hypothesis)
+            if cex:
+                return cex
+        return None
+
+    def bfs_for_cex_in_tree(self, hypothesis):
+        # BFS for cex
+        if self.automaton_type == "mdp":
+            to_check = [(self.root_node, hypothesis.initial_state, tuple(self.initial_value))]
+        else:
+            to_check = [(self.root_node, hypothesis.initial_state, ())]
+
+        while to_check:
+            (curr_node, curr_state, trace) = to_check.pop(0)
+            rep_trace = curr_state.prefix
+            if trace != rep_trace:
+                for i in curr_node.children.keys():
+                    freq_in_tree = self.frequency_query(trace, (i,))
+                    freq_in_hyp = self.frequency_query(rep_trace, (i,))
+                    if self.compatibility_checker.are_cells_different(freq_in_tree, freq_in_hyp):
+                        return trace + (i,)
+            for i in curr_node.children.keys():
+                for c in curr_node.children[i].values():
+                    o = c.output
+                    if self.automaton_type == 'mdp':
+                        next_state = next((out_state[0] for out_state in curr_state.transitions[i]
+                                           if out_state[0].output == o), None)
+                    else:
+                        next_state = next((out_state[0] for out_state in curr_state.transitions[i]
+                                           if out_state[1] == o), None)
+                    if not next_state:
+                        return trace + (i,)
+                    new_trace = trace + (i,) + (o,)
+                    to_check.append((c, next_state, new_trace))
+        return None
+
+    def equivalence_query(self, hypothesis):
+        """
+        Finds and returns a counterexample
+
+        Args:
+
+            hypothesis: current hypothesis
+
+        Returns:
+
+            counterexample
+
+        """
+        if self.last_cex and not self.is_cex_processed(hypothesis, self.last_cex):
+            return self.last_cex
+
+        if self.samples_cex_strategy:
+            cex = None
+            if self.samples_cex_strategy == 'bfs':
+                cex = self.bfs_for_cex_in_tree(hypothesis)
+            elif self.samples_cex_strategy.startswith('random'):
+                split_strategy = self.samples_cex_strategy.split(":")
+                try:
+                    nr_traces = int(split_strategy[1])
+                    stop_prob = float(split_strategy[2])
+                    cex = self.dfs_for_cex_in_tree(hypothesis, nr_traces, stop_prob)
+                except Exception as e:
+                    print("Problem in random DFS for cex in samples:", e)
+            if cex:
+                self.last_cex = cex
+                self.eq_oracle.reset_counter()
+                return cex
+
+        cex = self.eq_oracle.find_cex(hypothesis)
+        if cex:  # remove last output
+            cex = cex[:-1]
+        self.last_cex = cex
+        return cex
+
+    def is_cex_processed(self, hypothesis, cex):
+        if self.automaton_type == 'mdp':
+            cex = cex[1:]
+        last_inp = cex[-1]
+        hypothesis.reset_to_initial()
+        for i in range(0, len(cex) - 1, 2):
+            o = hypothesis.step_to(cex[i], cex[i + 1])
+            if o is None:
+                return False
+        o = hypothesis.step(last_inp)
+        return o is not None
```

## aalpy/learning_algs/stochastic_passive/ActiveAleriga.py

 * *Ordering differences only*

```diff
@@ -1,88 +1,88 @@
-from abc import ABC, abstractmethod
-from random import randint, choice
-
-from aalpy.learning_algs import run_Alergia
-
-
-class Sampler(ABC):
-    """
-    Abstract class whose implementations are used to provide samples for active passive learning.
-    """
-
-    @abstractmethod
-    def sample(self, sul, model):
-        """
-        Abstract method implementing sampling strategy.
-
-        Args:
-
-            sul: system under learning
-            model: current learned model
-
-        Returns:
-
-            Data to be added to the data set for the passive learnign.
-
-        """
-        pass
-
-
-class RandomWordSampler(Sampler):
-    def __init__(self, num_walks, min_walk_len, max_walk_len):
-        self.num_walks = num_walks
-        self.min_walk_len = min_walk_len
-        self.max_walk_len = max_walk_len
-
-    def sample(self, sul, model):
-        input_al = list({el for s in model.states for el in s.transitions.keys()})
-        samples = []
-
-        for _ in range(self.num_walks):
-            walk_len = randint(self.min_walk_len, self.max_walk_len)
-            random_walk = tuple(choice(input_al) for _ in range(walk_len))
-
-            outputs = sul.query(random_walk)
-
-            sample = [outputs.pop(0)]
-            for i in range(len(outputs)):
-                sample.append((random_walk[i], outputs[i]))
-
-            samples.append(sample)
-
-        return samples
-
-
-def run_active_Alergia(data, sul, sampler, n_iter, eps=0.005, compatibility_checker=None, automaton_type='mdp',
-                       print_info=True):
-    """
-    Active version of IOAlergia algorithm. Based on intermediate hypothesis sampling on the system is performed.
-    Sampled data is added to the learning data and more accurate model is learned.
-    Proposed in "Aichernig and Tappler, Probabilistic Black-Box Reachability Checking"
-
-    Args:
-
-        data: initial learning data, in form [[O, (I,O), (I,O)...] ,...] where O is outputs and I input.
-        sul: system under learning which is basis for sampling
-        sampler: instance of Sampler class
-        n_iter: number of iterations of active learning
-        eps: epsilon value if the default checker is used. Look in run_Alergia for description
-        compatibility_checker: passed to run_Alergia, check there for description
-        automaton_type: either 'mdp' or 'smm' (Markov decision process or Stochastic Mealy Machine)
-        print_info: print current learning iteration
-
-    Returns:
-
-        learned MDP
-
-    """
-    model = None
-    for i in range(n_iter):
-        if print_info:
-            print(f'Active Alergia Iteration: {i}')
-        model = run_Alergia(data, automaton_type='mdp', eps=eps, compatibility_checker=compatibility_checker)
-
-        new_samples = sampler.sample(sul, model)
-        data.extend(new_samples)
-
-    return model
-
+from abc import ABC, abstractmethod
+from random import randint, choice
+
+from aalpy.learning_algs import run_Alergia
+
+
+class Sampler(ABC):
+    """
+    Abstract class whose implementations are used to provide samples for active passive learning.
+    """
+
+    @abstractmethod
+    def sample(self, sul, model):
+        """
+        Abstract method implementing sampling strategy.
+
+        Args:
+
+            sul: system under learning
+            model: current learned model
+
+        Returns:
+
+            Data to be added to the data set for the passive learnign.
+
+        """
+        pass
+
+
+class RandomWordSampler(Sampler):
+    def __init__(self, num_walks, min_walk_len, max_walk_len):
+        self.num_walks = num_walks
+        self.min_walk_len = min_walk_len
+        self.max_walk_len = max_walk_len
+
+    def sample(self, sul, model):
+        input_al = list({el for s in model.states for el in s.transitions.keys()})
+        samples = []
+
+        for _ in range(self.num_walks):
+            walk_len = randint(self.min_walk_len, self.max_walk_len)
+            random_walk = tuple(choice(input_al) for _ in range(walk_len))
+
+            outputs = sul.query(random_walk)
+
+            sample = [outputs.pop(0)]
+            for i in range(len(outputs)):
+                sample.append((random_walk[i], outputs[i]))
+
+            samples.append(sample)
+
+        return samples
+
+
+def run_active_Alergia(data, sul, sampler, n_iter, eps=0.005, compatibility_checker=None, automaton_type='mdp',
+                       print_info=True):
+    """
+    Active version of IOAlergia algorithm. Based on intermediate hypothesis sampling on the system is performed.
+    Sampled data is added to the learning data and more accurate model is learned.
+    Proposed in "Aichernig and Tappler, Probabilistic Black-Box Reachability Checking"
+
+    Args:
+
+        data: initial learning data, in form [[O, (I,O), (I,O)...] ,...] where O is outputs and I input.
+        sul: system under learning which is basis for sampling
+        sampler: instance of Sampler class
+        n_iter: number of iterations of active learning
+        eps: epsilon value if the default checker is used. Look in run_Alergia for description
+        compatibility_checker: passed to run_Alergia, check there for description
+        automaton_type: either 'mdp' or 'smm' (Markov decision process or Stochastic Mealy Machine)
+        print_info: print current learning iteration
+
+    Returns:
+
+        learned MDP
+
+    """
+    model = None
+    for i in range(n_iter):
+        if print_info:
+            print(f'Active Alergia Iteration: {i}')
+        model = run_Alergia(data, automaton_type='mdp', eps=eps, compatibility_checker=compatibility_checker)
+
+        new_samples = sampler.sample(sul, model)
+        data.extend(new_samples)
+
+    return model
+
```

## aalpy/learning_algs/stochastic_passive/Alergia.py

```diff
@@ -1,282 +1,283 @@
-import time
-from collections import defaultdict
-from bisect import insort
-
-from aalpy.automata import MarkovChain, MdpState, Mdp, McState, StochasticMealyState, \
-    StochasticMealyMachine
-from aalpy.learning_algs.stochastic_passive.CompatibilityChecker import HoeffdingCompatibility
-from aalpy.learning_algs.stochastic_passive.FPTA import create_fpta
-
-state_automaton_map = {'mc': (McState, MarkovChain), 'mdp': (MdpState, Mdp),
-                       'smm': (StochasticMealyState, StochasticMealyMachine)}
-
-
-class Alergia:
-    def __init__(self, data, automaton_type, eps=0.005, compatibility_checker=None, optimize_for='accuracy',
-                 print_info=False):
-        assert eps == 'auto' or 0 < eps <= 2
-        assert optimize_for in {'memory', 'accuracy'}
-
-        self.automaton_type = automaton_type
-        self.print_info = print_info
-        self.optimize_for = optimize_for
-
-        if eps == 'auto':
-            eps = 10 / sum(len(d) - 1 for d in data)  # len - 1 to ignore initial output
-
-        self.diff_checker = HoeffdingCompatibility(eps) if not compatibility_checker else compatibility_checker
-
-        pta_start = time.time()
-
-        self.immutableTreeRoot, self.mutableTreeRoot = create_fpta(data, automaton_type, optimize_for)
-
-        pta_time = round(time.time() - pta_start, 2)
-        if self.print_info:
-            print(f'PTA Construction Time:  {pta_time}')
-
-    def compatibility_test(self, a, b):
-        if self.automaton_type != 'smm' and a.output != b.output:
-            return False
-
-        if not a.children.values() or not b.children.values():
-            return True
-
-        if self.diff_checker.are_states_different(a, b):
-            return False
-
-        for el in set(a.children.keys()).intersection(b.children.keys()):
-            if not self.compatibility_test(a.children[el], b.children[el]):
-                return False
-
-        return True
-
-    def merge(self, q_r, q_b):
-        t_q_b = self.get_blue_node(q_b)
-        b_prefix = q_b.getPrefix()
-        to_update = self.mutableTreeRoot
-        for p in b_prefix[:-1]:
-            to_update = to_update.children[p]
-
-        to_update.children[b_prefix[-1]] = q_r
-
-        self.fold(q_r, q_b, t_q_b)
-
-    def fold(self, red, blue, blue_tree_node):
-        for i, c in blue.children.items():
-            if i in red.children.keys():
-                red.input_frequency[i] += blue_tree_node.input_frequency[i]
-                self.fold(red.children[i], blue.children[i], self.get_blue_node(blue.children[i]))
-            else:
-                red.children[i] = blue.children[i]
-                red.input_frequency[i] = blue_tree_node.input_frequency[i]
-
-    def run(self):
-        start_time = time.time()
-
-        red = [self.mutableTreeRoot]  # representative nodes and will be included in the final output model
-        blue = self.mutableTreeRoot.successors()  # intermediate successors scheduled for testing
-
-        while blue:
-            lex_min_blue = min(list(blue), key=lambda x: len(x.getPrefix()))
-            merged = False
-
-            for q_r in red:
-                if self.compatibility_test(self.get_blue_node(q_r), self.get_blue_node(lex_min_blue)):
-                    self.merge(q_r, lex_min_blue)
-                    merged = True
-                    break
-
-            if not merged:
-                insort(red, lex_min_blue)
-
-            blue.clear()
-
-            for r in red:
-                for s in r.successors():
-                    if s not in red:
-                        blue.append(s)
-
-        assert sorted(red, key=lambda x: len(x.getPrefix())) == red
-
-        self.normalize(red)
-
-        for i, r in enumerate(red):
-            r.state_id = f'q{i}'
-
-        if self.print_info:
-            print(f'Alergia Learning Time: {round(time.time() - start_time, 2)}')
-            print(f'Alergia Learned {len(red)} state automaton.')
-
-        return self.to_automaton(red)
-
-    def normalize(self, red):
-        red_sorted = sorted(list(red), key=lambda x: len(x.getPrefix()))
-        for r in red_sorted:
-            r.children_prob = dict()  # Initializing in here saves many unnecessary initializations
-            if self.automaton_type == 'mc':
-                total_output = sum(r.input_frequency.values())
-                for i in r.input_frequency.keys():
-                    r.children_prob[i] = r.input_frequency[i] / total_output
-            else:
-                outputs_per_input = defaultdict(int)
-                for io, freq in r.input_frequency.items():
-                    outputs_per_input[io[0]] += freq
-                for io in r.input_frequency.keys():
-                    r.children_prob[io] = r.input_frequency[io] / outputs_per_input[io[0]]
-
-    def get_blue_node(self, red_node):
-        if self.optimize_for == 'memory':
-            return red_node
-        blue = self.immutableTreeRoot
-        for p in red_node.getPrefix():
-            blue = blue.children[p]
-        return blue
-
-    def to_automaton(self, red):
-        s_c = state_automaton_map[self.automaton_type][0]
-        a_c = state_automaton_map[self.automaton_type][1]
-
-        states = []
-        initial_state = None
-        red_mdp_map = dict()
-        for s in red:
-            if self.automaton_type != 'smm':
-                automaton_state = s_c(s.state_id, output=s.output)
-            else:
-                automaton_state = s_c(s.state_id)
-
-            automaton_state.prefix = s.getPrefix()
-            states.append(automaton_state)
-            red_mdp_map[tuple(s.getPrefix())] = automaton_state
-            red_mdp_map[automaton_state.state_id] = s
-            if not s.getPrefix():
-                initial_state = automaton_state
-
-        for s in states:
-            red_eq = red_mdp_map[s.state_id]
-            for io, c in red_eq.children.items():
-                destination = red_mdp_map[tuple(c.getPrefix())]
-                i = io if self.automaton_type == 'mc' else io[0]
-                if self.automaton_type == 'mdp':
-                    s.transitions[i].append((destination, red_eq.children_prob[io]))
-                elif self.automaton_type == 'mc':
-                    s.transitions.append((destination, red_eq.children_prob[i]))
-                elif self.automaton_type == 'smm':
-                    s.transitions[i].append((destination, io[1], red_eq.children_prob[io]))
-                else:
-                    s.transitions[i] = destination
-
-        return a_c(initial_state, states)
-
-
-def run_Alergia(data, automaton_type, eps=0.005, compatibility_checker=None, optimize_for='accuracy', print_info=False):
-    """
-    Run Alergia or IOAlergia on provided data.
-
-    Args:
-
-        data: data either in a form [[I,I,I],[I,I,I],...] if learning Markov Chains or [[O,(I,O),(I,O)...],
-        [O,(I,O_,...],..,] if learning MDPs (I represents input, O output).
-        Note that in whole data first symbol of each entry should be the same (Initial output of the MDP/MC).
-
-        eps: epsilon value if you are using default HoeffdingCompatibility. If it is set to 'auto' it will be computed
-        as 10/(all steps in the data)
-
-        automaton_type: either 'mdp' if you wish to learn an MDP, 'mc' if you want to learn Markov Chain, or 'smm' if
-        you want to learn stochastic Mealy machine
-
-        optimize_for: either 'memory' or 'accuracy'. memory will use 50% less memory, but will be more inaccurate.
-
-        compatibility_checker: impl. of class CompatibilityChecker, HoeffdingCompatibility with eps value by default
-
-        (note: not interchangeable, depends on data)
-        print_info:
-
-    Returns:
-
-        mdp, smm, or markov chain
-    """
-    assert automaton_type in {'mdp', 'mc', 'smm'}
-    alergia = Alergia(data, eps=eps, automaton_type=automaton_type, optimize_for=optimize_for,
-                      compatibility_checker=compatibility_checker, print_info=print_info)
-    model = alergia.run()
-    del alergia.mutableTreeRoot, alergia.immutableTreeRoot, alergia
-    return model
-
-
-def run_JAlergia(path_to_data_file, automaton_type, path_to_jAlergia_jar, eps=0.005,
-                 heap_memory='-Xmx2048M', optimize_for='accuracy'):
-    """
-    Run Alergia or IOAlergia on provided data.
-
-    Args:
-
-        path_to_data_file: either a data in a list of lists or a path to file containing data. 
-        Form [[I,I,I],[I,I,I],...] if learning Markov Chains or
-        [[O,I,O,I,O...], [O,(I,O_,...],..,] if learning MDPs (I represents input, O output).
-        Note that in whole data first symbol of each entry should be the same (Initial output of the MDP/MC).
-
-        eps: epsilon value
-        
-        heap_memory: java heap memory flag, increase if heap is full
-        
-        optimize_for: either 'memory' or 'accuracy'. memory will use 50% less memory, but will be more inaccurate.
-
-        automaton_type: either 'mdp' if you wish to learn an MDP, 'mc' if you want to learn Markov Chain,
-         or 'smm' if you
-                        want to learn stochastic Mealy machine
-
-
-    Returns:
-
-        learnedModel
-    """
-    # TODO rename path_to_data_file to data in next versions of AALpy after 20. may
-    assert automaton_type in {'mdp', 'smm', 'mc'}
-    assert optimize_for in {'memory', 'accuracy'}
-
-    import os
-    import subprocess
-    from aalpy.utils.FileHandler import load_automaton_from_file
-
-    save_file = "jAlergiaModel.dot"
-    delete_tmp_file = False
-    if os.path.exists(save_file):
-        os.remove(save_file)
-
-    if os.path.exists(path_to_jAlergia_jar):
-        path_to_jAlergia_jar = os.path.abspath(path_to_jAlergia_jar)
-    else:
-        print(f'JAlergia jar not found at {path_to_jAlergia_jar}.')
-        return
-
-    if isinstance(path_to_data_file, str):
-        if os.path.exists(path_to_data_file):
-            abs_path = os.path.abspath(path_to_data_file)
-        else:
-            print('Input file not found.')
-            return
-    else:
-        if not isinstance(path_to_data_file, (list, tuple)):
-            print('Data should be either a list of sequences or a path to the data file.')
-        with open('jAlergiaInputs.txt', 'w') as f:
-            for seq in path_to_data_file:
-                f.write(','.join([str(s) for s in seq])+'\n')
-        delete_tmp_file = True
-        abs_path = os.path.abspath('jAlergiaInputs.txt')
-
-    optimize_for = optimize_for[:3]
-
-    subprocess.call(['java', heap_memory, '-jar', path_to_jAlergia_jar, '-input', abs_path, '-eps', str(eps), '-type',
-                     automaton_type, '-optim', optimize_for])
-
-    if not os.path.exists(save_file):
-        print("JAlergia error occurred.")
-        return
-
-    model = load_automaton_from_file(save_file, automaton_type=automaton_type)
-    os.remove(save_file)
-    if delete_tmp_file:
-        os.remove('jAlergiaInputs.txt')
-
-    return model
+import time
+from collections import defaultdict
+from bisect import insort
+
+from aalpy.automata import MarkovChain, MdpState, Mdp, McState, StochasticMealyState, \
+    StochasticMealyMachine
+from aalpy.learning_algs.stochastic_passive.CompatibilityChecker import HoeffdingCompatibility
+from aalpy.learning_algs.stochastic_passive.FPTA import create_fpta
+
+state_automaton_map = {'mc': (McState, MarkovChain), 'mdp': (MdpState, Mdp),
+                       'smm': (StochasticMealyState, StochasticMealyMachine)}
+
+
+class Alergia:
+    def __init__(self, data, automaton_type, eps=0.005, compatibility_checker=None, optimize_for='accuracy',
+                 print_info=False):
+        assert eps == 'auto' or 0 < eps <= 2
+        assert optimize_for in {'memory', 'accuracy'}
+
+        self.automaton_type = automaton_type
+        self.print_info = print_info
+        self.optimize_for = optimize_for
+
+        if eps == 'auto':
+            eps = 10 / sum(len(d) - 1 for d in data)  # len - 1 to ignore initial output
+
+        self.diff_checker = HoeffdingCompatibility(eps) if not compatibility_checker else compatibility_checker
+
+        pta_start = time.time()
+
+        self.immutableTreeRoot, self.mutableTreeRoot = create_fpta(data, automaton_type, optimize_for)
+
+        pta_time = round(time.time() - pta_start, 2)
+        if self.print_info:
+            print(f'PTA Construction Time:  {pta_time}')
+
+    def compatibility_test(self, a, b):
+        if self.automaton_type != 'smm' and a.output != b.output:
+            return False
+
+        if not a.children.values() or not b.children.values():
+            return True
+
+        if self.diff_checker.are_states_different(a, b):
+            return False
+
+        for el in set(a.children.keys()).intersection(b.children.keys()):
+            if not self.compatibility_test(a.children[el], b.children[el]):
+                return False
+
+        return True
+
+    def merge(self, q_r, q_b):
+        t_q_b = self.get_blue_node(q_b)
+        b_prefix = q_b.getPrefix()
+        to_update = self.mutableTreeRoot
+        for p in b_prefix[:-1]:
+            to_update = to_update.children[p]
+
+        to_update.children[b_prefix[-1]] = q_r
+
+        self.fold(q_r, q_b, t_q_b)
+
+    def fold(self, red, blue, blue_tree_node):
+        for i, c in blue.children.items():
+            if i in red.children.keys():
+                red.input_frequency[i] += blue_tree_node.input_frequency[i]
+                self.fold(red.children[i], blue.children[i], self.get_blue_node(blue.children[i]))
+            else:
+                red.children[i] = blue.children[i]
+                red.input_frequency[i] = blue_tree_node.input_frequency[i]
+
+    def run(self):
+        start_time = time.time()
+
+        red = [self.mutableTreeRoot]  # representative nodes and will be included in the final output model
+        blue = self.mutableTreeRoot.successors()  # intermediate successors scheduled for testing
+
+        while blue:
+            lex_min_blue = min(list(blue))
+            merged = False
+
+            for q_r in red:
+                if self.compatibility_test(self.get_blue_node(q_r), self.get_blue_node(lex_min_blue)):
+                    self.merge(q_r, lex_min_blue)
+                    merged = True
+                    break
+
+            if not merged:
+                insort(red, lex_min_blue)
+
+            blue.clear()
+
+            for r in red:
+                for s in r.successors():
+                    if s not in red:
+                        blue.append(s)
+
+        assert sorted(red, key=lambda x: len(x.getPrefix())) == red
+
+        self.normalize(red)
+
+        for i, r in enumerate(red):
+            r.state_id = f'q{i}'
+
+        if self.print_info:
+            print(f'Alergia Learning Time: {round(time.time() - start_time, 2)}')
+            print(f'Alergia Learned {len(red)} state automaton.')
+
+        return self.to_automaton(red)
+
+    def normalize(self, red):
+        red_sorted = sorted(list(red), key=lambda x: len(x.getPrefix()))
+        for r in red_sorted:
+            r.children_prob = dict()  # Initializing in here saves many unnecessary initializations
+            if self.automaton_type == 'mc':
+                total_output = sum(r.input_frequency.values())
+                for i in r.input_frequency.keys():
+                    r.children_prob[i] = r.input_frequency[i] / total_output
+            else:
+                outputs_per_input = defaultdict(int)
+                for io, freq in r.input_frequency.items():
+                    outputs_per_input[io[0]] += freq
+                for io in r.input_frequency.keys():
+                    r.children_prob[io] = r.input_frequency[io] / outputs_per_input[io[0]]
+
+    def get_blue_node(self, red_node):
+        if self.optimize_for == 'memory':
+            return red_node
+        blue = self.immutableTreeRoot
+        for p in red_node.getPrefix():
+            blue = blue.children[p]
+        return blue
+
+    def to_automaton(self, red):
+        s_c = state_automaton_map[self.automaton_type][0]
+        a_c = state_automaton_map[self.automaton_type][1]
+
+        states = []
+        initial_state = None
+        red_mdp_map = dict()
+        for s in red:
+            if self.automaton_type != 'smm':
+                automaton_state = s_c(s.state_id, output=s.output)
+            else:
+                automaton_state = s_c(s.state_id)
+
+            automaton_state.prefix = s.getPrefix()
+            states.append(automaton_state)
+            red_mdp_map[tuple(s.getPrefix())] = automaton_state
+            red_mdp_map[automaton_state.state_id] = s
+            if not s.getPrefix():
+                initial_state = automaton_state
+
+        for s in states:
+            red_eq = red_mdp_map[s.state_id]
+            for io, c in red_eq.children.items():
+                destination = red_mdp_map[tuple(c.getPrefix())]
+                i = io if self.automaton_type == 'mc' else io[0]
+                if self.automaton_type == 'mdp':
+                    s.transitions[i].append((destination, red_eq.children_prob[io]))
+                elif self.automaton_type == 'mc':
+                    s.transitions.append((destination, red_eq.children_prob[i]))
+                elif self.automaton_type == 'smm':
+                    s.transitions[i].append((destination, io[1], red_eq.children_prob[io]))
+                else:
+                    s.transitions[i] = destination
+
+        return a_c(initial_state, states)
+
+
+def run_Alergia(data, automaton_type, eps=0.005, compatibility_checker=None, optimize_for='accuracy', print_info=False):
+    """
+    Run Alergia or IOAlergia on provided data.
+
+    Args:
+
+        data: data either in a form [[I,I,I],[I,I,I],...] if learning Markov Chains or [[O,(I,O),(I,O)...],
+        [O,(I,O), (I, O)_,...],..,] if learning MDPs, or [[I,O,I,O...], [I,O_,...],..,] if learning SMMs
+         (I represents input, O output).
+        Note that in whole data first symbol of each entry should be the same (Initial output of the MDP/MC).
+
+        eps: epsilon value if you are using default HoeffdingCompatibility. If it is set to 'auto' it will be computed
+        as 10/(all steps in the data)
+
+        automaton_type: either 'mdp' if you wish to learn an MDP, 'mc' if you want to learn Markov Chain, or 'smm' if
+        you want to learn stochastic Mealy machine
+
+        optimize_for: either 'memory' or 'accuracy'. memory will use 50% less memory, but will be more inaccurate.
+
+        compatibility_checker: impl. of class CompatibilityChecker, HoeffdingCompatibility with eps value by default
+
+        (note: not interchangeable, depends on data)
+        print_info:
+
+    Returns:
+
+        mdp, smm, or markov chain
+    """
+    assert automaton_type in {'mdp', 'mc', 'smm'}
+    alergia = Alergia(data, eps=eps, automaton_type=automaton_type, optimize_for=optimize_for,
+                      compatibility_checker=compatibility_checker, print_info=print_info)
+    model = alergia.run()
+    del alergia.mutableTreeRoot, alergia.immutableTreeRoot, alergia
+    return model
+
+
+def run_JAlergia(path_to_data_file, automaton_type, path_to_jAlergia_jar, eps=0.005,
+                 heap_memory='-Xmx2048M', optimize_for='accuracy'):
+    """
+    Run Alergia or IOAlergia on provided data.
+
+    Args:
+
+        path_to_data_file: either a data in a list of lists or a path to file containing data. 
+        Form [[I,I,I],[I,I,I],...] if learning Markov Chains or
+        [[O,I,O,I,O...], [O,I,O_,...],..,] if learning MDPs (I represents input, O output), or
+        [[I,O,I,O...], [I,O_,...],..,] if learning SMMs.
+        Note that in whole data first symbol of each entry should be the same (Initial output of the MDP/MC).
+
+        eps: epsilon value
+        
+        heap_memory: java heap memory flag, increase if heap is full
+        
+        optimize_for: either 'memory' or 'accuracy'. memory will use 50% less memory, but will be more inaccurate.
+
+        automaton_type: either 'mdp' if you wish to learn an MDP, 'mc' if you want to learn Markov Chain,
+         or 'smm' if you
+                        want to learn stochastic Mealy machine
+
+
+    Returns:
+
+        learnedModel
+    """
+    assert automaton_type in {'mdp', 'smm', 'mc'}
+    assert optimize_for in {'memory', 'accuracy'}
+
+    import os
+    import subprocess
+    from aalpy.utils.FileHandler import load_automaton_from_file
+
+    save_file = "jAlergiaModel.dot"
+    delete_tmp_file = False
+    if os.path.exists(save_file):
+        os.remove(save_file)
+
+    if os.path.exists(path_to_jAlergia_jar):
+        path_to_jAlergia_jar = os.path.abspath(path_to_jAlergia_jar)
+    else:
+        print(f'JAlergia jar not found at {path_to_jAlergia_jar}.')
+        return
+
+    if isinstance(path_to_data_file, str):
+        if os.path.exists(path_to_data_file):
+            abs_path = os.path.abspath(path_to_data_file)
+        else:
+            print('Input file not found.')
+            return
+    else:
+        if not isinstance(path_to_data_file, (list, tuple)):
+            print('Data should be either a list of sequences or a path to the data file.')
+        with open('jAlergiaInputs.txt', 'w') as f:
+            for seq in path_to_data_file:
+                f.write(','.join([str(s) for s in seq])+'\n')
+        delete_tmp_file = True
+        abs_path = os.path.abspath('jAlergiaInputs.txt')
+
+    optimize_for = optimize_for[:3]
+
+    subprocess.call(['java', heap_memory, '-jar', path_to_jAlergia_jar, '-input', abs_path, '-eps', str(eps), '-type',
+                     automaton_type, '-optim', optimize_for])
+
+    if not os.path.exists(save_file):
+        print("JAlergia error occurred.")
+        return
+
+    model = load_automaton_from_file(save_file, automaton_type=automaton_type)
+    os.remove(save_file)
+    if delete_tmp_file:
+        os.remove('jAlergiaInputs.txt')
+
+    return model
```

## aalpy/learning_algs/stochastic_passive/CompatibilityChecker.py

```diff
@@ -1,34 +1,56 @@
-from abc import ABC, abstractmethod
-from math import sqrt, log
-
-from aalpy.learning_algs.stochastic_passive.FPTA import AlergiaPtaNode
-
-
-class CompatibilityChecker(ABC):
-
-    @abstractmethod
-    def are_states_different(self, a: AlergiaPtaNode, b: AlergiaPtaNode, **kwargs) -> bool:
-        pass
-
-
-class HoeffdingCompatibility(CompatibilityChecker):
-    def __init__(self, eps):
-        self.eps = eps
-
-    def are_states_different(self, a: AlergiaPtaNode, b: AlergiaPtaNode, **kwargs):
-        n1 = sum(a.input_frequency.values())
-        n2 = sum(b.input_frequency.values())
-
-        if n1 > 0 and n2 > 0:
-            a_children = a.children.keys()
-            b_children = b.children.keys()
-            outs = set(a_children).union(b_children)
-
-            for o in outs:
-                # for non existing keys set freq to 0
-                a_freq = a.input_frequency[o] if o in a_children else 0
-                b_freq = b.input_frequency[o] if o in b_children else 0
-
-                if abs(a_freq / n1 - b_freq / n2) > ((sqrt(1 / n1) + sqrt(1 / n2)) * sqrt(0.5 * log(2 / self.eps))):
-                    return True
-        return False
+from abc import ABC, abstractmethod
+from collections import defaultdict
+from math import sqrt, log
+
+from aalpy.learning_algs.stochastic_passive.FPTA import AlergiaPtaNode
+
+
+class CompatibilityChecker(ABC):
+
+    @abstractmethod
+    def are_states_different(self, a: AlergiaPtaNode, b: AlergiaPtaNode, **kwargs) -> bool:
+        pass
+
+
+def get_two_stage_dict(input_dict: dict):
+    ret = defaultdict(dict)
+    for (in_sym, out_sym), value in input_dict.items():
+        ret[in_sym][out_sym] = value
+    return ret
+
+
+class HoeffdingCompatibility(CompatibilityChecker):
+    def __init__(self, eps):
+        self.eps = eps
+
+    def hoeffding_bound(self, a: dict, b: dict):
+        n1 = sum(a.values())
+        n2 = sum(b.values())
+
+        if n1 * n2 == 0:
+            return False
+
+        for o in set(a.keys()).union(b.keys()):
+            a_freq = a[o] if o in a else 0
+            b_freq = b[o] if o in b else 0
+
+            if abs(a_freq / n1 - b_freq / n2) > ((sqrt(1 / n1) + sqrt(1 / n2)) * sqrt(0.5 * log(2 / self.eps))):
+                return True
+        return False
+
+    def are_states_different(self, a: AlergiaPtaNode, b: AlergiaPtaNode, **kwargs):
+        # no data available for any node
+        if len(a.input_frequency) * len(b.input_frequency) == 0:
+            return False
+
+        # assuming tuples are used for IOAlergia and not as Alergia outputs
+        if not isinstance(list(a.input_frequency.keys())[0], tuple):
+            return self.hoeffding_bound(a.input_frequency, b.input_frequency)
+
+        # IOAlergia: check hoeffding bound conditioned on inputs
+        a_dict, b_dict = (get_two_stage_dict(x.input_frequency) for x in [a, b])
+
+        for key in filter(lambda x: x in a_dict.keys(), b_dict.keys()):
+            if self.hoeffding_bound(a_dict[key], b_dict[key]):
+                return True
+        return False
```

## aalpy/learning_algs/stochastic_passive/FPTA.py

```diff
@@ -1,113 +1,116 @@
-from collections import defaultdict
-
-
-class AlergiaPtaNode:
-    __slots__ = ['output', 'input_frequency', 'children', 'parent_io', 'state_id', 'children_prob']
-
-    def __init__(self, output):
-        self.output = output
-        self.input_frequency = defaultdict(int)
-        self.children = dict()
-        self.parent_io = None
-        # # for visualization
-        self.state_id = None
-        self.children_prob = None
-
-    def getPrefix(self):
-        prefix = ()
-        curr_node = self
-        while curr_node.parent_io is not None:
-            prefix = (curr_node.parent_io[1],) + prefix
-            curr_node = curr_node.parent_io[0]
-        return prefix
-
-    def successors(self):
-        return list(self.children.values())
-
-    def __lt__(self, other):
-        return len(self.getPrefix()) < len(other.getPrefix())
-
-    def __le__(self, other):
-        return len(self.getPrefix()) <= len(other.getPrefix())
-
-    def __eq__(self, other):
-        return self.getPrefix() == other.getPrefix()
-
-
-def create_fpta(data, automaton_type, optimize_for='accuracy'):
-    # in case of single tree
-    if optimize_for == 'memory':
-        return create_single_fpta(data, automaton_type)
-
-    is_iofpta = True if automaton_type != 'mc' else False
-    seq_iter_index = 0 if automaton_type == 'smm' else 1
-    not_smm = automaton_type != 'smm'
-    # NOTE: This approach with _copy is not optimal, but a big time save from doing deep copy at the end
-    if automaton_type != 'smm':
-        root_node, root_copy = AlergiaPtaNode(data[0][0]), AlergiaPtaNode(data[0][0])
-    else:
-        root_node, root_copy = AlergiaPtaNode(None), AlergiaPtaNode(None)
-
-    root_node.parent_io, root_copy.parent_io = None, None
-    for seq in data:
-        if not_smm and seq[0] != root_node.output:
-            print('All strings should have the same initial output')
-            assert False
-        curr_node, curr_copy = root_node, root_copy
-
-        for el in seq[seq_iter_index:]:
-            if el not in curr_node.children.keys():
-                if not_smm:
-                    out = el if not is_iofpta else el[1]
-                    node, node_copy = AlergiaPtaNode(out), AlergiaPtaNode(out)
-                else:
-                    node, node_copy = AlergiaPtaNode(None), AlergiaPtaNode(None)
-
-                node.parent_io, node_copy.parent_io = (curr_node, el), (curr_copy, el)
-
-                curr_node.children[el] = node
-                curr_copy.children[el] = node_copy
-
-            curr_node.input_frequency[el] += 1
-            curr_node = curr_node.children[el]
-
-            curr_copy.input_frequency[el] += 1
-            curr_copy = curr_copy.children[el]
-
-    return root_node, root_copy
-
-
-def create_single_fpta(data, automaton_type):
-    is_iofpta = True if automaton_type != 'mc' else False
-    seq_iter_index = 0 if automaton_type == 'smm' else 1
-    not_smm = automaton_type != 'smm'
-    # NOTE: This approach with _copy is not optimal, but a big time save from doing deep copy at the end
-    if automaton_type != 'smm':
-        root_node = AlergiaPtaNode(data[0][0])
-    else:
-        root_node = AlergiaPtaNode(None)
-
-    root_node.parent_io = None
-
-    for seq in data:
-        if not_smm and seq[0] != root_node.output:
-            print('All strings should have the same initial output')
-            assert False
-        curr_node = root_node
-
-        for el in seq[seq_iter_index:]:
-            if el not in curr_node.children.keys():
-                if not_smm:
-                    out = el if not is_iofpta else el[1]
-                    node = AlergiaPtaNode(out)
-                else:
-                    node = AlergiaPtaNode(None)
-
-                node.parent_io = (curr_node, el)
-
-                curr_node.children[el] = node
-
-            curr_node.input_frequency[el] += 1
-            curr_node = curr_node.children[el]
-
-    return None, root_node
+from collections import defaultdict
+from functools import total_ordering
+
+
+@total_ordering
+class AlergiaPtaNode:
+    __slots__ = ['output', 'input_frequency', 'children', 'parent_io', 'state_id', 'children_prob']
+
+    def __init__(self, output):
+        self.output = output
+        self.input_frequency = defaultdict(int)
+        self.children = dict()
+        self.parent_io = None
+        # # for visualization
+        self.state_id = None
+        self.children_prob = None
+
+    def getPrefix(self):
+        prefix = ()
+        curr_node = self
+        while curr_node.parent_io is not None:
+            prefix = (curr_node.parent_io[1],) + prefix
+            curr_node = curr_node.parent_io[0]
+        return prefix
+
+    def successors(self):
+        return list(self.children.values())
+
+    def __lt__(self, other):
+        s_prefix, o_prefix = self.getPrefix(), other.getPrefix()
+        return (len(s_prefix), s_prefix) < (len(o_prefix), o_prefix)
+
+    def __le__(self, other):
+        return self < other or self == other
+
+    def __eq__(self, other):
+        return self.getPrefix() == other.getPrefix()
+
+
+def create_fpta(data, automaton_type, optimize_for='accuracy'):
+    # in case of single tree
+    if optimize_for == 'memory':
+        return create_single_fpta(data, automaton_type)
+
+    is_iofpta = True if automaton_type != 'mc' else False
+    seq_iter_index = 0 if automaton_type == 'smm' else 1
+    not_smm = automaton_type != 'smm'
+    # NOTE: This approach with _copy is not optimal, but a big time save from doing deep copy at the end
+    if automaton_type != 'smm':
+        root_node, root_copy = AlergiaPtaNode(data[0][0]), AlergiaPtaNode(data[0][0])
+    else:
+        root_node, root_copy = AlergiaPtaNode(None), AlergiaPtaNode(None)
+
+    root_node.parent_io, root_copy.parent_io = None, None
+    for seq in data:
+        if not_smm and seq[0] != root_node.output:
+            print('All strings should have the same initial output')
+            assert False
+        curr_node, curr_copy = root_node, root_copy
+
+        for el in seq[seq_iter_index:]:
+            if el not in curr_node.children.keys():
+                if not_smm:
+                    out = el if not is_iofpta else el[1]
+                    node, node_copy = AlergiaPtaNode(out), AlergiaPtaNode(out)
+                else:
+                    node, node_copy = AlergiaPtaNode(None), AlergiaPtaNode(None)
+
+                node.parent_io, node_copy.parent_io = (curr_node, el), (curr_copy, el)
+
+                curr_node.children[el] = node
+                curr_copy.children[el] = node_copy
+
+            curr_node.input_frequency[el] += 1
+            curr_node = curr_node.children[el]
+
+            curr_copy.input_frequency[el] += 1
+            curr_copy = curr_copy.children[el]
+
+    return root_node, root_copy
+
+
+def create_single_fpta(data, automaton_type):
+    is_iofpta = True if automaton_type != 'mc' else False
+    seq_iter_index = 0 if automaton_type == 'smm' else 1
+    not_smm = automaton_type != 'smm'
+    # NOTE: This approach with _copy is not optimal, but a big time save from doing deep copy at the end
+    if automaton_type != 'smm':
+        root_node = AlergiaPtaNode(data[0][0])
+    else:
+        root_node = AlergiaPtaNode(None)
+
+    root_node.parent_io = None
+
+    for seq in data:
+        if not_smm and seq[0] != root_node.output:
+            print('All strings should have the same initial output')
+            assert False
+        curr_node = root_node
+
+        for el in seq[seq_iter_index:]:
+            if el not in curr_node.children.keys():
+                if not_smm:
+                    out = el if not is_iofpta else el[1]
+                    node = AlergiaPtaNode(out)
+                else:
+                    node = AlergiaPtaNode(None)
+
+                node.parent_io = (curr_node, el)
+
+                curr_node.children[el] = node
+
+            curr_node.input_frequency[el] += 1
+            curr_node = curr_node.children[el]
+
+    return None, root_node
```

## aalpy/oracles/BreadthFirstExplorationEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,51 +1,51 @@
-from aalpy.base.Oracle import Oracle
-from aalpy.base.SUL import SUL
-
-from itertools import product
-from random import shuffle
-
-
-class BreadthFirstExplorationEqOracle(Oracle):
-    """
-    Breadth-First Exploration of all possible input combinations up to a certain depth.
-    Extremely inefficient equivalence oracle and should only be used for demonstrations.
-    """
-
-    def __init__(self, alphabet, sul: SUL, depth=5):
-        """
-        Args:
-
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            depth: depth of the tree
-        """
-
-        super().__init__(alphabet, sul)
-        self.depth = depth
-        self.queue = []
-
-        # generate all test-cases
-        for seq in product(self.alphabet, repeat=self.depth):
-            input_seq = tuple([i for sub in seq for i in sub])
-            self.queue.append(input_seq)
-
-        shuffle(self.queue)
-
-    def find_cex(self, hypothesis):
-
-        while self.queue:
-            test_case = self.queue.pop()
-            self.reset_hyp_and_sul(hypothesis)
-
-            for ind, letter in enumerate(test_case):
-                out_hyp = hypothesis.step(letter)
-                out_sul = self.sul.step(letter)
-                self.num_steps += 1
-
-                if out_hyp != out_sul:
-                    self.sul.post()
-                    return test_case[:ind + 1]
-
-        return None
+from aalpy.base.Oracle import Oracle
+from aalpy.base.SUL import SUL
+
+from itertools import product
+from random import shuffle
+
+
+class BreadthFirstExplorationEqOracle(Oracle):
+    """
+    Breadth-First Exploration of all possible input combinations up to a certain depth.
+    Extremely inefficient equivalence oracle and should only be used for demonstrations.
+    """
+
+    def __init__(self, alphabet, sul: SUL, depth=5):
+        """
+        Args:
+
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            depth: depth of the tree
+        """
+
+        super().__init__(alphabet, sul)
+        self.depth = depth
+        self.queue = []
+
+        # generate all test-cases
+        for seq in product(self.alphabet, repeat=self.depth):
+            input_seq = tuple([i for sub in seq for i in sub])
+            self.queue.append(input_seq)
+
+        shuffle(self.queue)
+
+    def find_cex(self, hypothesis):
+
+        while self.queue:
+            test_case = self.queue.pop()
+            self.reset_hyp_and_sul(hypothesis)
+
+            for ind, letter in enumerate(test_case):
+                out_hyp = hypothesis.step(letter)
+                out_sul = self.sul.step(letter)
+                self.num_steps += 1
+
+                if out_hyp != out_sul:
+                    self.sul.post()
+                    return test_case[:ind + 1]
+
+        return None
```

## aalpy/oracles/CacheBasedEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,97 +1,97 @@
-from aalpy.base import Oracle, SUL
-from aalpy.base.SUL import CacheSUL
-
-from random import choice
-
-
-class CacheBasedEqOracle(Oracle):
-    """
-    Equivalence oracle where test case selection is based on the multiset of all traces observed during learning and
-    conformance checking. Firstly all leaves of the tree are gathered and then random leaves are extended with a suffix
-    of length (max_tree_depth + 'depth_increase') - len(prefix), where prefix is a path to the leaf.
-    """
-
-    def __init__(self, alphabet: list, sul: SUL, num_walks=100, depth_increase=5, reset_after_cex=True):
-        """
-
-        Args:
-
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            num_walks: number of random walks to perform
-
-            depth_increase: length of random walk that exceeds the maximum depth of the tree
-
-            reset_after_cex: if False, total number of queries will equal num_walks, if True, in each execution of
-                find_cex method at most num_walks will be executed
-        """
-
-        super().__init__(alphabet, sul)
-        self.cache_tree = None
-        self.num_walks = num_walks
-        self.depth_increase = depth_increase
-        self.reset_after_cex = reset_after_cex
-        self.num_walks_done = 0
-
-    def find_cex(self, hypothesis):
-
-        assert isinstance(self.sul, CacheSUL)
-        self.cache_tree = self.sul.cache
-
-        paths_to_leaves = self.get_paths(self.cache_tree.root_node)
-        max_tree_depth = len(max(paths_to_leaves, key=len))
-
-        while self.num_walks_done < self.num_walks:
-            self.num_walks_done += 1
-            self.reset_hyp_and_sul(hypothesis)
-
-            prefix = choice(paths_to_leaves)
-            walk_len = (max_tree_depth + self.depth_increase) - len(prefix)
-            inputs = []
-            inputs.extend(prefix)
-
-            for p in prefix:
-                hypothesis.step(p)
-                self.sul.step(p)
-                self.num_steps += 1
-
-            for _ in range(walk_len):
-                inputs.append(choice(self.alphabet))
-
-                out_sul = self.sul.step(inputs[-1])
-                out_hyp = hypothesis.step(inputs[-1])
-                self.num_steps += 1
-
-                if out_sul != out_hyp:
-                    if self.reset_after_cex:
-                        self.num_walks_done = 0
-                    self.sul.post()
-                    return inputs
-
-        return None
-
-    def get_paths(self, t, paths=None, current_path=None):
-        """
-
-        Args:
-          t: 
-          paths:  (Default value = None)
-          current_path:  (Default value = None)
-
-        Returns:
-
-        """
-        if paths is None:
-            paths = []
-        if current_path is None:
-            current_path = []
-
-        if len(t.children) == 0:
-            paths.append(current_path)
-        else:
-            for inp, child in t.children.items():
-                current_path.append(inp)
-                self.get_paths(child, paths, list(current_path))
-        return paths
+from aalpy.base import Oracle, SUL
+from aalpy.base.SUL import CacheSUL
+
+from random import choice
+
+
+class CacheBasedEqOracle(Oracle):
+    """
+    Equivalence oracle where test case selection is based on the multiset of all traces observed during learning and
+    conformance checking. Firstly all leaves of the tree are gathered and then random leaves are extended with a suffix
+    of length (max_tree_depth + 'depth_increase') - len(prefix), where prefix is a path to the leaf.
+    """
+
+    def __init__(self, alphabet: list, sul: SUL, num_walks=100, depth_increase=5, reset_after_cex=True):
+        """
+
+        Args:
+
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            num_walks: number of random walks to perform
+
+            depth_increase: length of random walk that exceeds the maximum depth of the tree
+
+            reset_after_cex: if False, total number of queries will equal num_walks, if True, in each execution of
+                find_cex method at most num_walks will be executed
+        """
+
+        super().__init__(alphabet, sul)
+        self.cache_tree = None
+        self.num_walks = num_walks
+        self.depth_increase = depth_increase
+        self.reset_after_cex = reset_after_cex
+        self.num_walks_done = 0
+
+    def find_cex(self, hypothesis):
+
+        assert isinstance(self.sul, CacheSUL)
+        self.cache_tree = self.sul.cache
+
+        paths_to_leaves = self.get_paths(self.cache_tree.root_node)
+        max_tree_depth = len(max(paths_to_leaves, key=len))
+
+        while self.num_walks_done < self.num_walks:
+            self.num_walks_done += 1
+            self.reset_hyp_and_sul(hypothesis)
+
+            prefix = choice(paths_to_leaves)
+            walk_len = (max_tree_depth + self.depth_increase) - len(prefix)
+            inputs = []
+            inputs.extend(prefix)
+
+            for p in prefix:
+                hypothesis.step(p)
+                self.sul.step(p)
+                self.num_steps += 1
+
+            for _ in range(walk_len):
+                inputs.append(choice(self.alphabet))
+
+                out_sul = self.sul.step(inputs[-1])
+                out_hyp = hypothesis.step(inputs[-1])
+                self.num_steps += 1
+
+                if out_sul != out_hyp:
+                    if self.reset_after_cex:
+                        self.num_walks_done = 0
+                    self.sul.post()
+                    return inputs
+
+        return None
+
+    def get_paths(self, t, paths=None, current_path=None):
+        """
+
+        Args:
+          t: 
+          paths:  (Default value = None)
+          current_path:  (Default value = None)
+
+        Returns:
+
+        """
+        if paths is None:
+            paths = []
+        if current_path is None:
+            current_path = []
+
+        if len(t.children) == 0:
+            paths.append(current_path)
+        else:
+            for inp, child in t.children.items():
+                current_path.append(inp)
+                self.get_paths(child, paths, list(current_path))
+        return paths
```

## aalpy/oracles/RandomWalkEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,89 +1,89 @@
-import random
-
-from aalpy.automata import Onfsm, Mdp, StochasticMealyMachine
-from aalpy.base import Oracle, SUL
-
-automaton_dict = {Onfsm: 'onfsm', Mdp: 'mdp', StochasticMealyMachine: 'smm'}
-
-
-class RandomWalkEqOracle(Oracle):
-    """
-    Equivalence oracle where queries contain random inputs. After every step, 'reset_prob' determines the probability
-    that the system will reset and a new query asked.
-    """
-
-    def __init__(self, alphabet: list, sul: SUL, num_steps=5000, reset_after_cex=True, reset_prob=0.09):
-        """
-
-        Args:
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            num_steps: number of steps to be preformed
-
-            reset_after_cex: if true, num_steps will be preformed after every counter example, else the total number
-                or steps will equal to num_steps
-
-            reset_prob: probability that the new query will be asked
-        """
-
-        super().__init__(alphabet, sul)
-        self.step_limit = num_steps
-        self.reset_after_cex = reset_after_cex
-        self.reset_prob = reset_prob
-        self.random_steps_done = 0
-        self.automata_type = None
-
-    def find_cex(self, hypothesis):
-        if not self.automata_type:
-            self.automata_type = automaton_dict.get(type(hypothesis), 'det')
-
-        inputs = []
-        outputs = []
-        self.reset_hyp_and_sul(hypothesis)
-
-        while self.random_steps_done < self.step_limit:
-            self.num_steps += 1
-            self.random_steps_done += 1
-
-            if random.random() <= self.reset_prob:
-                self.reset_hyp_and_sul(hypothesis)
-                inputs.clear()
-                outputs.clear()
-
-            inputs.append(random.choice(self.alphabet))
-
-            out_sul = self.sul.step(inputs[-1])
-            outputs.append(out_sul)
-
-            if self.automata_type == 'det':
-                out_hyp = hypothesis.step(inputs[-1])
-            else:
-                out_hyp = hypothesis.step_to(inputs[-1], out_sul)
-
-            if self.automata_type == 'det' and out_sul != out_hyp:
-                if self.reset_after_cex:
-                    self.random_steps_done = 0
-
-                self.sul.post()
-                return inputs
-            elif out_hyp is None:
-                if self.reset_after_cex:
-                    self.random_steps_done = 0
-                self.sul.post()
-
-                if self.automata_type == 'onfsm':
-                    return inputs, outputs
-                else:
-                    # hypothesis is MDP or SMM
-                    cex = [hypothesis.initial_state.output] if self.automata_type == 'mdp' else []
-                    for i, o in zip(inputs, outputs):
-                        cex.extend([i, o])
-                    return cex
-
-        return None
-
-    def reset_counter(self):
-        if self.reset_after_cex:
+import random
+
+from aalpy.automata import Onfsm, Mdp, StochasticMealyMachine
+from aalpy.base import Oracle, SUL
+
+automaton_dict = {Onfsm: 'onfsm', Mdp: 'mdp', StochasticMealyMachine: 'smm'}
+
+
+class RandomWalkEqOracle(Oracle):
+    """
+    Equivalence oracle where queries contain random inputs. After every step, 'reset_prob' determines the probability
+    that the system will reset and a new query asked.
+    """
+
+    def __init__(self, alphabet: list, sul: SUL, num_steps=5000, reset_after_cex=True, reset_prob=0.09):
+        """
+
+        Args:
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            num_steps: number of steps to be preformed
+
+            reset_after_cex: if true, num_steps will be preformed after every counter example, else the total number
+                or steps will equal to num_steps
+
+            reset_prob: probability that the new query will be asked
+        """
+
+        super().__init__(alphabet, sul)
+        self.step_limit = num_steps
+        self.reset_after_cex = reset_after_cex
+        self.reset_prob = reset_prob
+        self.random_steps_done = 0
+        self.automata_type = None
+
+    def find_cex(self, hypothesis):
+        if not self.automata_type:
+            self.automata_type = automaton_dict.get(type(hypothesis), 'det')
+
+        inputs = []
+        outputs = []
+        self.reset_hyp_and_sul(hypothesis)
+
+        while self.random_steps_done < self.step_limit:
+            self.num_steps += 1
+            self.random_steps_done += 1
+
+            if random.random() <= self.reset_prob:
+                self.reset_hyp_and_sul(hypothesis)
+                inputs.clear()
+                outputs.clear()
+
+            inputs.append(random.choice(self.alphabet))
+
+            out_sul = self.sul.step(inputs[-1])
+            outputs.append(out_sul)
+
+            if self.automata_type == 'det':
+                out_hyp = hypothesis.step(inputs[-1])
+            else:
+                out_hyp = hypothesis.step_to(inputs[-1], out_sul)
+
+            if self.automata_type == 'det' and out_sul != out_hyp:
+                if self.reset_after_cex:
+                    self.random_steps_done = 0
+
+                self.sul.post()
+                return inputs
+            elif out_hyp is None:
+                if self.reset_after_cex:
+                    self.random_steps_done = 0
+                self.sul.post()
+
+                if self.automata_type == 'onfsm':
+                    return inputs, outputs
+                else:
+                    # hypothesis is MDP or SMM
+                    cex = [hypothesis.initial_state.output] if self.automata_type == 'mdp' else []
+                    for i, o in zip(inputs, outputs):
+                        cex.extend([i, o])
+                    return cex
+
+        return None
+
+    def reset_counter(self):
+        if self.reset_after_cex:
             self.random_steps_done = 0
```

## aalpy/oracles/RandomWordEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,95 +1,95 @@
-from statistics import mean
-
-from aalpy.automata import Onfsm, Mdp, StochasticMealyMachine
-from aalpy.base import Oracle, SUL
-from random import randint, choice
-
-automaton_dict = {Onfsm: 'onfsm', Mdp: 'mdp', StochasticMealyMachine: 'smm'}
-
-
-class RandomWordEqOracle(Oracle):
-    """
-    Equivalence oracle where queries are of random length in a predefined range.
-    """
-
-    def __init__(self, alphabet: list, sul: SUL, num_walks=500, min_walk_len=10, max_walk_len=30,
-                 reset_after_cex=True):
-        """
-        Args:
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            num_walks: number of walks to perform during search for cex
-
-            min_walk_len: minimum length of each walk
-
-            max_walk_len: maximum length of each walk
-
-            reset_after_cex: if True, num_walks will be preformed after every counter example, else the total number
-                or walks will equal to num_walks
-        """
-
-        super().__init__(alphabet, sul)
-        self.num_walks = num_walks
-        self.min_walk_len = min_walk_len
-        self.max_walk_len = max_walk_len
-        self.reset_after_cex = reset_after_cex
-        self.num_walks_done = 0
-        self.automata_type = None
-
-        self.walk_lengths = [randint(min_walk_len, max_walk_len) for _ in range(num_walks)]
-
-    def find_cex(self, hypothesis):
-        if not self.automata_type:
-            self.automata_type = automaton_dict.get(type(hypothesis), 'det')
-
-        while self.num_walks_done < self.num_walks:
-            inputs = []
-            outputs = []
-            self.reset_hyp_and_sul(hypothesis)
-            self.num_walks_done += 1
-
-            num_steps = self.walk_lengths.pop(0)
-
-            for _ in range(num_steps):
-                inputs.append(choice(self.alphabet))
-
-                out_sul = self.sul.step(inputs[-1])
-                if self.automata_type == 'det':
-                    out_hyp = hypothesis.step(inputs[-1])
-                else:
-                    out_hyp = hypothesis.step_to(inputs[-1], out_sul)
-                    outputs.append(out_sul)
-
-                self.num_steps += 1
-
-                if self.automata_type == 'det' and out_sul != out_hyp:
-                    if self.reset_after_cex:
-                        self.walk_lengths = [randint(self.min_walk_len, self.max_walk_len) for _ in range(self.num_walks)]
-                        self.num_walks_done = 0
-
-                    self.sul.post()
-                    return inputs
-
-                elif out_hyp is None:
-                    self.sul.post()
-
-                    if self.reset_after_cex:
-                        self.walk_lengths = [randint(self.min_walk_len, self.max_walk_len) for _ in range(self.num_walks)]
-                        self.num_walks_done = 0
-
-                    if self.automata_type == 'onfsm':
-                        return inputs, outputs
-                    else:
-                        # hypothesis is MDP or SMM
-                        cex = [hypothesis.initial_state.output] if self.automata_type == 'mdp' else []
-                        for i, o in zip(inputs, outputs):
-                            cex.extend([i, o])
-                        return cex
-
-        return None
-
-    def reset_counter(self):
-        if self.reset_after_cex:
-            self.num_walks_done = 0
+from statistics import mean
+
+from aalpy.automata import Onfsm, Mdp, StochasticMealyMachine
+from aalpy.base import Oracle, SUL
+from random import randint, choice
+
+automaton_dict = {Onfsm: 'onfsm', Mdp: 'mdp', StochasticMealyMachine: 'smm'}
+
+
+class RandomWordEqOracle(Oracle):
+    """
+    Equivalence oracle where queries are of random length in a predefined range.
+    """
+
+    def __init__(self, alphabet: list, sul: SUL, num_walks=500, min_walk_len=10, max_walk_len=30,
+                 reset_after_cex=True):
+        """
+        Args:
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            num_walks: number of walks to perform during search for cex
+
+            min_walk_len: minimum length of each walk
+
+            max_walk_len: maximum length of each walk
+
+            reset_after_cex: if True, num_walks will be preformed after every counter example, else the total number
+                or walks will equal to num_walks
+        """
+
+        super().__init__(alphabet, sul)
+        self.num_walks = num_walks
+        self.min_walk_len = min_walk_len
+        self.max_walk_len = max_walk_len
+        self.reset_after_cex = reset_after_cex
+        self.num_walks_done = 0
+        self.automata_type = None
+
+        self.walk_lengths = [randint(min_walk_len, max_walk_len) for _ in range(num_walks)]
+
+    def find_cex(self, hypothesis):
+        if not self.automata_type:
+            self.automata_type = automaton_dict.get(type(hypothesis), 'det')
+
+        while self.num_walks_done < self.num_walks:
+            inputs = []
+            outputs = []
+            self.reset_hyp_and_sul(hypothesis)
+            self.num_walks_done += 1
+
+            num_steps = self.walk_lengths.pop(0)
+
+            for _ in range(num_steps):
+                inputs.append(choice(self.alphabet))
+
+                out_sul = self.sul.step(inputs[-1])
+                if self.automata_type == 'det':
+                    out_hyp = hypothesis.step(inputs[-1])
+                else:
+                    out_hyp = hypothesis.step_to(inputs[-1], out_sul)
+                    outputs.append(out_sul)
+
+                self.num_steps += 1
+
+                if self.automata_type == 'det' and out_sul != out_hyp:
+                    if self.reset_after_cex:
+                        self.walk_lengths = [randint(self.min_walk_len, self.max_walk_len) for _ in range(self.num_walks)]
+                        self.num_walks_done = 0
+
+                    self.sul.post()
+                    return inputs
+
+                elif out_hyp is None:
+                    self.sul.post()
+
+                    if self.reset_after_cex:
+                        self.walk_lengths = [randint(self.min_walk_len, self.max_walk_len) for _ in range(self.num_walks)]
+                        self.num_walks_done = 0
+
+                    if self.automata_type == 'onfsm':
+                        return inputs, outputs
+                    else:
+                        # hypothesis is MDP or SMM
+                        cex = [hypothesis.initial_state.output] if self.automata_type == 'mdp' else []
+                        for i, o in zip(inputs, outputs):
+                            cex.extend([i, o])
+                        return cex
+
+        return None
+
+    def reset_counter(self):
+        if self.reset_after_cex:
+            self.num_walks_done = 0
```

## aalpy/oracles/StatePrefixEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,78 +1,78 @@
-import random
-
-from aalpy.base.Oracle import Oracle
-from aalpy.base.SUL import SUL
-
-
-class StatePrefixEqOracle(Oracle):
-    """
-    Equivalence oracle that achieves guided exploration by starting random walks from each state a walk_per_state
-    times. Starting the random walk ensures that all states are reached at least walk_per_state times and that their
-    surrounding is randomly explored. Note that each state serves as a root of random exploration of maximum length
-    rand_walk_len exactly walk_per_state times during learning. Therefore excessive testing of initial states is
-    avoided.
-    """
-    def __init__(self, alphabet: list, sul: SUL, walks_per_state=10, walk_len=12, depth_first=False):
-        """
-        Args:
-
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            walks_per_state:individual walks per state of the automaton over the whole learning process
-
-            walk_len:length of random walk
-
-            depth_first:first explore newest states
-        """
-
-        super().__init__(alphabet, sul)
-        self.walks_per_state = walks_per_state
-        self.steps_per_walk = walk_len
-        self.depth_first = depth_first
-
-        self.freq_dict = dict()
-
-    def find_cex(self, hypothesis):
-
-        states_to_cover = []
-        for state in hypothesis.states:
-            if state.prefix is None:
-                state.prefix = hypothesis.get_shortest_path(hypothesis.initial_state, state)
-            if state.prefix not in self.freq_dict.keys():
-                self.freq_dict[state.prefix] = 0
-
-            states_to_cover.extend([state] * (self.walks_per_state - self.freq_dict[state.prefix]))
-
-        if self.depth_first:
-            # reverse sort the states by length of their access sequences
-            # first do the random walk on the state with longest access sequence
-            states_to_cover.sort(key=lambda x: len(x.prefix), reverse=True)
-        else:
-            random.shuffle(states_to_cover)
-
-        for state in states_to_cover:
-            self.freq_dict[state.prefix] = self.freq_dict[state.prefix] + 1
-
-            self.reset_hyp_and_sul(hypothesis)
-
-            prefix = state.prefix
-            for p in prefix:
-                hypothesis.step(p)
-                self.sul.step(p)
-                self.num_steps += 1
-
-            suffix = ()
-            for _ in range(self.steps_per_walk):
-                suffix += (random.choice(self.alphabet),)
-
-                out_sul = self.sul.step(suffix[-1])
-                out_hyp = hypothesis.step(suffix[-1])
-                self.num_steps += 1
-
-                if out_sul != out_hyp:
-                    self.sul.post()
-                    return prefix + suffix
-
-        return None
+import random
+
+from aalpy.base.Oracle import Oracle
+from aalpy.base.SUL import SUL
+
+
+class StatePrefixEqOracle(Oracle):
+    """
+    Equivalence oracle that achieves guided exploration by starting random walks from each state a walk_per_state
+    times. Starting the random walk ensures that all states are reached at least walk_per_state times and that their
+    surrounding is randomly explored. Note that each state serves as a root of random exploration of maximum length
+    rand_walk_len exactly walk_per_state times during learning. Therefore excessive testing of initial states is
+    avoided.
+    """
+    def __init__(self, alphabet: list, sul: SUL, walks_per_state=10, walk_len=12, depth_first=False):
+        """
+        Args:
+
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            walks_per_state:individual walks per state of the automaton over the whole learning process
+
+            walk_len:length of random walk
+
+            depth_first:first explore newest states
+        """
+
+        super().__init__(alphabet, sul)
+        self.walks_per_state = walks_per_state
+        self.steps_per_walk = walk_len
+        self.depth_first = depth_first
+
+        self.freq_dict = dict()
+
+    def find_cex(self, hypothesis):
+
+        states_to_cover = []
+        for state in hypothesis.states:
+            if state.prefix is None:
+                state.prefix = hypothesis.get_shortest_path(hypothesis.initial_state, state)
+            if state.prefix not in self.freq_dict.keys():
+                self.freq_dict[state.prefix] = 0
+
+            states_to_cover.extend([state] * (self.walks_per_state - self.freq_dict[state.prefix]))
+
+        if self.depth_first:
+            # reverse sort the states by length of their access sequences
+            # first do the random walk on the state with longest access sequence
+            states_to_cover.sort(key=lambda x: len(x.prefix), reverse=True)
+        else:
+            random.shuffle(states_to_cover)
+
+        for state in states_to_cover:
+            self.freq_dict[state.prefix] = self.freq_dict[state.prefix] + 1
+
+            self.reset_hyp_and_sul(hypothesis)
+
+            prefix = state.prefix
+            for p in prefix:
+                hypothesis.step(p)
+                self.sul.step(p)
+                self.num_steps += 1
+
+            suffix = ()
+            for _ in range(self.steps_per_walk):
+                suffix += (random.choice(self.alphabet),)
+
+                out_sul = self.sul.step(suffix[-1])
+                out_hyp = hypothesis.step(suffix[-1])
+                self.num_steps += 1
+
+                if out_sul != out_hyp:
+                    self.sul.post()
+                    return prefix + suffix
+
+        return None
```

## aalpy/oracles/TransitionFocusOracle.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-import random
-
-from aalpy.base.Oracle import Oracle
-from aalpy.base.SUL import SUL
-
-
-class TransitionFocusOracle(Oracle):
-    """
-    This equivalence oracle focuses either on the same state transitions or transitions that lead to the different
-    states. This equivalence oracle should be used on grammars like balanced parentheses. In such grammars,
-    all interesting behavior occurs on the transitions between states and potential bugs can be found only by
-    focusing on transitions.
-    """
-    def __init__(self, alphabet, sul: SUL, num_random_walks=500, walk_len=20, same_state_prob=0.2):
-        """
-        Args:
-            alphabet: input alphabet
-            sul: system under learning
-            num_random_walks: number of walks
-            walk_len: length of each walk
-            same_state_prob: probability that the next input will lead to same state transition
-        """
-
-        super().__init__(alphabet, sul)
-        self.num_walks = num_random_walks
-        self.steps_per_walk = walk_len
-        self.same_state_prob = same_state_prob
-
-    def find_cex(self, hypothesis):
-
-        for _ in range(self.num_walks):
-            self.reset_hyp_and_sul(hypothesis)
-
-            curr_state = hypothesis.initial_state
-            inputs = []
-            for _ in range(self.steps_per_walk):
-                if random.random() <= self.same_state_prob:
-                    possible_inputs = curr_state.get_same_state_transitions()
-                else:
-                    possible_inputs = curr_state.get_diff_state_transitions()
-
-                act = random.choice(possible_inputs) if possible_inputs else random.choice(self.alphabet)
-                inputs.append(act)
-
-                out_sul = self.sul.step(inputs[-1])
-                out_hyp = hypothesis.step(inputs[-1])
-                self.num_steps += 1
-
-                if out_sul != out_hyp:
-                    self.sul.post()
-                    return inputs
-
-        return None
+import random
+
+from aalpy.base.Oracle import Oracle
+from aalpy.base.SUL import SUL
+
+
+class TransitionFocusOracle(Oracle):
+    """
+    This equivalence oracle focuses either on the same state transitions or transitions that lead to the different
+    states. This equivalence oracle should be used on grammars like balanced parentheses. In such grammars,
+    all interesting behavior occurs on the transitions between states and potential bugs can be found only by
+    focusing on transitions.
+    """
+    def __init__(self, alphabet, sul: SUL, num_random_walks=500, walk_len=20, same_state_prob=0.2):
+        """
+        Args:
+            alphabet: input alphabet
+            sul: system under learning
+            num_random_walks: number of walks
+            walk_len: length of each walk
+            same_state_prob: probability that the next input will lead to same state transition
+        """
+
+        super().__init__(alphabet, sul)
+        self.num_walks = num_random_walks
+        self.steps_per_walk = walk_len
+        self.same_state_prob = same_state_prob
+
+    def find_cex(self, hypothesis):
+
+        for _ in range(self.num_walks):
+            self.reset_hyp_and_sul(hypothesis)
+
+            curr_state = hypothesis.initial_state
+            inputs = []
+            for _ in range(self.steps_per_walk):
+                if random.random() <= self.same_state_prob:
+                    possible_inputs = curr_state.get_same_state_transitions()
+                else:
+                    possible_inputs = curr_state.get_diff_state_transitions()
+
+                act = random.choice(possible_inputs) if possible_inputs else random.choice(self.alphabet)
+                inputs.append(act)
+
+                out_sul = self.sul.step(inputs[-1])
+                out_hyp = hypothesis.step(inputs[-1])
+                self.num_steps += 1
+
+                if out_sul != out_hyp:
+                    self.sul.post()
+                    return inputs
+
+        return None
```

## aalpy/oracles/UserInputEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-from aalpy.base import Oracle, SUL
-from aalpy.utils.FileHandler import visualize_automaton
-
-
-class UserInputEqOracle(Oracle):
-    """
-    Interactive equivalence oracle. For every counterexample, the current hypothesis will be visualized and the user can
-    enter the counterexample step by step.
-    The user provides elements of the input alphabet or commands.
-    When the element of the input alphabet is entered, the step will be performed in the current hypothesis and output
-    will be printed.
-
-    Commands offered to the users are:
-
-        print alphabet - prints the input alphabet
-
-        current inputs - inputs entered so far
-
-        cex - returns inputs entered so far as the counterexample
-
-        end - no counterexample exists
-
-        reset - resets the current state of the hypothesis and clears inputs
-    """
-    def __init__(self, alphabet: list, sul: SUL):
-        super().__init__(alphabet, sul)
-        self.curr_hypothesis = 0
-
-    def find_cex(self, hypothesis):
-
-        self.reset_hyp_and_sul(hypothesis)
-
-        self.curr_hypothesis += 1
-        inputs = []
-        visualize_automaton(hypothesis, path=f'Hypothesis_{self.curr_hypothesis}')
-        while True:
-            inp = input('Please provide an input: ')
-            if inp == 'help':
-                print('Use one of following commands [print alphabet, current inputs, cex, end, reset] '
-                      'or provide an input')
-                continue
-            if inp == 'print alphabet':
-                print(self.alphabet)
-                continue
-            if inp == 'current inputs':
-                print(inputs)
-                continue
-            if inp == 'cex':
-                if inputs:
-                    self.sul.post()
-                    return inputs
-            if inp == 'end':
-                return None
-            if inp == 'reset':
-                inputs.clear()
-                self.reset_hyp_and_sul(hypothesis)
-                print('You are back in the initial state. Please provide an input: ')
-                continue
-            if inp not in self.alphabet:
-                print("Provided input is not in the input alphabet.")
-                continue
-            inputs.append(inp)
-            self.num_steps += 1
-            out_hyp = hypothesis.step(inp)
-            out_sul = self.sul.step(inp)
-            print('Hypothesis Output :', out_hyp)
-            print('SUL Output        :', out_sul)
-            if out_hyp != out_sul:
-                print('Counterexample found.\nIf you want to return it, type \'cex\'.')
+from aalpy.base import Oracle, SUL
+from aalpy.utils.FileHandler import visualize_automaton
+
+
+class UserInputEqOracle(Oracle):
+    """
+    Interactive equivalence oracle. For every counterexample, the current hypothesis will be visualized and the user can
+    enter the counterexample step by step.
+    The user provides elements of the input alphabet or commands.
+    When the element of the input alphabet is entered, the step will be performed in the current hypothesis and output
+    will be printed.
+
+    Commands offered to the users are:
+
+        print alphabet - prints the input alphabet
+
+        current inputs - inputs entered so far
+
+        cex - returns inputs entered so far as the counterexample
+
+        end - no counterexample exists
+
+        reset - resets the current state of the hypothesis and clears inputs
+    """
+    def __init__(self, alphabet: list, sul: SUL):
+        super().__init__(alphabet, sul)
+        self.curr_hypothesis = 0
+
+    def find_cex(self, hypothesis):
+
+        self.reset_hyp_and_sul(hypothesis)
+
+        self.curr_hypothesis += 1
+        inputs = []
+        visualize_automaton(hypothesis, path=f'Hypothesis_{self.curr_hypothesis}')
+        while True:
+            inp = input('Please provide an input: ')
+            if inp == 'help':
+                print('Use one of following commands [print alphabet, current inputs, cex, end, reset] '
+                      'or provide an input')
+                continue
+            if inp == 'print alphabet':
+                print(self.alphabet)
+                continue
+            if inp == 'current inputs':
+                print(inputs)
+                continue
+            if inp == 'cex':
+                if inputs:
+                    self.sul.post()
+                    return inputs
+            if inp == 'end':
+                return None
+            if inp == 'reset':
+                inputs.clear()
+                self.reset_hyp_and_sul(hypothesis)
+                print('You are back in the initial state. Please provide an input: ')
+                continue
+            if inp not in self.alphabet:
+                print("Provided input is not in the input alphabet.")
+                continue
+            inputs.append(inp)
+            self.num_steps += 1
+            out_hyp = hypothesis.step(inp)
+            out_sul = self.sul.step(inp)
+            print('Hypothesis Output :', out_hyp)
+            print('SUL Output        :', out_sul)
+            if out_hyp != out_sul:
+                print('Counterexample found.\nIf you want to return it, type \'cex\'.')
```

## aalpy/oracles/WMethodEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,131 +1,131 @@
-from itertools import product
-from random import shuffle, choice, randint
-
-from aalpy.base.Oracle import Oracle
-from aalpy.base.SUL import SUL
-
-
-class WMethodEqOracle(Oracle):
-    """
-    Equivalence oracle based on characterization set/ W-set. From 'Tsun S. Chow.   Testing software design modeled by
-    finite-state machines'.
-    """
-    def __init__(self, alphabet: list, sul: SUL, max_number_of_states, shuffle_test_set=True):
-        """
-        Args:
-
-            alphabet: input alphabet
-            sul: system under learning
-            max_number_of_states: maximum number of states in the automaton
-            shuffle_test_set: if True, test cases will be shuffled
-        """
-
-        super().__init__(alphabet, sul)
-        self.m = max_number_of_states
-        self.shuffle = shuffle_test_set
-        self.cache = set()
-
-    def find_cex(self, hypothesis):
-
-        if not hypothesis.characterization_set:
-            hypothesis.characterization_set = hypothesis.compute_characterization_set()
-
-        # covers every transition of the specification at least once.
-        transition_cover = [state.prefix + (letter,) for state in hypothesis.states for letter in self.alphabet]
-
-        middle = []
-        for i in range(self.m + 1 - len(hypothesis.states)):
-            middle.extend(list(product(self.alphabet, repeat=i)))
-
-        test_set = []
-        for seq in product(transition_cover, middle, hypothesis.characterization_set):
-            inp_seq = tuple([i for sub in seq for i in sub])
-            if inp_seq not in self.cache:
-                test_set.append(inp_seq)
-
-        if self.shuffle:
-            shuffle(test_set)
-        else:
-            test_set.sort(key=len, reverse=True)
-
-        for seq in test_set:
-            self.reset_hyp_and_sul(hypothesis)
-            outputs = []
-
-            for ind, letter in enumerate(seq):
-                out_hyp = hypothesis.step(letter)
-                out_sul = self.sul.step(letter)
-                self.num_steps += 1
-
-                outputs.append(out_sul)
-                if out_hyp != out_sul:
-                    self.sul.post()
-                    return seq[:ind + 1]
-            self.cache.add(seq)
-
-        return None
-
-
-class RandomWMethodEqOracle(Oracle):
-    """
-    Randomized version of the W-Method equivalence oracle.
-    Random walks stem from fixed prefix (path to the state). At the end of the random
-    walk an element from the characterization set is added to the test case.
-    """
-    def __init__(self, alphabet: list, sul: SUL, walks_per_state=12, walk_len=12):
-        """
-        Args:
-
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            walks_per_state: number of random walks that should start from each state
-
-            walk_len: length of random walk
-        """
-
-        super().__init__(alphabet, sul)
-        self.walks_per_state = walks_per_state
-        self.random_walk_len = walk_len
-        self.freq_dict = dict()
-
-    def find_cex(self, hypothesis):
-
-        if not hypothesis.characterization_set:
-            hypothesis.characterization_set = hypothesis.compute_characterization_set()
-            # fix for non-minimal intermediate hypothesis that can occur in KV
-            if not hypothesis.characterization_set:
-                hypothesis.characterization_set = [(a,) for a in hypothesis.get_input_alphabet()]
-
-        states_to_cover = []
-        for state in hypothesis.states:
-            if state.prefix is None:
-                state.prefix = hypothesis.get_shortest_path(hypothesis.initial_state, state)
-            if state.prefix not in self.freq_dict.keys():
-                self.freq_dict[state.prefix] = 0
-
-            states_to_cover.extend([state] * (self.walks_per_state - self.freq_dict[state.prefix]))
-
-        shuffle(states_to_cover)
-
-        for state in states_to_cover:
-            self.freq_dict[state.prefix] = self.freq_dict[state.prefix] + 1
-
-            self.reset_hyp_and_sul(hypothesis)
-
-            prefix = state.prefix
-            random_walk = tuple(choice(self.alphabet) for _ in range(randint(1, self.random_walk_len)))
-
-            test_case = prefix + random_walk + choice(hypothesis.characterization_set)
-
-            for ind, i in enumerate(test_case):
-                output_hyp = hypothesis.step(i)
-                output_sul = self.sul.step(i)
-                self.num_steps += 1
-
-                if output_sul != output_hyp:
-                    self.sul.post()
-                    return test_case[:ind + 1]
-
-        return None
+from itertools import product
+from random import shuffle, choice, randint
+
+from aalpy.base.Oracle import Oracle
+from aalpy.base.SUL import SUL
+
+
+class WMethodEqOracle(Oracle):
+    """
+    Equivalence oracle based on characterization set/ W-set. From 'Tsun S. Chow.   Testing software design modeled by
+    finite-state machines'.
+    """
+    def __init__(self, alphabet: list, sul: SUL, max_number_of_states, shuffle_test_set=True):
+        """
+        Args:
+
+            alphabet: input alphabet
+            sul: system under learning
+            max_number_of_states: maximum number of states in the automaton
+            shuffle_test_set: if True, test cases will be shuffled
+        """
+
+        super().__init__(alphabet, sul)
+        self.m = max_number_of_states
+        self.shuffle = shuffle_test_set
+        self.cache = set()
+
+    def find_cex(self, hypothesis):
+
+        if not hypothesis.characterization_set:
+            hypothesis.characterization_set = hypothesis.compute_characterization_set()
+
+        # covers every transition of the specification at least once.
+        transition_cover = [state.prefix + (letter,) for state in hypothesis.states for letter in self.alphabet]
+
+        middle = []
+        for i in range(self.m + 1 - len(hypothesis.states)):
+            middle.extend(list(product(self.alphabet, repeat=i)))
+
+        test_set = []
+        for seq in product(transition_cover, middle, hypothesis.characterization_set):
+            inp_seq = tuple([i for sub in seq for i in sub])
+            if inp_seq not in self.cache:
+                test_set.append(inp_seq)
+
+        if self.shuffle:
+            shuffle(test_set)
+        else:
+            test_set.sort(key=len, reverse=True)
+
+        for seq in test_set:
+            self.reset_hyp_and_sul(hypothesis)
+            outputs = []
+
+            for ind, letter in enumerate(seq):
+                out_hyp = hypothesis.step(letter)
+                out_sul = self.sul.step(letter)
+                self.num_steps += 1
+
+                outputs.append(out_sul)
+                if out_hyp != out_sul:
+                    self.sul.post()
+                    return seq[:ind + 1]
+            self.cache.add(seq)
+
+        return None
+
+
+class RandomWMethodEqOracle(Oracle):
+    """
+    Randomized version of the W-Method equivalence oracle.
+    Random walks stem from fixed prefix (path to the state). At the end of the random
+    walk an element from the characterization set is added to the test case.
+    """
+    def __init__(self, alphabet: list, sul: SUL, walks_per_state=12, walk_len=12):
+        """
+        Args:
+
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            walks_per_state: number of random walks that should start from each state
+
+            walk_len: length of random walk
+        """
+
+        super().__init__(alphabet, sul)
+        self.walks_per_state = walks_per_state
+        self.random_walk_len = walk_len
+        self.freq_dict = dict()
+
+    def find_cex(self, hypothesis):
+
+        if not hypothesis.characterization_set:
+            hypothesis.characterization_set = hypothesis.compute_characterization_set()
+            # fix for non-minimal intermediate hypothesis that can occur in KV
+            if not hypothesis.characterization_set:
+                hypothesis.characterization_set = [(a,) for a in hypothesis.get_input_alphabet()]
+
+        states_to_cover = []
+        for state in hypothesis.states:
+            if state.prefix is None:
+                state.prefix = hypothesis.get_shortest_path(hypothesis.initial_state, state)
+            if state.prefix not in self.freq_dict.keys():
+                self.freq_dict[state.prefix] = 0
+
+            states_to_cover.extend([state] * (self.walks_per_state - self.freq_dict[state.prefix]))
+
+        shuffle(states_to_cover)
+
+        for state in states_to_cover:
+            self.freq_dict[state.prefix] = self.freq_dict[state.prefix] + 1
+
+            self.reset_hyp_and_sul(hypothesis)
+
+            prefix = state.prefix
+            random_walk = tuple(choice(self.alphabet) for _ in range(randint(1, self.random_walk_len)))
+
+            test_case = prefix + random_walk + choice(hypothesis.characterization_set)
+
+            for ind, i in enumerate(test_case):
+                output_hyp = hypothesis.step(i)
+                output_sul = self.sul.step(i)
+                self.num_steps += 1
+
+                if output_sul != output_hyp:
+                    self.sul.post()
+                    return test_case[:ind + 1]
+
+        return None
```

## aalpy/oracles/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-from .BreadthFirstExplorationEqOracle import BreadthFirstExplorationEqOracle
-from .CacheBasedEqOracle import CacheBasedEqOracle
-from .kWayStateCoverageEqOracle import KWayStateCoverageEqOracle
-from .kWayTransitionCoverageEqOracle import KWayTransitionCoverageEqOracle
-from .RandomWalkEqOracle import RandomWalkEqOracle
-from .RandomWordEqOracle import RandomWordEqOracle
-from .StatePrefixEqOracle import StatePrefixEqOracle
-from .TransitionFocusOracle import TransitionFocusOracle
-from .UserInputEqOracle import UserInputEqOracle
-from .WMethodEqOracle import RandomWMethodEqOracle, WMethodEqOracle
-from .PacOracle import PacOracle
+from .BreadthFirstExplorationEqOracle import BreadthFirstExplorationEqOracle
+from .CacheBasedEqOracle import CacheBasedEqOracle
+from .kWayStateCoverageEqOracle import KWayStateCoverageEqOracle
+from .kWayTransitionCoverageEqOracle import KWayTransitionCoverageEqOracle
+from .RandomWalkEqOracle import RandomWalkEqOracle
+from .RandomWordEqOracle import RandomWordEqOracle
+from .StatePrefixEqOracle import StatePrefixEqOracle
+from .TransitionFocusOracle import TransitionFocusOracle
+from .UserInputEqOracle import UserInputEqOracle
+from .WMethodEqOracle import RandomWMethodEqOracle, WMethodEqOracle
+from .PacOracle import PacOracle
```

## aalpy/oracles/kWayStateCoverageEqOracle.py

```diff
@@ -1,81 +1,88 @@
-from random import choices, shuffle
-
-from aalpy.base import Oracle, SUL
-from itertools import combinations, permutations
-
-
-class KWayStateCoverageEqOracle(Oracle):
-    """
-    A test case will be computed for every k-combination or k-permutation of states with additional
-    random walk at the end.
-    """
-
-    def __init__(self, alphabet: list, sul: SUL, k=2, random_walk_len=20, method='combinations'):
-        """
-
-        Args:
-
-            alphabet: input alphabet
-
-            sul: system under learning
-
-            k: k value used for k-wise combinations/permutations of states
-
-            random_walk_len: length of random walk performed at the end of each combination/permutation
-
-            method: either 'combinations' or 'permutations'
-        """
-        super().__init__(alphabet, sul)
-        assert k > 1 and method in ['combinations', 'permutations']
-        self.k = k
-        self.cache = set()
-        self.fun = combinations if method == 'combinations' else permutations
-        self.random_walk_len = random_walk_len
-
-    def find_cex(self, hypothesis):
-
-        if len(hypothesis.states) == 1:
-            for _ in range(self.random_walk_len):
-                path = choices(self.alphabet, k=self.random_walk_len)
-                hypothesis.reset_to_initial()
-                self.sul.post()
-                self.sul.pre()
-                for i, p in enumerate(path):
-                    out_sul = self.sul.step(p)
-                    out_hyp = hypothesis.step(p)
-                    self.num_steps += 1
-
-                    if out_sul != out_hyp:
-                        self.sul.post()
-                        return path[:i + 1]
-
-        states = hypothesis.states
-        shuffle(states)
-
-        for comb in self.fun(hypothesis.states, self.k):
-            prefixes = frozenset([c.prefix for c in comb])
-            if prefixes in self.cache:
-                continue
-            else:
-                self.cache.add(prefixes)
-
-            index = 0
-            path = comb[0].prefix
-            while index < len(comb) - 1:
-                path += hypothesis.get_shortest_path(comb[index], comb[index + 1])
-                index += 1
-
-            path += tuple(choices(self.alphabet, k=self.random_walk_len))
-
-            self.reset_hyp_and_sul(hypothesis)
-
-            for i, p in enumerate(path):
-                out_sul = self.sul.step(p)
-                out_hyp = hypothesis.step(p)
-                self.num_steps += 1
-
-                if out_sul != out_hyp:
-                    self.sul.post()
-                    return path[:i + 1]
-
-        return None
+from random import choices, shuffle
+
+from aalpy.base import Oracle, SUL
+from itertools import combinations, permutations
+
+
+class KWayStateCoverageEqOracle(Oracle):
+    """
+    A test case will be computed for every k-combination or k-permutation of states with additional
+    random walk at the end.
+    """
+
+    def __init__(self, alphabet: list, sul: SUL, k=2, random_walk_len=100, method='combinations'):
+        """
+
+        Args:
+
+            alphabet: input alphabet
+
+            sul: system under learning
+
+            k: k value used for k-wise combinations/permutations of states
+
+            random_walk_len: length of random walk performed at the end of each combination/permutation
+
+            method: either 'combinations' or 'permutations'
+        """
+        super().__init__(alphabet, sul)
+        assert k > 1 and method in ['combinations', 'permutations']
+        self.k = k
+        self.cache = set()
+        self.fun = combinations if method == 'combinations' else permutations
+        self.random_walk_len = random_walk_len
+
+    def find_cex(self, hypothesis):
+
+        if len(hypothesis.states) == 1:
+            for _ in range(self.random_walk_len):
+                path = choices(self.alphabet, k=self.random_walk_len)
+                hypothesis.reset_to_initial()
+                self.sul.post()
+                self.sul.pre()
+                for i, p in enumerate(path):
+                    out_sul = self.sul.step(p)
+                    out_hyp = hypothesis.step(p)
+                    self.num_steps += 1
+
+                    if out_sul != out_hyp:
+                        return path[:i + 1]
+
+        states = hypothesis.states
+        shuffle(states)
+
+        for comb in self.fun(hypothesis.states, self.k):
+            prefixes = frozenset([c.prefix for c in comb])
+            if prefixes in self.cache:
+                continue
+            else:
+                self.cache.add(prefixes)
+
+            index = 0
+            path = comb[0].prefix
+            valid_comb = True
+            while index < len(comb) - 1:
+                path_between_states = hypothesis.get_shortest_path(comb[index], comb[index + 1])
+                if path_between_states is None:
+                    valid_comb = False
+                    break
+                path += path_between_states
+                index += 1
+
+            if not valid_comb:
+                continue
+
+            path += tuple(choices(self.alphabet, k=self.random_walk_len))
+
+            self.reset_hyp_and_sul(hypothesis)
+
+            for i, p in enumerate(path):
+                out_sul = self.sul.step(p)
+                out_hyp = hypothesis.step(p)
+                self.num_steps += 1
+
+                if out_sul != out_hyp:
+                    self.sul.post()
+                    return path[:i + 1]
+
+        return None
```

## aalpy/oracles/kWayTransitionCoverageEqOracle.py

 * *Ordering differences only*

```diff
@@ -1,165 +1,165 @@
-from collections import namedtuple
-from itertools import product
-from random import choices, randint, random
-
-from aalpy.base import SUL, Automaton, Oracle
-
-KWayTransition = namedtuple("KWayTransition", "start_state end_state steps")
-Path = namedtuple("Path", "start_state end_state steps kWayTransitions, transitions_log")
-
-
-class KWayTransitionCoverageEqOracle(Oracle):
-    """
-    This Equivalence oracle selects test cases based on k-way transitions coverage. It does that
-    by generating random queries and finding the smallest subset with the highest coverage. In other words, this oracle
-    finds counter examples by running random paths that cover all pairwise / k-way transitions.
-    """
-
-    def __init__(self, alphabet: list, sul: SUL, k: int = 2, method='random',
-                 num_generate_paths: int = 1000,
-                 max_path_len: int = 50,
-                 max_number_of_steps: int = 0,
-                 optimize: str = 'steps',
-                 random_walk_len=10):
-        """
-        Args:
-
-            alphabet: input alphabet
-            sul: system under learning
-            k: k value used for K-Way transitions, i.e the number of steps between the start and the end of a transition
-            method: defines how the queries are generated 'random' or 'prefix'
-            num_generate_paths: number of random queries used to find the optimal subset
-            max_path_len: the maximum step size of a generated path
-            max_number_of_steps: maximum number of steps that will be executed on the SUL (0 = no limit)
-            optimize: minimize either the number of  'steps' or 'queries' that are executed
-            random_walk_len: the number of steps that are added by 'prefix' generated paths
-
-        """
-        super().__init__(alphabet, sul)
-        assert k >= 2
-        assert method in ['random', 'prefix']
-        assert optimize in ['steps', 'queries']
-
-        self.k = k
-        self.method = method
-        self.num_generate_paths = num_generate_paths
-        self.max_path_len = max_path_len
-        self.max_number_of_steps = max_number_of_steps
-        self.optimize = optimize
-        self.random_walk_len = random_walk_len
-
-        self.cached_paths = list()
-
-    def find_cex(self, hypothesis: Automaton):
-        if self.method == 'random':
-            paths = self.generate_random_paths(hypothesis) + self.cached_paths
-            self.cached_paths = self.greedy_set_cover(hypothesis, paths)
-
-            for path in self.cached_paths:
-                counter_example = self.check_path(hypothesis, path.steps)
-
-                if counter_example is not None:
-                    return counter_example
-
-        elif self.method == 'prefix':
-            for steps in self.generate_prefix_steps(hypothesis):
-                counter_example = self.check_path(hypothesis, steps)
-
-                if counter_example is not None:
-                    return counter_example
-        return None
-
-    def greedy_set_cover(self, hypothesis: Automaton, paths: list):
-        result = list()
-        covered = set()
-        step_count = 0
-
-        size_of_universe = len(hypothesis.states) * pow(len(self.alphabet), self.k)
-
-        while size_of_universe > len(covered):
-            path = self.select_optimal_path(covered, paths)
-
-            if path is not None:
-                covered = set.union(covered, path.kWayTransitions)
-                paths.remove(path)
-                result.append(path)
-                step_count += len(path.steps)
-
-            if path is None or not paths:
-                paths = [self.create_path(hypothesis, steps) for steps in self.generate_prefix_steps(hypothesis)]
-
-            if self.max_number_of_steps != 0 and step_count > self.max_number_of_steps:
-                print("stop")
-                break
-
-        return result
-
-    def select_optimal_path(self, covered: set, paths: list) -> Path:
-        result = None
-
-        if self.optimize == 'steps':
-            result = max(paths, key=lambda p: len(
-                p.kWayTransitions - covered) / len(p.steps))
-
-        if self.optimize == 'queries':
-            result = max(paths, key=lambda p: len(p.kWayTransitions - covered))
-
-        return result if len(result.kWayTransitions - covered) != 0 else None
-
-    def generate_random_paths(self, hypothesis: Automaton) -> list:
-        result = list()
-
-        for _ in range(self.num_generate_paths):
-            random_length = randint(self.k, self.max_path_len)
-            steps = tuple(choices(self.alphabet, k=random_length))
-            path = self.create_path(hypothesis, steps)
-            result.append(path)
-
-        return result
-
-    def generate_prefix_steps(self, hypothesis: Automaton) -> tuple:
-        for state in reversed(hypothesis.states):
-            prefix = state.prefix
-            for steps in sorted(product(self.alphabet, repeat=self.k), key=lambda k: random()):
-                yield prefix + steps + tuple(choices(self.alphabet, k=self.random_walk_len))
-
-    def create_path(self, hypothesis: Automaton, steps: tuple) -> Path:
-        transitions = set()
-        transitions_log = list()
-
-        prev_states = list()
-        end_states = list()
-
-        hypothesis.reset_to_initial()
-
-        for i, s in enumerate(steps):
-            prev_states.append(hypothesis.current_state)
-            hypothesis.step(s)
-            end_states.append(hypothesis.current_state)
-
-        for i in range(len(steps) - self.k + 1):
-            prev_state = prev_states[i]
-            end_state = end_states[i + self.k - 1]
-            chunk = tuple(steps[i:i + self.k])
-
-            transition = KWayTransition(prev_state.state_id, end_state.state_id, chunk)
-
-            transitions_log.append(transition)
-            transitions.add(transition)
-
-        return Path(hypothesis.initial_state, end_states[-1], steps, transitions, transitions_log)
-
-    def check_path(self, hypothesis: Automaton, steps: tuple):
-        self.reset_hyp_and_sul(hypothesis)
-
-        for i, s in enumerate(steps):
-            out_sul = self.sul.step(s)
-            out_hyp = hypothesis.step(s)
-
-            self.num_steps += 1
-
-            if out_sul != out_hyp:
-                self.sul.post()
-                return steps[:i + 1]
-
-        return None
+from collections import namedtuple
+from itertools import product
+from random import choices, randint, random
+
+from aalpy.base import SUL, Automaton, Oracle
+
+KWayTransition = namedtuple("KWayTransition", "start_state end_state steps")
+Path = namedtuple("Path", "start_state end_state steps kWayTransitions, transitions_log")
+
+
+class KWayTransitionCoverageEqOracle(Oracle):
+    """
+    This Equivalence oracle selects test cases based on k-way transitions coverage. It does that
+    by generating random queries and finding the smallest subset with the highest coverage. In other words, this oracle
+    finds counter examples by running random paths that cover all pairwise / k-way transitions.
+    """
+
+    def __init__(self, alphabet: list, sul: SUL, k: int = 2, method='random',
+                 num_generate_paths: int = 1000,
+                 max_path_len: int = 50,
+                 max_number_of_steps: int = 0,
+                 optimize: str = 'steps',
+                 random_walk_len=10):
+        """
+        Args:
+
+            alphabet: input alphabet
+            sul: system under learning
+            k: k value used for K-Way transitions, i.e the number of steps between the start and the end of a transition
+            method: defines how the queries are generated 'random' or 'prefix'
+            num_generate_paths: number of random queries used to find the optimal subset
+            max_path_len: the maximum step size of a generated path
+            max_number_of_steps: maximum number of steps that will be executed on the SUL (0 = no limit)
+            optimize: minimize either the number of  'steps' or 'queries' that are executed
+            random_walk_len: the number of steps that are added by 'prefix' generated paths
+
+        """
+        super().__init__(alphabet, sul)
+        assert k >= 2
+        assert method in ['random', 'prefix']
+        assert optimize in ['steps', 'queries']
+
+        self.k = k
+        self.method = method
+        self.num_generate_paths = num_generate_paths
+        self.max_path_len = max_path_len
+        self.max_number_of_steps = max_number_of_steps
+        self.optimize = optimize
+        self.random_walk_len = random_walk_len
+
+        self.cached_paths = list()
+
+    def find_cex(self, hypothesis: Automaton):
+        if self.method == 'random':
+            paths = self.generate_random_paths(hypothesis) + self.cached_paths
+            self.cached_paths = self.greedy_set_cover(hypothesis, paths)
+
+            for path in self.cached_paths:
+                counter_example = self.check_path(hypothesis, path.steps)
+
+                if counter_example is not None:
+                    return counter_example
+
+        elif self.method == 'prefix':
+            for steps in self.generate_prefix_steps(hypothesis):
+                counter_example = self.check_path(hypothesis, steps)
+
+                if counter_example is not None:
+                    return counter_example
+        return None
+
+    def greedy_set_cover(self, hypothesis: Automaton, paths: list):
+        result = list()
+        covered = set()
+        step_count = 0
+
+        size_of_universe = len(hypothesis.states) * pow(len(self.alphabet), self.k)
+
+        while size_of_universe > len(covered):
+            path = self.select_optimal_path(covered, paths)
+
+            if path is not None:
+                covered = set.union(covered, path.kWayTransitions)
+                paths.remove(path)
+                result.append(path)
+                step_count += len(path.steps)
+
+            if path is None or not paths:
+                paths = [self.create_path(hypothesis, steps) for steps in self.generate_prefix_steps(hypothesis)]
+
+            if self.max_number_of_steps != 0 and step_count > self.max_number_of_steps:
+                print("stop")
+                break
+
+        return result
+
+    def select_optimal_path(self, covered: set, paths: list) -> Path:
+        result = None
+
+        if self.optimize == 'steps':
+            result = max(paths, key=lambda p: len(
+                p.kWayTransitions - covered) / len(p.steps))
+
+        if self.optimize == 'queries':
+            result = max(paths, key=lambda p: len(p.kWayTransitions - covered))
+
+        return result if len(result.kWayTransitions - covered) != 0 else None
+
+    def generate_random_paths(self, hypothesis: Automaton) -> list:
+        result = list()
+
+        for _ in range(self.num_generate_paths):
+            random_length = randint(self.k, self.max_path_len)
+            steps = tuple(choices(self.alphabet, k=random_length))
+            path = self.create_path(hypothesis, steps)
+            result.append(path)
+
+        return result
+
+    def generate_prefix_steps(self, hypothesis: Automaton) -> tuple:
+        for state in reversed(hypothesis.states):
+            prefix = state.prefix
+            for steps in sorted(product(self.alphabet, repeat=self.k), key=lambda k: random()):
+                yield prefix + steps + tuple(choices(self.alphabet, k=self.random_walk_len))
+
+    def create_path(self, hypothesis: Automaton, steps: tuple) -> Path:
+        transitions = set()
+        transitions_log = list()
+
+        prev_states = list()
+        end_states = list()
+
+        hypothesis.reset_to_initial()
+
+        for i, s in enumerate(steps):
+            prev_states.append(hypothesis.current_state)
+            hypothesis.step(s)
+            end_states.append(hypothesis.current_state)
+
+        for i in range(len(steps) - self.k + 1):
+            prev_state = prev_states[i]
+            end_state = end_states[i + self.k - 1]
+            chunk = tuple(steps[i:i + self.k])
+
+            transition = KWayTransition(prev_state.state_id, end_state.state_id, chunk)
+
+            transitions_log.append(transition)
+            transitions.add(transition)
+
+        return Path(hypothesis.initial_state, end_states[-1], steps, transitions, transitions_log)
+
+    def check_path(self, hypothesis: Automaton, steps: tuple):
+        self.reset_hyp_and_sul(hypothesis)
+
+        for i, s in enumerate(steps):
+            out_sul = self.sul.step(s)
+            out_hyp = hypothesis.step(s)
+
+            self.num_steps += 1
+
+            if out_sul != out_hyp:
+                self.sul.post()
+                return steps[:i + 1]
+
+        return None
```

## aalpy/utils/AutomatonGenerators.py

```diff
@@ -1,542 +1,566 @@
-import random
-import warnings
-
-from aalpy.automata import Dfa, DfaState, MdpState, Mdp, MealyMachine, MealyState, \
-    MooreMachine, MooreState, OnfsmState, Onfsm, MarkovChain, McState, StochasticMealyState, StochasticMealyMachine
-
-
-def generate_random_deterministic_automata(automaton_type,
-                                           num_states,
-                                           input_alphabet_size,
-                                           output_alphabet_size,
-                                           compute_prefixes=False,
-                                           ensure_minimality=True,
-                                           **kwargs
-                                           ):
-    """
-    Generates a random deterministic automata of 'automaton_type'.
-
-    Args:
-        automaton_type: type of automaton, either 'dfa', 'mealy', or 'moore'
-        num_states: number of states
-        input_alphabet_size: size of input alphabet
-        output_alphabet_size: size of output alphabet (ignored for DFAs)
-        compute_prefixes: compute prefixes leading to each state
-        ensure_minimality: ensure that the automaton is minimal
-        **kwargs:
-            : 'custom_input_alphabet'  a list of custom input alphabet values
-            : 'custom_output_alphabet' a list of custom output alphabet values
-            : 'num_accepting_states' number of accepting states for DFA generation
-
-    Returns:
-
-        Random deterministic automaton of user defined type, size. If ensure_minimality is set to False returned
-        automaton is not necessarily minimal. If minimality is reacquired and random automaton cannot be produced in
-        multiple interactions, non-minimal automaton will be returned and a warning message printed.
-    """
-
-    assert automaton_type in {'dfa', 'mealy', 'moore'}
-    if output_alphabet_size < 2 or output_alphabet_size is None:
-        output_alphabet_size = 2
-
-    state_class_map = {'dfa': DfaState, 'mealy': MealyState, 'moore': MooreState}
-    automaton_class_map = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
-
-    input_alphabet = [f'i{i + 1}' for i in range(input_alphabet_size)]
-    output_alphabet = [f'o{i + 1}' for i in range(output_alphabet_size)] if automaton_type != 'dfa' else [True, False]
-
-    # For backwards comparability or if uses passes custom input output functions
-    if 'custom_input_alphabet' in kwargs:
-        input_alphabet = kwargs.get('custom_input_alphabet')
-    if 'custom_output_alphabet' in kwargs:
-        output_alphabet = kwargs.get('custom_output_alphabet')
-    accepting_state_ids = None
-    if 'num_accepting_states' in kwargs:
-        num_accepting_states = kwargs.get('num_accepting_states')
-        accepting_state_ids = [f's{i + 1}' for i in random.sample(list(range(num_states)), k=num_accepting_states)]
-
-    states = [state_class_map[automaton_type](state_id=f's{i + 1}') for i in range(num_states)]
-    state_id_state_map = {state.state_id: state for state in states}
-
-    if automaton_type != 'mealy':
-        for state in states:
-            output = random.choice(output_alphabet)
-            if automaton_type == 'dfa':
-                if accepting_state_ids is not None:
-                    output = state.state_id in accepting_state_ids
-                state.is_accepting = output
-            else:
-                state.output = output
-
-    state_buffer = [state.state_id for state in states]
-    queue = [states[0]]
-    state_buffer.remove(states[0].state_id)
-    visited_states = set()
-
-    while queue:
-        state = queue.pop(0)
-        visited_states.add(state.state_id)
-        for i in input_alphabet:
-            # states from which to choose next state (while all states have not be reached)
-            if state_buffer:
-                new_state_candidates = [state_id_state_map[state_id] for state_id in state_buffer]
-            else:
-                new_state_candidates = states
-
-            new_state = random.choice(new_state_candidates)
-
-            if new_state.state_id in state_buffer:
-                state_buffer.remove(new_state.state_id)
-
-            state.transitions[i] = new_state
-
-            if automaton_type == 'mealy':
-                state.output_fun[i] = random.choice(output_alphabet)
-
-        for child in state.transitions.values():
-            if child.state_id not in visited_states and child not in queue:
-                queue.append(child)
-
-    random_automaton = automaton_class_map[automaton_type](states[0], states)
-
-    if compute_prefixes:
-        for state in random_automaton.states:
-            state.prefix = random_automaton.get_shortest_path(random_automaton.initial_state, state)
-            if state != random_automaton.initial_state and not state.prefix:
-                print('Non-reachable state:', state.state_id)
-
-    if ensure_minimality:
-        minimality_iterations = 1
-        while not random_automaton.is_minimal():
-            # to avoid infinite loops
-            if minimality_iterations == 100:
-                warnings.warn(f'Non-minimal automaton ({automaton_type}, num_states : {num_states}) returned.')
-                break
-
-            custom_args = {}
-            if 'custom_input_alphabet' in kwargs:
-                custom_args['custom_input_alphabet'] = kwargs.get('custom_input_alphabet')
-            if 'custom_output_alphabet' in kwargs:
-                custom_args['custom_output_alphabet'] = kwargs.get('custom_output_alphabet')
-            if 'num_accepting_states' in kwargs:
-                custom_args['num_accepting_states'] = kwargs.get('num_accepting_states')
-
-            random_automaton = generate_random_deterministic_automata(automaton_type,
-                                                                      num_states,
-                                                                      input_alphabet_size,
-                                                                      output_alphabet_size,
-                                                                      compute_prefixes, # compute prefixes
-                                                                      False, # ensure minimality
-                                                                      **custom_args)
-
-    return random_automaton
-
-
-def generate_random_mealy_machine(num_states, input_alphabet, output_alphabet,
-                                  compute_prefixes=False, ensure_minimality=True) -> MealyMachine:
-    """
-    Generates a random Mealy machine. Kept for backwards compatibility.
-
-    Args:
-
-        num_states: number of states
-        input_alphabet: input alphabet
-        output_alphabet: output alphabet
-        compute_prefixes: if true, shortest path to reach each state will be computed (Default value = False)
-        ensure_minimality: returned automaton will be minimal
-
-    Returns:
-
-        Mealy machine with num_states states
-    """
-
-    random_mealy_machine = generate_random_deterministic_automata('mealy', num_states,
-                                                                  input_alphabet_size=len(input_alphabet),
-                                                                  output_alphabet_size=len(output_alphabet),
-                                                                  ensure_minimality=ensure_minimality,
-                                                                  compute_prefixes=compute_prefixes,
-                                                                  custom_input_alphabet=input_alphabet,
-                                                                  custom_output_alphabet=output_alphabet)
-
-    return random_mealy_machine
-
-
-def generate_random_moore_machine(num_states, input_alphabet, output_alphabet,
-                                  compute_prefixes=False, ensure_minimality=True) -> MooreMachine:
-    """
-    Generates a random Moore machine.
-
-    Args:
-
-        num_states: number of states
-        input_alphabet: input alphabet
-        output_alphabet: output alphabet
-        compute_prefixes: if true, shortest path to reach each state will be computed (Default value = False)
-        ensure_minimality: returned automaton will be minimal
-
-    Returns:
-
-        Random Moore machine with num_states states
-
-    """
-    random_moore_machine = generate_random_deterministic_automata('moore', num_states,
-                                                                  input_alphabet_size=len(input_alphabet),
-                                                                  output_alphabet_size=len(output_alphabet),
-                                                                  ensure_minimality=ensure_minimality,
-                                                                  compute_prefixes=compute_prefixes,
-                                                                  custom_input_alphabet=input_alphabet,
-                                                                  custom_output_alphabet=output_alphabet)
-
-    return random_moore_machine
-
-
-def generate_random_dfa(num_states, alphabet, num_accepting_states=1,
-                        compute_prefixes=False, ensure_minimality=True) -> Dfa:
-    """
-    Generates a random DFA.
-
-    Args:
-
-        num_states: number of states
-        alphabet: input alphabet
-        num_accepting_states: number of accepting states (Default value = 1)
-        compute_prefixes: if true, shortest path to reach each state will be computed (Default value = False)
-        ensure_minimality: returned automaton will be minimal
-
-    Returns:
-
-        Randomly generated DFA
-
-    """
-    if num_states <= num_accepting_states:
-        num_accepting_states = num_states - 1
-
-    random_dfa = generate_random_deterministic_automata('dfa', num_states,
-                                                        input_alphabet_size=len(alphabet),
-                                                        output_alphabet_size=2,
-                                                        ensure_minimality=ensure_minimality,
-                                                        compute_prefixes=compute_prefixes,
-                                                        custom_input_alphabet=alphabet,
-                                                        num_accepting_states=num_accepting_states)
-
-    return random_dfa
-
-
-def generate_random_mdp(num_states, input_size, output_size, possible_probabilities=None):
-    """
-    Generates random MDP.
-
-    Args:
-
-        num_states: number of states
-        input_size: number of inputs
-        output_size: user predefined outputs
-        possible_probabilities: list of possible probability pairs to choose from
-
-    Returns:
-
-        random MDP
-
-    """
-
-    inputs = [f'i{i + 1}' for i in range(input_size)]
-    outputs = [f'o{i + 1}' for i in range(output_size)]
-
-    if not possible_probabilities:
-        possible_probabilities = [(1.,), (1.,), (1.,), (0.9, 0.1),
-                                  (0.8, 0.2), (0.7, 0.3), (0.8, 0.1, 0.1), (0.7, 0.2, 0.1), (0.6, 0.2, 0.1, 0.1)]
-        # ensure that there are no infinite loops
-        max_prob_num = min(num_states, input_size)
-        possible_probabilities = [p for p in possible_probabilities if len(p) <= max_prob_num]
-
-    state_outputs = outputs.copy()
-    states = []
-    for i in range(num_states):
-        curr_output = state_outputs.pop(0) if state_outputs else random.choice(outputs)
-        states.append(MdpState(f'q{i}', curr_output))
-
-    state_buffer = list(states)
-    for state in states:
-        for i in inputs:
-            prob = random.choice(possible_probabilities)
-            reached_states = []
-            for _ in prob:
-                while True:
-                    new_state = random.choice(state_buffer) if state_buffer else random.choice(states)
-
-                    # ensure determinism
-                    if new_state.output not in {s.output for s in reached_states}:
-                        break
-
-                    if state_buffer:
-                        state_buffer.remove(new_state)
-
-                reached_states.append(new_state)
-
-            for prob, reached_state in zip(prob, reached_states):
-                state.transitions[i].append((reached_state, prob))
-
-    return Mdp(states[0], states)
-
-
-def generate_random_smm(num_states, input_size, output_size, possible_probabilities=None):
-    """
-    Generates random MDP.
-
-    Args:
-
-        num_states: number of states
-        input_size: number of inputs
-        output_size: number of outputs
-        possible_probabilities: list of possible probability pairs to choose from
-
-    Returns:
-
-        random SMM
-
-    """
-
-    inputs = [f'i{i + 1}' for i in range(input_size)]
-    outputs = [f'o{i + 1}' for i in range(output_size)]
-
-    if not possible_probabilities:
-        possible_probabilities = [(1.,), (1.,), (1.,), (0.9, 0.1),
-                                  (0.8, 0.2), (0.7, 0.3), (0.8, 0.1, 0.1), (0.7, 0.2, 0.1), (0.6, 0.2, 0.1, 0.1)]
-        # ensure that there are no infinite loops
-        max_prob_num = min(num_states, input_size)
-        possible_probabilities = [p for p in possible_probabilities if len(p) <= max_prob_num]
-
-    states = []
-    for i in range(num_states):
-        states.append(StochasticMealyState(f'q{i}'))
-
-    state_buffer = list(states)
-    output_buffer = outputs.copy()
-    for state in states:
-        for i in inputs:
-            prob = random.choice(possible_probabilities)
-            reached_states = []
-            transition_outputs = []
-            for _ in prob:
-                while True:
-                    o = random.choice(output_buffer) if output_buffer else random.choice(outputs)
-                    new_state = random.choice(state_buffer) if state_buffer else random.choice(states)
-
-                    # ensure determinism
-                    if o not in transition_outputs:
-                        break
-
-                    if output_buffer:
-                        output_buffer.remove(o)
-                    if state_buffer:
-                        state_buffer.remove(new_state)
-
-                reached_states.append(new_state)
-                transition_outputs.append(o)
-
-            for index in range(len(prob)):
-                state.transitions[i].append((reached_states[index], transition_outputs[index], prob[index]))
-
-    return StochasticMealyMachine(states[0], states)
-
-
-def generate_random_ONFSM(num_states, num_inputs, num_outputs, multiple_out_prob=0.33):
-    """
-    Randomly generate an observable non-deterministic finite-state machine.
-
-    Args:
-
-      num_states: number of states
-      num_inputs: number of inputs
-      num_outputs: number of outputs
-      multiple_out_prob: probability that state will have multiple outputs (Default value = 0.5)
-
-    Returns:
-
-        randomly generated ONFSM
-
-    """
-    inputs = [f'i{i + 1}' for i in range(num_inputs)]
-    outputs = [f'o{i + 1}' for i in range(num_outputs)]
-
-    states = []
-    for i in range(num_states):
-        state = OnfsmState(f's{i}')
-        states.append(state)
-
-    state_buffer = states.copy()
-
-    for state in states:
-        for i in inputs:
-            state_outputs = 1
-            if random.random() <= multiple_out_prob and num_outputs >= 2:
-                state_outputs = random.randint(2, num_outputs)
-
-            random_out = random.sample(outputs, state_outputs)
-            for index in range(state_outputs):
-                if state_buffer:
-                    new_state = random.choice(state_buffer)
-                    state_buffer.remove(new_state)
-                else:
-                    new_state = random.choice(states)
-                state.transitions[i].append((random_out[index], new_state))
-
-    return Onfsm(states[0], states)
-
-
-def generate_random_markov_chain(num_states):
-    assert num_states >= 3
-    possible_probabilities = [1.0, 1.0, 0.8, 0.5, 0.9]
-    states = []
-
-    for i in range(num_states):
-        states.append(McState(f'q{i}', i))
-
-    for index, state in enumerate(states[:-1]):
-        prob = random.choice(possible_probabilities)
-        if prob == 1.:
-            new_state = states[index + 1]
-            state.transitions.append((new_state, prob))
-        else:
-            next_state = states[index + 1]
-            up_states = list(states)
-            up_states.remove(next_state)
-            rand_state = random.choice(up_states)
-
-            state.transitions.append((next_state, prob))
-            state.transitions.append((rand_state, round(1 - prob, 2)))
-
-    return MarkovChain(states[0], states)
-
-
-def dfa_from_state_setup(state_setup) -> Dfa:
-    """
-        First state in the state setup is the initial state.
-        Example state setup:
-        state_setup = {
-                "a": (True, {"x": "b1", "y": "a"}),
-                "b1": (False, {"x": "b2", "y": "a"}),
-                "b2": (True, {"x": "b3", "y": "a"}),
-                "b3": (False, {"x": "b4", "y": "a"}),
-                "b4": (False, {"x": "c", "y": "a"}),
-                "c": (True, {"x": "a", "y": "a"}),
-            }
-
-        Args:
-
-            state_setup: map from state_id to tuple(output and transitions_dict)
-
-        Returns:
-
-            DFA
-        """
-    # state_setup should map from state_id to tuple(is_accepting and transitions_dict)
-
-    # build states with state_id and output
-    states = {key: DfaState(key, val[0]) for key, val in state_setup.items()}
-
-    # add transitions to states
-    for state_id, state in states.items():
-        for _input, target_state_id in state_setup[state_id][1].items():
-            state.transitions[_input] = states[target_state_id]
-
-    # states to list
-    states = [state for state in states.values()]
-
-    # build moore machine with first state as starting state
-    dfa = Dfa(states[0], states)
-
-    for state in states:
-        state.prefix = dfa.get_shortest_path(dfa.initial_state, state)
-
-    return dfa
-
-
-def moore_from_state_setup(state_setup) -> MooreMachine:
-    """
-    First state in the state setup is the initial state.
-    Example state setup:
-    state_setup = {
-            "a": ("a", {"x": "b1", "y": "a"}),
-            "b1": ("b", {"x": "b2", "y": "a"}),
-            "b2": ("b", {"x": "b3", "y": "a"}),
-            "b3": ("b", {"x": "b4", "y": "a"}),
-            "b4": ("b", {"x": "c", "y": "a"}),
-            "c": ("c", {"x": "a", "y": "a"}),
-        }
-
-    Args:
-
-        state_setup: map from state_id to tuple(output and transitions_dict)
-
-    Returns:
-
-        Moore machine
-    """
-
-    # build states with state_id and output
-    states = {key: MooreState(key, val[0]) for key, val in state_setup.items()}
-
-    # add transitions to states
-    for state_id, state in states.items():
-        for _input, target_state_id in state_setup[state_id][1].items():
-            state.transitions[_input] = states[target_state_id]
-
-    # states to list
-    states = [state for state in states.values()]
-
-    # build moore machine with first state as starting state
-    mm = MooreMachine(states[0], states)
-
-    for state in states:
-        state.prefix = mm.get_shortest_path(mm.initial_state, state)
-
-    return mm
-
-
-def mealy_from_state_setup(state_setup) -> MealyMachine:
-    """
-        First state in the state setup is the initial state.
-        state_setup = {
-            "a": {"x": ("o1", "b1"), "y": ("o2", "a")},
-            "b1": {"x": ("o3", "b2"), "y": ("o1", "a")},
-            "b2": {"x": ("o1", "b3"), "y": ("o2", "a")},
-            "b3": {"x": ("o3", "b4"), "y": ("o1", "a")},
-            "b4": {"x": ("o1", "c"), "y": ("o4", "a")},
-            "c": {"x": ("o3", "a"), "y": ("o5", "a")},
-        }
-
-
-    Args:
-
-        state_setup:
-            state_setup should map from state_id to tuple(transitions_dict).
-
-    Returns:
-
-        Mealy Machine
-    """
-    # state_setup should map from state_id to tuple(transitions_dict).
-    # Each entry in transition dict is <input> : <output, new_state_id>
-
-    # build states with state_id and output
-    states = {key: MealyState(key) for key, _ in state_setup.items()}
-
-    # add transitions to states
-    for state_id, state in states.items():
-        for _input, (output, new_state) in state_setup[state_id].items():
-            state.transitions[_input] = states[new_state]
-            state.output_fun[_input] = output
-
-    # states to list
-    states = [state for state in states.values()]
-
-    # build moore machine with first state as starting state
-    mm = MealyMachine(states[0], states)
-
-    for state in states:
-        state.prefix = mm.get_shortest_path(mm.initial_state, state)
-
-    return mm
-
-
+import random
+import warnings
+
+from aalpy.automata import Dfa, DfaState, MdpState, Mdp, MealyMachine, MealyState, \
+    MooreMachine, MooreState, OnfsmState, Onfsm, MarkovChain, McState, StochasticMealyState, StochasticMealyMachine
+
+
+def generate_random_deterministic_automata(automaton_type,
+                                           num_states,
+                                           input_alphabet_size,
+                                           output_alphabet_size,
+                                           compute_prefixes=False,
+                                           ensure_minimality=True,
+                                           **kwargs
+                                           ):
+    """
+    Generates a random deterministic automata of 'automaton_type'.
+
+    Args:
+        automaton_type: type of automaton, either 'dfa', 'mealy', or 'moore'
+        num_states: number of states
+        input_alphabet_size: size of input alphabet
+        output_alphabet_size: size of output alphabet (ignored for DFAs)
+        compute_prefixes: compute prefixes leading to each state
+        ensure_minimality: ensure that the automaton is minimal
+        **kwargs:
+            : 'custom_input_alphabet'  a list of custom input alphabet values
+            : 'custom_output_alphabet' a list of custom output alphabet values
+            : 'num_accepting_states' number of accepting states for DFA generation
+
+    Returns:
+
+        Random deterministic automaton of user defined type, size. If ensure_minimality is set to False returned
+        automaton is not necessarily minimal. If minimality is reacquired and random automaton cannot be produced in
+        multiple interactions, non-minimal automaton will be returned and a warning message printed.
+    """
+
+    assert automaton_type in {'dfa', 'mealy', 'moore'}
+    if output_alphabet_size < 2 or output_alphabet_size is None:
+        output_alphabet_size = 2
+
+    state_class_map = {'dfa': DfaState, 'mealy': MealyState, 'moore': MooreState}
+    automaton_class_map = {'dfa': Dfa, 'mealy': MealyMachine, 'moore': MooreMachine}
+
+    input_alphabet = [f'i{i + 1}' for i in range(input_alphabet_size)]
+    output_alphabet = [f'o{i + 1}' for i in range(output_alphabet_size)] if automaton_type != 'dfa' else [True, False]
+
+    # For backwards comparability or if uses passes custom input output functions
+    if 'custom_input_alphabet' in kwargs:
+        input_alphabet = kwargs.get('custom_input_alphabet')
+    if 'custom_output_alphabet' in kwargs:
+        output_alphabet = kwargs.get('custom_output_alphabet')
+    accepting_state_ids = None
+    if 'num_accepting_states' in kwargs:
+        num_accepting_states = kwargs.get('num_accepting_states')
+        accepting_state_ids = [f's{i + 1}' for i in random.sample(list(range(num_states)), k=num_accepting_states)]
+
+    states = [state_class_map[automaton_type](state_id=f's{i + 1}') for i in range(num_states)]
+    state_id_state_map = {state.state_id: state for state in states}
+
+    if automaton_type != 'mealy':
+        for state in states:
+            output = random.choice(output_alphabet)
+            if automaton_type == 'dfa':
+                if accepting_state_ids is not None:
+                    output = state.state_id in accepting_state_ids
+                state.is_accepting = output
+            else:
+                state.output = output
+
+    state_buffer = [state.state_id for state in states]
+    queue = [states[0]]
+    state_buffer.remove(states[0].state_id)
+    visited_states = set()
+
+    while queue:
+        state = queue.pop(0)
+        visited_states.add(state.state_id)
+        for i in input_alphabet:
+            # states from which to choose next state (while all states have not be reached)
+            if state_buffer:
+                new_state_candidates = [state_id_state_map[state_id] for state_id in state_buffer]
+            else:
+                new_state_candidates = states
+
+            new_state = random.choice(new_state_candidates)
+
+            if new_state.state_id in state_buffer:
+                state_buffer.remove(new_state.state_id)
+
+            state.transitions[i] = new_state
+
+            if automaton_type == 'mealy':
+                state.output_fun[i] = random.choice(output_alphabet)
+
+        for child in state.transitions.values():
+            if child.state_id not in visited_states and child not in queue:
+                queue.append(child)
+
+    random_automaton = automaton_class_map[automaton_type](states[0], states)
+
+    if compute_prefixes:
+        for state in random_automaton.states:
+            state.prefix = random_automaton.get_shortest_path(random_automaton.initial_state, state)
+            if state != random_automaton.initial_state and not state.prefix:
+                print('Non-reachable state:', state.state_id)
+
+    if ensure_minimality:
+        minimality_iterations = 1
+        while not random_automaton.is_minimal():
+            # to avoid infinite loops
+            if minimality_iterations == 100:
+                warnings.warn(f'Non-minimal automaton ({automaton_type}, num_states : {num_states}) returned.')
+                break
+
+            custom_args = {}
+            if 'custom_input_alphabet' in kwargs:
+                custom_args['custom_input_alphabet'] = kwargs.get('custom_input_alphabet')
+            if 'custom_output_alphabet' in kwargs:
+                custom_args['custom_output_alphabet'] = kwargs.get('custom_output_alphabet')
+            if 'num_accepting_states' in kwargs:
+                custom_args['num_accepting_states'] = kwargs.get('num_accepting_states')
+
+            random_automaton = generate_random_deterministic_automata(automaton_type,
+                                                                      num_states,
+                                                                      input_alphabet_size,
+                                                                      output_alphabet_size,
+                                                                      compute_prefixes,  # compute prefixes
+                                                                      False,  # ensure minimality
+                                                                      **custom_args)
+
+    return random_automaton
+
+
+def generate_random_mealy_machine(num_states, input_alphabet, output_alphabet,
+                                  compute_prefixes=False, ensure_minimality=True) -> MealyMachine:
+    """
+    Generates a random Mealy machine. Kept for backwards compatibility.
+
+    Args:
+
+        num_states: number of states
+        input_alphabet: input alphabet
+        output_alphabet: output alphabet
+        compute_prefixes: if true, shortest path to reach each state will be computed (Default value = False)
+        ensure_minimality: returned automaton will be minimal
+
+    Returns:
+
+        Mealy machine with num_states states
+    """
+
+    random_mealy_machine = generate_random_deterministic_automata('mealy', num_states,
+                                                                  input_alphabet_size=len(input_alphabet),
+                                                                  output_alphabet_size=len(output_alphabet),
+                                                                  ensure_minimality=ensure_minimality,
+                                                                  compute_prefixes=compute_prefixes,
+                                                                  custom_input_alphabet=input_alphabet,
+                                                                  custom_output_alphabet=output_alphabet)
+
+    return random_mealy_machine
+
+
+def generate_random_moore_machine(num_states, input_alphabet, output_alphabet,
+                                  compute_prefixes=False, ensure_minimality=True) -> MooreMachine:
+    """
+    Generates a random Moore machine.
+
+    Args:
+
+        num_states: number of states
+        input_alphabet: input alphabet
+        output_alphabet: output alphabet
+        compute_prefixes: if true, shortest path to reach each state will be computed (Default value = False)
+        ensure_minimality: returned automaton will be minimal
+
+    Returns:
+
+        Random Moore machine with num_states states
+
+    """
+    random_moore_machine = generate_random_deterministic_automata('moore', num_states,
+                                                                  input_alphabet_size=len(input_alphabet),
+                                                                  output_alphabet_size=len(output_alphabet),
+                                                                  ensure_minimality=ensure_minimality,
+                                                                  compute_prefixes=compute_prefixes,
+                                                                  custom_input_alphabet=input_alphabet,
+                                                                  custom_output_alphabet=output_alphabet)
+
+    return random_moore_machine
+
+
+def generate_random_dfa(num_states, alphabet, num_accepting_states=1,
+                        compute_prefixes=False, ensure_minimality=True) -> Dfa:
+    """
+    Generates a random DFA.
+
+    Args:
+
+        num_states: number of states
+        alphabet: input alphabet
+        num_accepting_states: number of accepting states (Default value = 1)
+        compute_prefixes: if true, shortest path to reach each state will be computed (Default value = False)
+        ensure_minimality: returned automaton will be minimal
+
+    Returns:
+
+        Randomly generated DFA
+
+    """
+    if num_states <= num_accepting_states:
+        num_accepting_states = num_states - 1
+
+    random_dfa = generate_random_deterministic_automata('dfa', num_states,
+                                                        input_alphabet_size=len(alphabet),
+                                                        output_alphabet_size=2,
+                                                        ensure_minimality=ensure_minimality,
+                                                        compute_prefixes=compute_prefixes,
+                                                        custom_input_alphabet=alphabet,
+                                                        num_accepting_states=num_accepting_states)
+
+    return random_dfa
+
+
+def generate_random_mdp(num_states, input_size, output_size, possible_probabilities=None):
+    """
+    Generates random MDP.
+
+    Args:
+
+        num_states: number of states
+        input_size: number of inputs
+        output_size: user predefined outputs
+        possible_probabilities: list of possible probability pairs to choose from
+
+    Returns:
+
+        random MDP
+
+    """
+
+    inputs = [f'i{i + 1}' for i in range(input_size)]
+    outputs = [f'o{i + 1}' for i in range(output_size)]
+
+    if not possible_probabilities:
+        possible_probabilities = [(1.,), (1.,), (1.,), (0.9, 0.1),
+                                  (0.8, 0.2), (0.7, 0.3), (0.8, 0.1, 0.1), (0.7, 0.2, 0.1), (0.6, 0.2, 0.1, 0.1)]
+        # ensure that there are no infinite loops
+        max_prob_num = min(num_states, input_size)
+        possible_probabilities = [p for p in possible_probabilities if len(p) <= max_prob_num]
+
+    state_outputs = outputs.copy()
+    states = []
+    for i in range(num_states):
+        curr_output = state_outputs.pop(0) if state_outputs else random.choice(outputs)
+        states.append(MdpState(f'q{i}', curr_output))
+
+    state_buffer = list(states)
+    for state in states:
+        for i in inputs:
+            prob = random.choice(possible_probabilities)
+            reached_states = []
+            for _ in prob:
+                while True:
+                    new_state = random.choice(state_buffer) if state_buffer else random.choice(states)
+
+                    # ensure determinism
+                    if new_state.output not in {s.output for s in reached_states}:
+                        if state_buffer:
+                            state_buffer.remove(new_state)
+                        break
+
+                reached_states.append(new_state)
+
+            for prob, reached_state in zip(prob, reached_states):
+                state.transitions[i].append((reached_state, prob))
+
+    return Mdp(states[0], states)
+
+
+def generate_random_smm(num_states, input_size, output_size, possible_probabilities=None):
+    """
+    Generates random SMM.
+
+    Args:
+
+        num_states: number of states
+        input_size: number of inputs
+        output_size: number of outputs
+        possible_probabilities: list of possible probability pairs to choose from
+
+    Returns:
+
+        random SMM
+
+    """
+
+    inputs = [f'i{i + 1}' for i in range(input_size)]
+    outputs = [f'o{i + 1}' for i in range(output_size)]
+
+    if not possible_probabilities:
+        possible_probabilities = [(1.,), (1.,), (1.,), (0.9, 0.1),
+                                  (0.8, 0.2), (0.7, 0.3), (0.8, 0.1, 0.1), (0.7, 0.2, 0.1), (0.6, 0.2, 0.1, 0.1)]
+        # ensure that there are no infinite loops
+        max_prob_num = min(num_states, input_size)
+        possible_probabilities = [p for p in possible_probabilities if len(p) <= max_prob_num]
+
+    states = []
+    for i in range(num_states):
+        states.append(StochasticMealyState(f'q{i}'))
+
+    state_buffer = list(states)
+    output_buffer = outputs.copy()
+    for state in states:
+        for i in inputs:
+            prob = random.choice(possible_probabilities)
+            reached_states = []
+            transition_outputs = []
+            for _ in prob:
+                while True:
+                    o = random.choice(output_buffer) if output_buffer else random.choice(outputs)
+                    new_state = random.choice(state_buffer) if state_buffer else random.choice(states)
+
+                    # ensure determinism
+                    if o not in transition_outputs:
+                        if output_buffer:
+                            output_buffer.remove(o)
+                        if state_buffer:
+                            state_buffer.remove(new_state)
+                        break
+
+                reached_states.append(new_state)
+                transition_outputs.append(o)
+
+            for index in range(len(prob)):
+                state.transitions[i].append((reached_states[index], transition_outputs[index], prob[index]))
+
+    return StochasticMealyMachine(states[0], states)
+
+
+def generate_random_ONFSM(num_states, num_inputs, num_outputs, multiple_out_prob=0.33):
+    """
+    Randomly generate an observable non-deterministic finite-state machine.
+
+    Args:
+
+      num_states: number of states
+      num_inputs: number of inputs
+      num_outputs: number of outputs
+      multiple_out_prob: probability that state will have multiple outputs (Default value = 0.5)
+
+    Returns:
+
+        randomly generated ONFSM
+
+    """
+    inputs = [f'i{i + 1}' for i in range(num_inputs)]
+    outputs = [f'o{i + 1}' for i in range(num_outputs)]
+
+    states = []
+    for i in range(num_states):
+        state = OnfsmState(f's{i}')
+        states.append(state)
+
+    state_buffer = states.copy()
+
+    for state in states:
+        for i in inputs:
+            state_outputs = 1
+            if random.random() <= multiple_out_prob and num_outputs >= 2:
+                state_outputs = random.randint(2, num_outputs)
+
+            random_out = random.sample(outputs, state_outputs)
+            for index in range(state_outputs):
+                if state_buffer:
+                    new_state = random.choice(state_buffer)
+                    state_buffer.remove(new_state)
+                else:
+                    new_state = random.choice(states)
+                state.transitions[i].append((random_out[index], new_state))
+
+    return Onfsm(states[0], states)
+
+
+def generate_random_markov_chain(num_states):
+    assert num_states >= 3
+    possible_probabilities = [1.0, 1.0, 0.8, 0.5, 0.9]
+    states = []
+
+    for i in range(num_states):
+        states.append(McState(f'q{i}', i))
+
+    for index, state in enumerate(states[:-1]):
+        prob = random.choice(possible_probabilities)
+        if prob == 1.:
+            new_state = states[index + 1]
+            state.transitions.append((new_state, prob))
+        else:
+            next_state = states[index + 1]
+            up_states = list(states)
+            up_states.remove(next_state)
+            rand_state = random.choice(up_states)
+
+            state.transitions.append((next_state, prob))
+            state.transitions.append((rand_state, round(1 - prob, 2)))
+
+    return MarkovChain(states[0], states)
+
+
+def dfa_from_state_setup(state_setup) -> Dfa:
+    """
+        First state in the state setup is the initial state.
+        Example state setup:
+        state_setup = {
+                "a": (True, {"x": "b1", "y": "a"}),
+                "b1": (False, {"x": "b2", "y": "a"}),
+                "b2": (True, {"x": "b3", "y": "a"}),
+                "b3": (False, {"x": "b4", "y": "a"}),
+                "b4": (False, {"x": "c", "y": "a"}),
+                "c": (True, {"x": "a", "y": "a"}),
+            }
+
+        Args:
+
+            state_setup: map from state_id to tuple(output and transitions_dict)
+
+        Returns:
+
+            DFA
+        """
+    # state_setup should map from state_id to tuple(is_accepting and transitions_dict)
+
+    # build states with state_id and output
+    states = {key: DfaState(key, val[0]) for key, val in state_setup.items()}
+
+    # add transitions to states
+    for state_id, state in states.items():
+        for _input, target_state_id in state_setup[state_id][1].items():
+            state.transitions[_input] = states[target_state_id]
+
+    # states to list
+    states = [state for state in states.values()]
+
+    # build moore machine with first state as starting state
+    dfa = Dfa(states[0], states)
+
+    for state in states:
+        state.prefix = dfa.get_shortest_path(dfa.initial_state, state)
+
+    return dfa
+
+
+def moore_from_state_setup(state_setup) -> MooreMachine:
+    """
+    First state in the state setup is the initial state.
+    Example state setup:
+    state_setup = {
+            "a": ("a", {"x": "b1", "y": "a"}),
+            "b1": ("b", {"x": "b2", "y": "a"}),
+            "b2": ("b", {"x": "b3", "y": "a"}),
+            "b3": ("b", {"x": "b4", "y": "a"}),
+            "b4": ("b", {"x": "c", "y": "a"}),
+            "c": ("c", {"x": "a", "y": "a"}),
+        }
+
+    Args:
+
+        state_setup: map from state_id to tuple(output and transitions_dict)
+
+    Returns:
+
+        Moore machine
+    """
+
+    # build states with state_id and output
+    states = {key: MooreState(key, val[0]) for key, val in state_setup.items()}
+
+    # add transitions to states
+    for state_id, state in states.items():
+        for _input, target_state_id in state_setup[state_id][1].items():
+            state.transitions[_input] = states[target_state_id]
+
+    # states to list
+    states = [state for state in states.values()]
+
+    # build moore machine with first state as starting state
+    mm = MooreMachine(states[0], states)
+
+    for state in states:
+        state.prefix = mm.get_shortest_path(mm.initial_state, state)
+
+    return mm
+
+
+def mealy_from_state_setup(state_setup) -> MealyMachine:
+    """
+        First state in the state setup is the initial state.
+        state_setup = {
+            "a": {"x": ("o1", "b1"), "y": ("o2", "a")},
+            "b1": {"x": ("o3", "b2"), "y": ("o1", "a")},
+            "b2": {"x": ("o1", "b3"), "y": ("o2", "a")},
+            "b3": {"x": ("o3", "b4"), "y": ("o1", "a")},
+            "b4": {"x": ("o1", "c"), "y": ("o4", "a")},
+            "c": {"x": ("o3", "a"), "y": ("o5", "a")},
+        }
+
+
+    Args:
+
+        state_setup:
+            state_setup should map from state_id to tuple(transitions_dict).
+
+    Returns:
+
+        Mealy Machine
+    """
+    # state_setup should map from state_id to tuple(transitions_dict).
+    # Each entry in transition dict is <input> : <output, new_state_id>
+
+    # build states with state_id and output
+    states = {key: MealyState(key) for key, _ in state_setup.items()}
+
+    # add transitions to states
+    for state_id, state in states.items():
+        for _input, (output, new_state) in state_setup[state_id].items():
+            state.transitions[_input] = states[new_state]
+            state.output_fun[_input] = output
+
+    # states to list
+    states = [state for state in states.values()]
+
+    # build moore machine with first state as starting state
+    mm = MealyMachine(states[0], states)
+
+    for state in states:
+        state.prefix = mm.get_shortest_path(mm.initial_state, state)
+
+    return mm
+
+
+def mdp_from_state_setup(state_setup):
+    from aalpy.automata import MdpState, Mdp
+    states_map = {key: MdpState(key, output=value[0]) for key, value in state_setup.items()}
+
+    for key, values in state_setup.items():
+        source = states_map[key]
+        for i, transitions in values[1].items():
+            for node, prob in transitions:
+                source.transitions[i].append((states_map[node], prob))
+
+    initial_state = states_map[list(state_setup.keys())[0]]
+    return Mdp(initial_state, list(states_map.values()))
+
+
+def smm_from_state_setup(state_setup):
+    from aalpy.automata import StochasticMealyState, StochasticMealyMachine
+    states_map = {key: StochasticMealyState(key) for key in state_setup.keys()}
+
+    for key, values in state_setup.items():
+        source = states_map[key]
+        for i, transitions in values.items():
+            for node, output, prob in transitions:
+                source.transitions[i].append((states_map[node], output, prob))
+
+    initial_state = states_map[list(state_setup.keys())[0]]
+    return StochasticMealyMachine(initial_state, list(states_map.values()))
```

## aalpy/utils/BenchmarkSULs.py

```diff
@@ -1,387 +1,549 @@
-def get_Angluin_dfa():
-    from aalpy.utils.AutomatonGenerators import dfa_from_state_setup
-
-    anguin_dfa = {
-        'q0': (True, {'a': 'q1', 'b': 'q2'}),
-        'q1': (False, {'a': 'q0', 'b': 'q3'}),
-        'q2': (False, {'a': 'q3', 'b': 'q0'}),
-        'q3': (False, {'a': 'q2', 'b': 'q1'})
-    }
-
-    return dfa_from_state_setup(anguin_dfa)
-
-
-def get_benchmark_ONFSM():
-    """
-    Returns ONFSM presented in 'Learning Finite State Models of Observable Nondeterministic Systems in a Testing
-    Context'.
-    """
-    from aalpy.automata import Onfsm, OnfsmState
-
-    a = OnfsmState('q0')
-    b = OnfsmState('q1')
-    c = OnfsmState('g2')
-    d = OnfsmState('q3')
-
-    a.transitions['a'].append((0, b))
-    a.transitions['b'].append((2, a))
-    a.transitions['b'].append((0, c))
-
-    b.transitions['a'].append((2, a))
-    b.transitions['b'].append((3, b))
-
-    c.transitions['a'].append((2, d))
-    c.transitions['b'].append((0, c))
-    c.transitions['b'].append((3, c))
-
-    d.transitions['a'].append((2, b))
-    d.transitions['b'].append((3, d))
-
-    return Onfsm(a, [a, b, c, d])
-
-
-def get_ONFSM():
-    """
-    Returns example of an ONFSM.
-    """
-    from aalpy.automata import Onfsm, OnfsmState
-
-    q0 = OnfsmState('q0')
-    q1 = OnfsmState('q1')
-    q2 = OnfsmState('q2')
-    q3 = OnfsmState('q3')
-    q4 = OnfsmState('q4')
-    q5 = OnfsmState('q5')
-    q6 = OnfsmState('q6')
-    q7 = OnfsmState('q7')
-    q8 = OnfsmState('q8')
-
-    q0.transitions['a'].append((2, q1))
-    q0.transitions['b'].append((0, q0))
-
-    q1.transitions['a'].append((2, q0))
-    q1.transitions['b'].append((0, q2))
-
-    q2.transitions['a'].append((1, q2))
-    q2.transitions['b'].append((0, q3))
-
-    q3.transitions['a'].append((2, q8))
-    q3.transitions['b'].append((0, q4))
-
-    q4.transitions['a'].append((1, q4))
-    q4.transitions['b'].append((0, q5))
-
-    q5.transitions['a'].append((2, q6))
-    q5.transitions['b'].append((0, q7))
-
-    q6.transitions['a'].append((2, q5))
-    q6.transitions['b'].append((0, q6))
-
-    q7.transitions['a'].append((1, q7))
-    q7.transitions['b'].append(('O', q0))
-
-    q8.transitions['a'].append((2, q3))
-    q8.transitions['b'].append((0, q8))
-
-    return Onfsm(q0, [q0, q1, q2, q3, q4, q5, q6, q7, q8])
-
-
-def get_faulty_coffee_machine_MDP():
-    from aalpy.automata import Mdp, MdpState
-
-    q0 = MdpState("q0", "init")
-    q1 = MdpState("q1", "beep")
-    q2 = MdpState("q2", "coffee")
-
-    q0.transitions['but'].append((q0, 1))
-    q0.transitions['coin'].append((q1, 1))
-    q1.transitions['but'].append((q0, 0.1))
-    q1.transitions['but'].append((q2, 0.9))
-    q1.transitions['coin'].append((q1, 1))
-    q2.transitions['but'].append((q0, 1))
-    q2.transitions['coin'].append((q1, 1))
-
-    mdp = Mdp(q0, [q0, q1, q2])
-
-    return mdp
-
-
-def get_weird_coffee_machine_MDP():
-    from aalpy.automata import Mdp, MdpState
-
-    q0 = MdpState("q0", "init")
-    q1 = MdpState("q1", "beep")
-    q2 = MdpState("q2", "coffee")
-    q3 = MdpState("q3", "beep")
-    q4 = MdpState("q4", "coffee")
-    q5 = MdpState("q5", "init")
-    q6 = MdpState("q6", "crash")
-
-    q0.transitions['but'].append((q0, 1))
-    q0.transitions['coin'].append((q1, 1))
-    q0.transitions['koin'].append((q3, 1))
-
-    q1.transitions['but'].append((q0, 0.1))
-    q1.transitions['but'].append((q2, 0.9))
-
-    q3.transitions['but'].append((q0, 0.1))
-    q3.transitions['but'].append((q4, 0.9))
-
-    q1.transitions['coin'].append((q1, 1))
-    q3.transitions['koin'].append((q3, 1))
-    q1.transitions['koin'].append((q3, 1))
-    q3.transitions['coin'].append((q1, 1))
-
-    q2.transitions['but'].append((q0, 1))
-    q2.transitions['coin'].append((q1, 1))
-    q2.transitions['koin'].append((q3, 1))
-
-    q4.transitions['coin'].append((q1, 1))
-    q4.transitions['koin'].append((q3, 1))
-
-    q4.transitions['but'].append((q5, 1))
-
-    q5.transitions['but'].append((q6, 1))
-    q5.transitions['coin'].append((q6, 1))
-    q5.transitions['koin'].append((q6, 1))
-
-    q6.transitions['but'].append((q6, 1))
-    q6.transitions['coin'].append((q6, 1))
-    q6.transitions['koin'].append((q6, 1))
-
-    mdp = Mdp(q0, [q0, q1, q2, q3, q4, q5, q6])
-
-    return mdp
-
-
-def get_faulty_coffee_machine_SMM():
-    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
-
-    s0 = StochasticMealyState('s0')
-    s1 = StochasticMealyState('s1')
-    s2 = StochasticMealyState('s2')
-
-    s0.transitions['but'].append((s0, 'init', 1.))
-    s0.transitions['coin'].append((s1, 'beep', 1.))
-    s1.transitions['but'].append((s0, 'init', 0.1))
-    s1.transitions['but'].append((s2, 'coffee', 0.9))
-    s1.transitions['coin'].append((s1, 'beep', 1.))
-    s2.transitions['but'].append((s0, 'init', 1.))
-    s2.transitions['coin'].append((s1, 'beep', 1.))
-
-    smm = StochasticMealyMachine(s0, [s0, s1, s2])
-
-    return smm
-
-
-def get_minimal_faulty_coffee_machine_SMM():
-    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
-
-    s0 = StochasticMealyState('s0')
-    s1 = StochasticMealyState('s1')
-
-    s0.transitions['but'].append((s0, 'init', 1.))
-    s0.transitions['coin'].append((s1, 'beep', 1.))
-    s1.transitions['but'].append((s0, 'init', 0.1))
-    s1.transitions['but'].append((s0, 'coffee', 0.9))
-    s1.transitions['coin'].append((s1, 'beep', 1.))
-
-    smm = StochasticMealyMachine(s0, [s0, s1])
-
-    return smm
-
-
-def get_faulty_mqtt_SMM():
-    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
-
-    s0 = StochasticMealyState('s0')
-    s1 = StochasticMealyState('s1')
-    s2 = StochasticMealyState('s2')
-
-    s0.transitions['connect'].append((s1, 'CONNACK', 1.))
-    s0.transitions['disconnect'].append((s0, 'CONCLOSED', 1.))
-    s0.transitions['publish'].append((s0, 'CONCLOSED', 1.))
-    s0.transitions['subscribe'].append((s0, 'CONCLOSED', 1.))
-    s0.transitions['unsubscribe'].append((s0, 'CONCLOSED', 1.))
-
-    s1.transitions['connect'].append((s0, 'CONCLOSED', 1.))
-    s1.transitions['disconnect'].append((s0, 'CONCLOSED', 1.))
-    s1.transitions['publish'].append((s1, 'PUBACK', 0.9))
-    s1.transitions['publish'].append((s0, 'CONCLOSED', 0.1))
-    s1.transitions['subscribe'].append((s2, 'SUBACK', 1.))
-    s1.transitions['unsubscribe'].append((s1, 'UNSUBACK', 1.))
-
-    s2.transitions['connect'].append((s0, 'CONCLOSED', 1.))
-    s2.transitions['disconnect'].append((s0, 'CONCLOSED', 1.))
-    s2.transitions['publish'].append((s2, 'PUBLISH_PUBACK', 1.))
-    s2.transitions['subscribe'].append((s2, 'SUBACK', 1.))
-    s2.transitions['unsubscribe'].append((s1, 'UNSUBACK', 0.8))
-    s2.transitions['unsubscribe'].append((s2, 'SUBACK', 0.2))
-
-    smm = StochasticMealyMachine(s0, [s0, s1, s2])
-
-    return smm
-
-
-def get_small_gridworld():
-    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
-
-    s0 = StochasticMealyState('s0')
-    s1 = StochasticMealyState('s1')
-    s2 = StochasticMealyState('s2')
-    s3 = StochasticMealyState('s3')
-
-    p_g = 0.8
-    p_m = 0.6
-
-    # gridworld of the form
-    # W W W W with a start in the top left
-    # W G M W states like s0 s1
-    # W M G W             s2 s3
-    # W W W W
-
-    s0.transitions['north'].append((s0, 'wall', 1.))
-    s0.transitions['west'].append((s0, 'wall', 1.))
-    s0.transitions['east'].append((s1, 'mud', p_m))
-    s0.transitions['east'].append((s3, 'grass', 1 - p_m))
-    s0.transitions['south'].append((s2, 'mud', p_m))
-    s0.transitions['south'].append((s3, 'grass', 1 - p_m))
-
-    s1.transitions['north'].append((s1, 'wall', 1.))
-    s1.transitions['east'].append((s1, 'wall', 1.))
-    s1.transitions['west'].append((s0, 'grass', p_g))
-    s1.transitions['west'].append((s2, 'mud', 1 - p_g))
-    s1.transitions['south'].append((s3, 'grass', p_g))
-    s1.transitions['south'].append((s2, 'mud', 1 - p_g))
-
-    s2.transitions['south'].append((s2, 'wall', 1.))
-    s2.transitions['west'].append((s2, 'wall', 1.))
-    s2.transitions['east'].append((s3, 'grass', p_g))
-    s2.transitions['east'].append((s1, 'mud', 1 - p_g))
-    s2.transitions['north'].append((s0, 'grass', p_g))
-    s2.transitions['south'].append((s1, 'mud', 1 - p_g))
-
-    s3.transitions['south'].append((s3, 'wall', 1.))
-    s3.transitions['east'].append((s3, 'wall', 1.))
-    s3.transitions['west'].append((s2, 'mud', p_m))
-    s3.transitions['west'].append((s0, 'grass', 1 - p_m))
-    s3.transitions['north'].append((s1, 'mud', p_m))
-    s3.transitions['north'].append((s0, 'grass', 1 - p_m))
-
-    smm = StochasticMealyMachine(s0, [s0, s1, s2, s3])
-
-    return smm
-
-
-class MockMqttExample:
-
-    def __init__(self):
-        self.state = 'CONCLOSED'
-        self.topics = set()
-
-    def subscribe(self, topic: str):
-        if '\n' in topic or '\u0000' in topic:
-            self.state = 'CONCLOSED'
-            self.topics.clear()
-        elif self.state != 'CONCLOSED':
-            self.topics.add(topic)
-            self.state = 'SUBACK'
-
-        return self.state
-
-    def unsubscribe(self, topic):
-        if '\n' in topic or '\u0000' in topic:
-            self.state = 'CONCLOSED'
-            self.topics.clear()
-        elif self.state != 'CONCLOSED':
-            if topic in self.topics:
-                self.topics.remove(topic)
-            self.state = 'UNSUBACK'
-
-        return self.state
-
-    def connect(self):
-        if self.state == 'CONCLOSED':
-            self.state = 'CONNACK'
-        else:
-            self.topics.clear()
-            self.state = 'CONCLOSED'
-        return self.state
-
-    def disconnect(self):
-        self.state = 'CONCLOSED'
-        self.topics.clear()
-        return self.state
-
-    def publish(self, topic):
-        if '\n' in topic or '\u0000' in topic:
-            self.state = 'CONCLOSED'
-            self.topics.clear()
-        if self.state != 'CONCLOSED':
-            if topic not in self.topics:
-                self.state = 'PUBACK'
-            else:
-                self.state = 'PUBACK_PUBACK'
-        return self.state
-
-
-class DateValidator:
-    """
-    Class mimicking Date Validator API.
-    It does not account for the leap years.
-    The format of the dates is %d/%m/%Y'
-    """
-
-    def is_date_accepted(self, date_string: str):
-        values = date_string.split('/')
-        if len(values) != 3:
-            return False
-        try:
-            day = int(values[0])
-            month = int(values[1])
-            year = int(values[2])
-        except ValueError:
-            return False
-
-        if not (0 <= year <= 9999):
-            return False
-
-        if month == 2 and not (1 <= day <= 28):
-            return False
-
-        if month in [1, 3, 5, 7, 8, 10, 12] and not (1 <= day <= 31):
-            return False
-
-        elif not (1 <= day <= 31):
-            return False
-
-        return True
-
-
-def get_small_pomdp():
-    from aalpy.automata import Mdp, MdpState
-
-    q0 = MdpState("q0", "init")
-    q1 = MdpState("q1", "beep")
-    q2 = MdpState("q2", "beep")
-    q3 = MdpState("q3", "coffee")
-    q4 = MdpState("q4", "tea")
-
-    q0.transitions['but'].append((q0, 1))
-    q0.transitions['coin'].append((q1, 0.8))
-    q0.transitions['coin'].append((q2, 0.2))
-
-    q1.transitions['coin'].append((q1, 1))
-    q1.transitions['but'].append((q3, 1))
-
-    q2.transitions['coin'].append((q2, 0.3))
-    q2.transitions['coin'].append((q1, 0.7))
-    q2.transitions['but'].append((q4, 1))
-
-    q3.transitions['coin'].append((q3, 1))
-    q3.transitions['but'].append((q3, 1))
-
-    q4.transitions['coin'].append((q4, 1))
-    q4.transitions['but'].append((q4, 1))
-
-    return Mdp(q0, [q0, q1, q2, q3, q4])
+def get_Angluin_dfa():
+    from aalpy.utils.AutomatonGenerators import dfa_from_state_setup
+
+    anguin_dfa = {
+        'q0': (True, {'a': 'q1', 'b': 'q2'}),
+        'q1': (False, {'a': 'q0', 'b': 'q3'}),
+        'q2': (False, {'a': 'q3', 'b': 'q0'}),
+        'q3': (False, {'a': 'q2', 'b': 'q1'})
+    }
+
+    return dfa_from_state_setup(anguin_dfa)
+
+
+def get_benchmark_ONFSM():
+    """
+    Returns ONFSM presented in 'Learning Finite State Models of Observable Nondeterministic Systems in a Testing
+    Context'.
+    """
+    from aalpy.automata import Onfsm, OnfsmState
+
+    a = OnfsmState('q0')
+    b = OnfsmState('q1')
+    c = OnfsmState('g2')
+    d = OnfsmState('q3')
+
+    a.transitions['a'].append((0, b))
+    a.transitions['b'].append((2, a))
+    a.transitions['b'].append((0, c))
+
+    b.transitions['a'].append((2, a))
+    b.transitions['b'].append((3, b))
+
+    c.transitions['a'].append((2, d))
+    c.transitions['b'].append((0, c))
+    c.transitions['b'].append((3, c))
+
+    d.transitions['a'].append((2, b))
+    d.transitions['b'].append((3, d))
+
+    return Onfsm(a, [a, b, c, d])
+
+
+def get_ONFSM():
+    """
+    Returns example of an ONFSM.
+    """
+    from aalpy.automata import Onfsm, OnfsmState
+
+    q0 = OnfsmState('q0')
+    q1 = OnfsmState('q1')
+    q2 = OnfsmState('q2')
+    q3 = OnfsmState('q3')
+    q4 = OnfsmState('q4')
+    q5 = OnfsmState('q5')
+    q6 = OnfsmState('q6')
+    q7 = OnfsmState('q7')
+    q8 = OnfsmState('q8')
+
+    q0.transitions['a'].append((2, q1))
+    q0.transitions['b'].append((0, q0))
+
+    q1.transitions['a'].append((2, q0))
+    q1.transitions['b'].append((0, q2))
+
+    q2.transitions['a'].append((1, q2))
+    q2.transitions['b'].append((0, q3))
+
+    q3.transitions['a'].append((2, q8))
+    q3.transitions['b'].append((0, q4))
+
+    q4.transitions['a'].append((1, q4))
+    q4.transitions['b'].append((0, q5))
+
+    q5.transitions['a'].append((2, q6))
+    q5.transitions['b'].append((0, q7))
+
+    q6.transitions['a'].append((2, q5))
+    q6.transitions['b'].append((0, q6))
+
+    q7.transitions['a'].append((1, q7))
+    q7.transitions['b'].append(('O', q0))
+
+    q8.transitions['a'].append((2, q3))
+    q8.transitions['b'].append((0, q8))
+
+    return Onfsm(q0, [q0, q1, q2, q3, q4, q5, q6, q7, q8])
+
+
+def get_faulty_coffee_machine_MDP():
+    from aalpy.automata import Mdp, MdpState
+
+    q0 = MdpState("q0", "init")
+    q1 = MdpState("q1", "beep")
+    q2 = MdpState("q2", "coffee")
+
+    q0.transitions['but'].append((q0, 1))
+    q0.transitions['coin'].append((q1, 1))
+    q1.transitions['but'].append((q0, 0.1))
+    q1.transitions['but'].append((q2, 0.9))
+    q1.transitions['coin'].append((q1, 1))
+    q2.transitions['but'].append((q0, 1))
+    q2.transitions['coin'].append((q1, 1))
+
+    mdp = Mdp(q0, [q0, q1, q2])
+
+    return mdp
+
+
+def get_weird_coffee_machine_MDP():
+    from aalpy.automata import Mdp, MdpState
+
+    q0 = MdpState("q0", "init")
+    q1 = MdpState("q1", "beep")
+    q2 = MdpState("q2", "coffee")
+    q3 = MdpState("q3", "beep")
+    q4 = MdpState("q4", "coffee")
+    q5 = MdpState("q5", "init")
+    q6 = MdpState("q6", "crash")
+
+    q0.transitions['but'].append((q0, 1))
+    q0.transitions['coin'].append((q1, 1))
+    q0.transitions['koin'].append((q3, 1))
+
+    q1.transitions['but'].append((q0, 0.1))
+    q1.transitions['but'].append((q2, 0.9))
+
+    q3.transitions['but'].append((q0, 0.1))
+    q3.transitions['but'].append((q4, 0.9))
+
+    q1.transitions['coin'].append((q1, 1))
+    q3.transitions['koin'].append((q3, 1))
+    q1.transitions['koin'].append((q3, 1))
+    q3.transitions['coin'].append((q1, 1))
+
+    q2.transitions['but'].append((q0, 1))
+    q2.transitions['coin'].append((q1, 1))
+    q2.transitions['koin'].append((q3, 1))
+
+    q4.transitions['coin'].append((q1, 1))
+    q4.transitions['koin'].append((q3, 1))
+
+    q4.transitions['but'].append((q5, 1))
+
+    q5.transitions['but'].append((q6, 1))
+    q5.transitions['coin'].append((q6, 1))
+    q5.transitions['koin'].append((q6, 1))
+
+    q6.transitions['but'].append((q6, 1))
+    q6.transitions['coin'].append((q6, 1))
+    q6.transitions['koin'].append((q6, 1))
+
+    mdp = Mdp(q0, [q0, q1, q2, q3, q4, q5, q6])
+
+    return mdp
+
+
+def get_faulty_coffee_machine_SMM():
+    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
+
+    s0 = StochasticMealyState('q0')
+    s1 = StochasticMealyState('q1')
+    s2 = StochasticMealyState('q2')
+
+    s0.transitions['but'].append((s0, 'init', 1.))
+    s0.transitions['coin'].append((s1, 'beep', 1.))
+    s1.transitions['but'].append((s0, 'init', 0.1))
+    s1.transitions['but'].append((s2, 'coffee', 0.9))
+    s1.transitions['coin'].append((s1, 'beep', 1.))
+    s2.transitions['but'].append((s0, 'init', 1.))
+    s2.transitions['coin'].append((s1, 'beep', 1.))
+
+    smm = StochasticMealyMachine(s0, [s0, s1, s2])
+
+    return smm
+
+
+def get_minimal_faulty_coffee_machine_SMM():
+    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
+
+    s0 = StochasticMealyState('q0')
+    s1 = StochasticMealyState('q1')
+
+    s0.transitions['but'].append((s0, 'init', 1.))
+    s0.transitions['coin'].append((s1, 'beep', 1.))
+    s1.transitions['but'].append((s0, 'init', 0.1))
+    s1.transitions['but'].append((s0, 'coffee', 0.9))
+    s1.transitions['coin'].append((s1, 'beep', 1.))
+
+    smm = StochasticMealyMachine(s0, [s0, s1])
+
+    return smm
+
+
+def get_faulty_mqtt_SMM():
+    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
+
+    s0 = StochasticMealyState('q0')
+    s1 = StochasticMealyState('q1')
+    s2 = StochasticMealyState('q2')
+
+    s0.transitions['connect'].append((s1, 'CONNACK', 1.))
+    s0.transitions['disconnect'].append((s0, 'CONCLOSED', 1.))
+    s0.transitions['publish'].append((s0, 'CONCLOSED', 1.))
+    s0.transitions['subscribe'].append((s0, 'CONCLOSED', 1.))
+    s0.transitions['unsubscribe'].append((s0, 'CONCLOSED', 1.))
+
+    s1.transitions['connect'].append((s0, 'CONCLOSED', 1.))
+    s1.transitions['disconnect'].append((s0, 'CONCLOSED', 1.))
+    s1.transitions['publish'].append((s1, 'PUBACK', 0.9))
+    s1.transitions['publish'].append((s0, 'CONCLOSED', 0.1))
+    s1.transitions['subscribe'].append((s2, 'SUBACK', 1.))
+    s1.transitions['unsubscribe'].append((s1, 'UNSUBACK', 1.))
+
+    s2.transitions['connect'].append((s0, 'CONCLOSED', 1.))
+    s2.transitions['disconnect'].append((s0, 'CONCLOSED', 1.))
+    s2.transitions['publish'].append((s2, 'PUBLISH_PUBACK', 1.))
+    s2.transitions['subscribe'].append((s2, 'SUBACK', 1.))
+    s2.transitions['unsubscribe'].append((s1, 'UNSUBACK', 0.8))
+    s2.transitions['unsubscribe'].append((s2, 'SUBACK', 0.2))
+
+    smm = StochasticMealyMachine(s0, [s0, s1, s2])
+
+    return smm
+
+
+def get_small_gridworld():
+    from aalpy.automata import StochasticMealyMachine, StochasticMealyState
+
+    s0 = StochasticMealyState('q0')
+    s1 = StochasticMealyState('q1')
+    s2 = StochasticMealyState('q2')
+    s3 = StochasticMealyState('q3')
+
+    p_g = 0.8
+    p_m = 0.6
+
+    # gridworld of the form
+    # W W W W with a start in the top left
+    # W G M W states like s0 s1
+    # W M G W             s2 s3
+    # W W W W
+
+    s0.transitions['north'].append((s0, 'wall', 1.))
+    s0.transitions['west'].append((s0, 'wall', 1.))
+    s0.transitions['east'].append((s1, 'mud', p_m))
+    s0.transitions['east'].append((s3, 'grass', 1 - p_m))
+    s0.transitions['south'].append((s2, 'mud', p_m))
+    s0.transitions['south'].append((s3, 'grass', 1 - p_m))
+
+    s1.transitions['north'].append((s1, 'wall', 1.))
+    s1.transitions['east'].append((s1, 'wall', 1.))
+    s1.transitions['west'].append((s0, 'grass', p_g))
+    s1.transitions['west'].append((s2, 'mud', 1 - p_g))
+    s1.transitions['south'].append((s3, 'grass', p_g))
+    s1.transitions['south'].append((s2, 'mud', 1 - p_g))
+
+    s2.transitions['south'].append((s2, 'wall', 1.))
+    s2.transitions['west'].append((s2, 'wall', 1.))
+    s2.transitions['east'].append((s3, 'grass', p_g))
+    s2.transitions['east'].append((s1, 'mud', 1 - p_g))
+    s2.transitions['north'].append((s0, 'grass', p_g))
+    s2.transitions['south'].append((s1, 'mud', 1 - p_g))
+
+    s3.transitions['south'].append((s3, 'wall', 1.))
+    s3.transitions['east'].append((s3, 'wall', 1.))
+    s3.transitions['west'].append((s2, 'mud', p_m))
+    s3.transitions['west'].append((s0, 'grass', 1 - p_m))
+    s3.transitions['north'].append((s1, 'mud', p_m))
+    s3.transitions['north'].append((s0, 'grass', 1 - p_m))
+
+    smm = StochasticMealyMachine(s0, [s0, s1, s2, s3])
+
+    return smm
+
+
+class MockMqttExample:
+
+    def __init__(self):
+        self.state = 'CONCLOSED'
+        self.topics = set()
+
+    def subscribe(self, topic: str):
+        if '\n' in topic or '\u0000' in topic:
+            self.state = 'CONCLOSED'
+            self.topics.clear()
+        elif self.state != 'CONCLOSED':
+            self.topics.add(topic)
+            self.state = 'SUBACK'
+
+        return self.state
+
+    def unsubscribe(self, topic):
+        if '\n' in topic or '\u0000' in topic:
+            self.state = 'CONCLOSED'
+            self.topics.clear()
+        elif self.state != 'CONCLOSED':
+            if topic in self.topics:
+                self.topics.remove(topic)
+            self.state = 'UNSUBACK'
+
+        return self.state
+
+    def connect(self):
+        if self.state == 'CONCLOSED':
+            self.state = 'CONNACK'
+        else:
+            self.topics.clear()
+            self.state = 'CONCLOSED'
+        return self.state
+
+    def disconnect(self):
+        self.state = 'CONCLOSED'
+        self.topics.clear()
+        return self.state
+
+    def publish(self, topic):
+        if '\n' in topic or '\u0000' in topic:
+            self.state = 'CONCLOSED'
+            self.topics.clear()
+        if self.state != 'CONCLOSED':
+            if topic not in self.topics:
+                self.state = 'PUBACK'
+            else:
+                self.state = 'PUBACK_PUBACK'
+        return self.state
+
+
+class DateValidator:
+    """
+    Class mimicking Date Validator API.
+    It does not account for the leap years.
+    The format of the dates is %d/%m/%Y'
+    """
+
+    def is_date_accepted(self, date_string: str):
+        values = date_string.split('/')
+        if len(values) != 3:
+            return False
+        try:
+            day = int(values[0])
+            month = int(values[1])
+            year = int(values[2])
+        except ValueError:
+            return False
+
+        if not (0 <= year <= 9999):
+            return False
+
+        if month == 2 and not (1 <= day <= 28):
+            return False
+
+        if month in [1, 3, 5, 7, 8, 10, 12] and not (1 <= day <= 31):
+            return False
+
+        elif not (1 <= day <= 31):
+            return False
+
+        return True
+
+
+def get_small_pomdp():
+    from aalpy.automata import Mdp, MdpState
+
+    q0 = MdpState("q0", "init")
+    q1 = MdpState("q1", "beep")
+    q2 = MdpState("q2", "beep")
+    q3 = MdpState("q3", "coffee")
+    q4 = MdpState("q4", "tea")
+
+    q0.transitions['but'].append((q0, 1))
+    q0.transitions['coin'].append((q1, 0.8))
+    q0.transitions['coin'].append((q2, 0.2))
+
+    q1.transitions['coin'].append((q1, 1))
+    q1.transitions['but'].append((q3, 1))
+
+    q2.transitions['coin'].append((q2, 0.3))
+    q2.transitions['coin'].append((q1, 0.7))
+    q2.transitions['but'].append((q4, 1))
+
+    q3.transitions['coin'].append((q3, 1))
+    q3.transitions['but'].append((q3, 1))
+
+    q4.transitions['coin'].append((q4, 1))
+    q4.transitions['but'].append((q4, 1))
+
+    return Mdp(q0, [q0, q1, q2, q3, q4])
+
+#
+# class CarAlarmSystem:
+#
+#     def __init__(self, max_wait_time=700):
+#         self.timer = 0
+#         self.max_wait_time = max_wait_time
+#         self.bonnet_open = False
+#         self.trunk_open = False
+#         self.doors_opened = [False, False, False, False]
+#         # Is alarm active
+#         self.alarm_active = False
+#         # Is alarm triggered
+#         self.alarm_triggered = False
+#         # Is car locked
+#         self.is_locked = False
+#         # Time at which car was locked and alarm activated
+#         self.locked_time = None
+#         self.alarm_activation_time = None
+#
+#     def __get_alarm_status(self):
+#         if self.is_locked and self.alarm_active:
+#             self.alarm_triggered = True
+#         if not self.alarm_triggered:
+#             return None
+#         alarm_note = ''
+#         if abs(self.timer - self.alarm_activation_time) <= 30:
+#             alarm_note += '_ALARM_SOUND'
+#         if abs(self.timer - self.alarm_activation_time) <= 500:
+#             alarm_note += '_ALARM_LIGHTS'
+#         return alarm_note
+#
+#     def open_bonnet(self):
+#         if self.bonnet_open:
+#             return 'Bonnet already opened'
+#         self.bonnet_open = True
+#         alarm_note = self.__get_alarm_status()
+#         return_msg = 'Bonnet opened'
+#         if alarm_note:
+#             return_msg += alarm_note
+#         return return_msg
+#
+#     def close_bonnet(self):
+#         if not self.bonnet_open:
+#             return 'Bonnet already closed'
+#         self.bonnet_open = False
+#         alarm_note = self.__get_alarm_status()
+#         return_msg = 'Bonnet closed'
+#         if alarm_note:
+#             return_msg += alarm_note
+#         return return_msg
+#
+#     def open_trunk(self):
+#         if self.trunk_open:
+#             return 'Trunk already opened'
+#         self.trunk_open = True
+#         alarm_note = self.__get_alarm_status()
+#         return_msg = 'Trunk opened'
+#         if alarm_note:
+#             return_msg += alarm_note
+#         return return_msg
+#
+#     def close_trunk(self):
+#         if not self.trunk_open:
+#             return 'Trunk already closed'
+#         self.trunk_open = False
+#         alarm_note = self.__get_alarm_status()
+#         return_msg = 'Trunk closed'
+#         if alarm_note:
+#             return_msg += alarm_note
+#         return return_msg
+#
+#     def open_door(self, door_id):
+#         door_id = door_id - 1
+#         if self.doors_opened[door_id]:
+#             return 'Door already opened'
+#         self.doors_opened[door_id] = True
+#         alarm_note = self.__get_alarm_status()
+#         return_msg = 'Doors opened'
+#         if alarm_note:
+#             return_msg += alarm_note
+#         return return_msg
+#
+#     def close_door(self, door_id):
+#         door_id = door_id - 1
+#         if not self.doors_opened[door_id]:
+#             return 'Door already closed'
+#         self.doors_opened[door_id] = False
+#         alarm_note = self.__get_alarm_status()
+#         return_msg = 'Doors closed'
+#         if alarm_note:
+#             return_msg += alarm_note
+#         return return_msg
+#
+#     def lock_vehicle(self):
+#         if self.is_locked:
+#             return 'Locked'
+#         if not self.bonnet_open and not self.trunk_open and len(list(set(self.doors_opened))) == 1 and self.doors_opened[0] is False:
+#             self.is_locked = True
+#             self.locked_time = self.timer
+#             return 'Locked'
+#         return 'Cannot lock'
+#
+#     def unlock_vehicle(self):
+#         self.is_locked = False
+#         self.alarm_active = False
+#         self.locked_time = None
+#         return 'Unlocked'
+#
+#     def wait(self, time_to_wait):
+#         self.timer += time_to_wait
+#         self.timer = min(self.timer, self.max_wait_time)
+#         if self.locked_time and abs(self.timer - self.locked_time) >= 20:
+#             self.alarm_active = True
+#             self.alarm_activation_time = self.timer
+#         if self.alarm_active:
+#             return 'Alarm Activated'
+#         return 'Waiting'
+#
+#     def reset(self):
+#         self.timer = 0
+#         self.bonnet_open = False
+#         self.doors_opened = [False, False, False, False]
+#         # Is alarm active
+#         self.alarm_active = False
+#         # Is alarm triggered
+#         self.alarm_triggered = False
+#         # Is car locked
+#         self.is_locked = False
+#         # Time at which car was locked and alarm activated
+#         self.locked_time = None
+#         self.alarm_activation_time = None
+#
+#
+# if __name__ == '__main__':
+#     from aalpy.learning_algs import run_Lstar
+#     from aalpy.oracles import RandomWMethodEqOracle
+#     from aalpy.SULs import FunctionDecorator, PyClassSUL
+#     # class under learning (do not instantiate it)
+#     car_alarm_class = CarAlarmSystem
+#
+#     # methods weapped in the function decorators
+#     input_al = [FunctionDecorator(car_alarm_class.open_trunk), FunctionDecorator(car_alarm_class.close_trunk),
+#                 FunctionDecorator(car_alarm_class.open_bonnet), FunctionDecorator(car_alarm_class.close_bonnet),
+#                 FunctionDecorator(car_alarm_class.lock_vehicle), FunctionDecorator(car_alarm_class.unlock_vehicle),
+#
+#                 FunctionDecorator(car_alarm_class.open_door, 1), FunctionDecorator(car_alarm_class.close_door, 1),
+#                 FunctionDecorator(car_alarm_class.open_door, 2), FunctionDecorator(car_alarm_class.close_door, 2),
+#                 FunctionDecorator(car_alarm_class.open_door, 3), FunctionDecorator(car_alarm_class.close_door, 3),
+#                 FunctionDecorator(car_alarm_class.open_door, 4), FunctionDecorator(car_alarm_class.close_door, 4),
+#
+#                 FunctionDecorator(car_alarm_class.wait, 10),
+#                 FunctionDecorator(car_alarm_class.wait, 20),
+#                 FunctionDecorator(car_alarm_class.wait, 200),
+#                 ]
+#
+#     sul = PyClassSUL(car_alarm_class)
+#
+#     eq_oracle = RandomWMethodEqOracle(input_al, sul, walks_per_state=100, walk_len=10)
+#
+#     learned_model = run_Lstar(input_al, sul, eq_oracle=eq_oracle, automaton_type='mealy', cache_and_non_det_check=True)
+#     print(learned_model)
```

## aalpy/utils/DataHandler.py

 * *Ordering differences only*

```diff
@@ -1,73 +1,73 @@
-from abc import ABC, abstractmethod
-
-
-class DataHandler(ABC):
-    """
-    Abstract class used for data loading for Alergia algorithm. Usage of class is not needed, but recommended for
-    consistency.
-    """
-
-    @abstractmethod
-    def tokenize_data(self, path):
-        pass
-
-
-class CharacterTokenizer(DataHandler):
-    """
-    Used for Markov Chain data parsing.
-    Processes data where each input is a single character.
-    Each input sequence is in the separate line.
-    """
-
-    def tokenize_data(self, path):
-        data = []
-        lines = open(path).read().splitlines()
-        for l in lines:
-            data.append(list(l))
-        return data
-
-
-class DelimiterTokenizer(DataHandler):
-    """
-    Used for Markov Chain data parsing.
-    Processes data where each input is separated by the delimiter.
-    Each input sequence is in the separate line.
-    """
-
-    def tokenize_data(self, path, delimiter=','):
-        data = []
-        lines = open(path).read().splitlines()
-        for l in lines:
-            data.append(l.split(delimiter))
-        return data
-
-
-class IODelimiterTokenizer(DataHandler):
-    """
-    Used for Markov Decision Process data parsing.
-    Processes data where each input/output is separated by the io_delimiter, and i/o pairs are separated
-    by word delimiter.
-    Each [output, tuple(input,output)*] sequence is in the separate line.
-    """
-
-    def tokenize_data(self, path, io_delimiter='/', word_delimiter=','):
-        data = []
-        lines = open(path).read().splitlines()
-        for l in lines:
-            words = l.split(word_delimiter)
-            seq = [words[0]]
-            for w in words[1:]:
-                i_o = w.split(io_delimiter)
-                if len(i_o) != 2:
-                    print('Data formatting error. io_delimiter should split words into <input> <delim> <output>'
-                          'where <delim> is values of param \"io_delimiter\'"')
-                    exit(-1)
-                seq.append(tuple([try_int(i_o[0]), try_int(i_o[1])]))
-            data.append(seq)
-        return data
-
-
-def try_int(x):
-    if str.isdigit(x):
-        return int(x)
-    return x
+from abc import ABC, abstractmethod
+
+
+class DataHandler(ABC):
+    """
+    Abstract class used for data loading for Alergia algorithm. Usage of class is not needed, but recommended for
+    consistency.
+    """
+
+    @abstractmethod
+    def tokenize_data(self, path):
+        pass
+
+
+class CharacterTokenizer(DataHandler):
+    """
+    Used for Markov Chain data parsing.
+    Processes data where each input is a single character.
+    Each input sequence is in the separate line.
+    """
+
+    def tokenize_data(self, path):
+        data = []
+        lines = open(path).read().splitlines()
+        for l in lines:
+            data.append(list(l))
+        return data
+
+
+class DelimiterTokenizer(DataHandler):
+    """
+    Used for Markov Chain data parsing.
+    Processes data where each input is separated by the delimiter.
+    Each input sequence is in the separate line.
+    """
+
+    def tokenize_data(self, path, delimiter=','):
+        data = []
+        lines = open(path).read().splitlines()
+        for l in lines:
+            data.append(l.split(delimiter))
+        return data
+
+
+class IODelimiterTokenizer(DataHandler):
+    """
+    Used for Markov Decision Process data parsing.
+    Processes data where each input/output is separated by the io_delimiter, and i/o pairs are separated
+    by word delimiter.
+    Each [output, tuple(input,output)*] sequence is in the separate line.
+    """
+
+    def tokenize_data(self, path, io_delimiter='/', word_delimiter=','):
+        data = []
+        lines = open(path).read().splitlines()
+        for l in lines:
+            words = l.split(word_delimiter)
+            seq = [words[0]]
+            for w in words[1:]:
+                i_o = w.split(io_delimiter)
+                if len(i_o) != 2:
+                    print('Data formatting error. io_delimiter should split words into <input> <delim> <output>'
+                          'where <delim> is values of param \"io_delimiter\'"')
+                    exit(-1)
+                seq.append(tuple([try_int(i_o[0]), try_int(i_o[1])]))
+            data.append(seq)
+        return data
+
+
+def try_int(x):
+    if str.isdigit(x):
+        return int(x)
+    return x
```

## aalpy/utils/FileHandler.py

```diff
@@ -1,301 +1,301 @@
-import os
-import sys
-import traceback
-from pathlib import Path
-
-from pydot import Dot, Node, Edge, graph_from_dot_file
-
-from aalpy.automata import Dfa, MooreMachine, Mdp, Onfsm, MealyState, DfaState, MooreState, MealyMachine, \
-    MdpState, StochasticMealyMachine, StochasticMealyState, OnfsmState, MarkovChain, McState
-
-file_types = ['dot', 'png', 'svg', 'pdf', 'string']
-automaton_types = {Dfa: 'dfa', MealyMachine: 'mealy', MooreMachine: 'moore', Mdp: 'mdp',
-                   StochasticMealyMachine: 'smm', Onfsm: 'onfsm', MarkovChain: 'mc'}
-
-
-def _wrap_label(label):
-    """
-    Adds a " " around a label if not already present on both ends.
-    """
-    if label[0] == '\"' and label[-1] == '\"':
-        return label
-    return f'\"{label}\"'
-
-
-def _get_node(state, automaton_type):
-    if automaton_type == 'dfa':
-        if state.is_accepting:
-            return Node(state.state_id, label=_wrap_label(state.state_id), shape='doublecircle')
-        return Node(state.state_id, label=_wrap_label(state.state_id))
-    if automaton_type == 'mealy':
-        return Node(state.state_id, label=_wrap_label(state.state_id))
-    if automaton_type == 'moore':
-        return Node(state.state_id, label=_wrap_label(f'{state.state_id}|{state.output}'), shape='record', style='rounded')
-    if automaton_type == 'onfsm':
-        return Node(state.state_id, label=_wrap_label(state.state_id))
-    if automaton_type == 'mc':
-        return Node(state.state_id, label=_wrap_label(f'{state.output}'))
-    if automaton_type == 'mdp':
-        return Node(state.state_id, label=_wrap_label(f'{state.output}'))
-    if automaton_type == 'smm':
-        return Node(state.state_id, label=_wrap_label(state.state_id))
-
-
-def _add_transition_to_graph(graph, state, automaton_type, display_same_state_trans, round_floats):
-    if automaton_type == 'dfa' or automaton_type == 'moore':
-        for i in state.transitions.keys():
-            new_state = state.transitions[i]
-            if not display_same_state_trans and new_state.state_id == state.state_id:
-                continue
-            graph.add_edge(Edge(state.state_id, new_state.state_id, label=_wrap_label(f'{i}')))
-    if automaton_type == 'mealy':
-        for i in state.transitions.keys():
-            new_state = state.transitions[i]
-            if not display_same_state_trans and new_state.state_id == state.state_id:
-                continue
-            graph.add_edge(Edge(state.state_id, new_state.state_id, label=_wrap_label(f'{i}/{state.output_fun[i]}')))
-    if automaton_type == 'onfsm':
-        for i in state.transitions.keys():
-            new_state = state.transitions[i]
-            for s in new_state:
-                if not display_same_state_trans and state.state_id == s[1].state_id:
-                    continue
-                graph.add_edge(Edge(state.state_id, s[1].state_id, label=_wrap_label(f'{i}/{s[0]}')))
-    if automaton_type == 'mc':
-        for new_state, prob in state.transitions:
-            prob = round(prob, round_floats) if round_floats else prob
-            graph.add_edge(Edge(state.state_id, new_state.state_id, label=f'{prob}'))
-    if automaton_type == 'mdp':
-        for i in state.transitions.keys():
-            new_state = state.transitions[i]
-            for s in new_state:
-                if not display_same_state_trans and s[0].state_id == state.state_id:
-                    continue
-                prob = round(s[1], round_floats) if round_floats else s[1]
-                graph.add_edge(Edge(state.state_id, s[0].state_id, label=_wrap_label(f'{i}:{prob}')))
-    if automaton_type == 'smm':
-        for i in state.transitions.keys():
-            new_state = state.transitions[i]
-            for s in new_state:
-                if not display_same_state_trans and s[0].state_id == state.state_id:
-                    continue
-                prob = round(s[2], round_floats) if round_floats else s[2]
-                graph.add_edge(Edge(state.state_id, s[0].state_id, label=_wrap_label(f'{i}/{s[1]}:{prob}')))
-
-
-def visualize_automaton(automaton, path="LearnedModel", file_type="pdf", display_same_state_trans=True):
-    """
-    Create a graphical representation of the automaton.
-    Function is round in the separate thread in the background.
-    If possible, it will be opened by systems default program.
-
-    Args:
-
-        automaton: automaton to be visualized
-
-        path: pathlike or str, file in which visualization will be saved (Default value = "LearnedModel.pdf")
-
-        file_type: type of file/visualization. Can be ['png', 'svg', 'pdf'] (Default value = "pdf")
-
-        display_same_state_trans: if True, same state transitions will be displayed (Default value = True)
-
-    """
-    print('Visualization started in the background thread.')
-
-    if len(automaton.states) >= 25:
-        print(f'Visualizing {len(automaton.states)} state automaton could take some time.')
-
-    import threading
-    visualization_thread = threading.Thread(target=save_automaton_to_file, name="Visualization",
-                                            args=(automaton, path, file_type, display_same_state_trans, True, 2))
-    visualization_thread.start()
-
-
-def save_automaton_to_file(automaton, path="LearnedModel", file_type="dot",
-                           display_same_state_trans=True, visualize=False, round_floats=None):
-    """
-    The Standard of the automata strictly follows the syntax found at: https://automata.cs.ru.nl/Syntax/Overview.
-    For non-deterministic and stochastic systems syntax can be found on AALpy's Wiki.
-
-    Args:
-
-        automaton: automaton to be saved to file
-
-        path: pathlike or str, file in which visualization will be saved (Default value = "LearnedModel")
-
-        file_type: type of file/visualization. Can be ['dot', 'png', 'svg', 'pdf'] (Default value = "dot)
-
-        display_same_state_trans: True, should not be set to false except from the visualization method
-            (Default value = True)
-
-        visualize: visualize the automaton
-
-        round_floats: for stochastic automata, round the floating point numbers to defined number of decimal places
-
-    Returns:
-
-    """
-    path = Path(path)
-    file_type = file_type.lower()
-    assert file_type in file_types, f"Filetype {file_type} not in allowed filetypes"
-    path = path.with_suffix(f".{file_type}")
-    if file_type == 'dot' and not display_same_state_trans:
-        print("When saving to file all transitions will be saved")
-        display_same_state_trans = True
-    automaton_type = automaton_types[automaton.__class__]
-
-    graph = Dot(path.stem, graph_type='digraph')
-    for state in automaton.states:
-        graph.add_node(_get_node(state, automaton_type))
-
-    for state in automaton.states:
-        _add_transition_to_graph(graph, state, automaton_type, display_same_state_trans, round_floats)
-
-    # add initial node
-    graph.add_node(Node('__start0', shape='none', label=''))
-    graph.add_edge(Edge('__start0', automaton.initial_state.state_id, label=''))
-
-    if file_type == 'string':
-        return graph.to_string()
-    else:
-        try:
-            graph.write(path=path, format=file_type if file_type != 'dot' else 'raw')
-            print(f'Model saved to {path.as_posix()}.')
-
-            if visualize and file_type in {'pdf', 'png', 'svg'}:
-                try:
-                    import webbrowser
-                    webbrowser.open(path.resolve().as_uri())
-                except OSError:
-                    traceback.print_exc()
-                    print(f'Could not open the file {path.as_posix()}.', file=sys.stderr)
-        except OSError:
-            traceback.print_exc()
-            print(f'Could not write to the file {path.as_posix()}.', file=sys.stderr)
-
-
-def _process_label(label, source, destination, automaton_type):
-    if automaton_type == 'dfa' or automaton_type == 'moore':
-        source.transitions[int(label) if label.isdigit() else label] = destination
-    if automaton_type == 'mealy':
-        inp, out = label.split('/', maxsplit=1)
-        inp = int(inp) if inp.isdigit() else inp
-        out = int(out) if out.isdigit() else out
-        source.transitions[inp] = destination
-        source.output_fun[inp] = out
-    if automaton_type == 'onfsm':
-        inp, out = label.split('/', maxsplit=1)
-        inp = int(inp) if inp.isdigit() else inp
-        out = int(out) if out.isdigit() else out
-        source.transitions[inp].append((out, destination))
-    if automaton_type == 'mc':
-        prob = label
-        source.transitions.append((destination, float(prob)))
-    if automaton_type == 'mdp':
-        inp, prob = label.split(':', maxsplit=1)
-        inp = int(inp) if inp.isdigit() else inp
-        prob = float(prob)
-        source.transitions[inp].append((destination, prob))
-    if automaton_type == 'smm':
-        inp, out_prob = label.split('/', maxsplit=1)
-        out, prob = out_prob.split(':', maxsplit=1)
-        inp = int(inp) if inp.isdigit() else inp
-        out = int(out) if out.isdigit() else out
-        source.transitions[inp].append((destination, out, float(prob)))
-
-
-def _process_node_label(node, label, node_label_dict, node_type, automaton_type):
-    node_name = node.get_name()
-    if automaton_type == 'mdp' or automaton_type == 'mc':
-        node_label_dict[node_name] = node_type(node_name, label)
-    else:
-        if automaton_type == 'moore' and label != "":
-            label_output = _strip_label(label)
-            label, output = label_output.split("|", maxsplit=1)
-            output = output if not output.isdigit() else int(output)
-            node_label_dict[node_name] = node_type(label, output)
-        else:
-            node_label_dict[node_name] = node_type(label)
-        if automaton_type == 'dfa':
-            if 'shape' in node.get_attributes().keys() and 'doublecircle' in node.get_attributes()['shape']:
-                node_label_dict[node_name].is_accepting = True
-
-
-def _strip_label(label: str) -> str:
-    label = label.strip()
-    if label[0] == '\"' and label[-1] == label[0]:
-        label = label[1:-1]
-    if label[0] == '{' and label[-1] == '}':
-        label = label[1:-1]
-    label = label.replace(" ", "")
-    return label
-
-
-def load_automaton_from_file(path, automaton_type, compute_prefixes=False):
-    """
-    Loads the automaton from the file.
-    Standard of the automatas strictly follows syntax found at: https://automata.cs.ru.nl/Syntax/Overview.
-    For non-deterministic and stochastic systems syntax can be found on AALpy's Wiki.
-
-    Args:
-
-        path: pathlike or str to the file
-
-        automaton_type: type of the automaton, if not specified it will be automatically determined according,
-            one of ['dfa', 'mealy', 'moore', 'mdp', 'smm', 'onfsm', 'mc']
-
-        compute_prefixes: it True, shortest path to reach every state will be computed and saved in the prefix of
-            the state. Useful when loading the model to use them as a equivalence oracle. (Default value = False)
-
-    Returns:
-      automaton
-
-    """
-    path = Path(path)
-    graph = graph_from_dot_file(path)[0]
-
-    assert automaton_type in automaton_types.values()
-
-    id_node_aut_map = {'dfa': (DfaState, Dfa), 'mealy': (MealyState, MealyMachine), 'moore': (MooreState, MooreMachine),
-                       'onfsm': (OnfsmState, Onfsm), 'mdp': (MdpState, Mdp), 'mc': (McState, MarkovChain),
-                       'smm': (StochasticMealyState, StochasticMealyMachine)}
-
-    nodeType, aut_type = id_node_aut_map[automaton_type]
-
-    node_label_dict = dict()
-    for n in graph.get_node_list():
-        if n.get_name() == '__start0' or n.get_name() == '' or n.get_name() == '"\\n"':
-            continue
-        label = None
-        if 'label' in n.get_attributes().keys():
-            label = n.get_attributes()['label']
-            label = _strip_label(label)
-
-        _process_node_label(n, label, node_label_dict, nodeType, automaton_type)
-
-    initial_node = None
-    for edge in graph.get_edge_list():
-        if edge.get_source() == '__start0':
-            initial_node = node_label_dict[edge.get_destination()]
-            continue
-
-        source = node_label_dict[edge.get_source()]
-        destination = node_label_dict[edge.get_destination()]
-
-        label = edge.get_attributes()['label']
-        label = _strip_label(label)
-        _process_label(label, source, destination, automaton_type)
-
-    if initial_node is None:
-        print("No initial state found. \n"
-              "Please follow syntax found at: https://github.com/DES-Lab/AALpy/wiki/"
-              "Loading,Saving,-Syntax-and-Visualization-of-Automata ")
-        assert False
-
-    automaton = aut_type(initial_node, list(node_label_dict.values()))
-    if automaton_type != 'mc' and not automaton.is_input_complete():
-        print('Warning: Loaded automaton is not input complete.')
-    if compute_prefixes and not automaton_type == 'mc':
-        for state in automaton.states:
-            state.prefix = automaton.get_shortest_path(automaton.initial_state, state)
-    return automaton
+import os
+import sys
+import traceback
+from pathlib import Path
+
+from pydot import Dot, Node, Edge, graph_from_dot_file
+
+from aalpy.automata import Dfa, MooreMachine, Mdp, Onfsm, MealyState, DfaState, MooreState, MealyMachine, \
+    MdpState, StochasticMealyMachine, StochasticMealyState, OnfsmState, MarkovChain, McState
+
+file_types = ['dot', 'png', 'svg', 'pdf', 'string']
+automaton_types = {Dfa: 'dfa', MealyMachine: 'mealy', MooreMachine: 'moore', Mdp: 'mdp',
+                   StochasticMealyMachine: 'smm', Onfsm: 'onfsm', MarkovChain: 'mc'}
+
+
+def _wrap_label(label):
+    """
+    Adds a " " around a label if not already present on both ends.
+    """
+    if label[0] == '\"' and label[-1] == '\"':
+        return label
+    return f'\"{label}\"'
+
+
+def _get_node(state, automaton_type):
+    if automaton_type == 'dfa':
+        if state.is_accepting:
+            return Node(state.state_id, label=_wrap_label(state.state_id), shape='doublecircle')
+        return Node(state.state_id, label=_wrap_label(state.state_id))
+    if automaton_type == 'mealy':
+        return Node(state.state_id, label=_wrap_label(state.state_id))
+    if automaton_type == 'moore':
+        return Node(state.state_id, label=_wrap_label(f'{state.state_id}|{state.output}'), shape='record', style='rounded')
+    if automaton_type == 'onfsm':
+        return Node(state.state_id, label=_wrap_label(state.state_id))
+    if automaton_type == 'mc':
+        return Node(state.state_id, label=_wrap_label(f'{state.output}'))
+    if automaton_type == 'mdp':
+        return Node(state.state_id, label=_wrap_label(f'{state.output}'))
+    if automaton_type == 'smm':
+        return Node(state.state_id, label=_wrap_label(state.state_id))
+
+
+def _add_transition_to_graph(graph, state, automaton_type, display_same_state_trans, round_floats):
+    if automaton_type == 'dfa' or automaton_type == 'moore':
+        for i in state.transitions.keys():
+            new_state = state.transitions[i]
+            if not display_same_state_trans and new_state.state_id == state.state_id:
+                continue
+            graph.add_edge(Edge(state.state_id, new_state.state_id, label=_wrap_label(f'{i}')))
+    if automaton_type == 'mealy':
+        for i in state.transitions.keys():
+            new_state = state.transitions[i]
+            if not display_same_state_trans and new_state.state_id == state.state_id:
+                continue
+            graph.add_edge(Edge(state.state_id, new_state.state_id, label=_wrap_label(f'{i}/{state.output_fun[i]}')))
+    if automaton_type == 'onfsm':
+        for i in state.transitions.keys():
+            new_state = state.transitions[i]
+            for s in new_state:
+                if not display_same_state_trans and state.state_id == s[1].state_id:
+                    continue
+                graph.add_edge(Edge(state.state_id, s[1].state_id, label=_wrap_label(f'{i}/{s[0]}')))
+    if automaton_type == 'mc':
+        for new_state, prob in state.transitions:
+            prob = round(prob, round_floats) if round_floats else prob
+            graph.add_edge(Edge(state.state_id, new_state.state_id, label=f'{prob}'))
+    if automaton_type == 'mdp':
+        for i in state.transitions.keys():
+            new_state = state.transitions[i]
+            for s in new_state:
+                if not display_same_state_trans and s[0].state_id == state.state_id:
+                    continue
+                prob = round(s[1], round_floats) if round_floats else s[1]
+                graph.add_edge(Edge(state.state_id, s[0].state_id, label=_wrap_label(f'{i}:{prob}')))
+    if automaton_type == 'smm':
+        for i in state.transitions.keys():
+            new_state = state.transitions[i]
+            for s in new_state:
+                if not display_same_state_trans and s[0].state_id == state.state_id:
+                    continue
+                prob = round(s[2], round_floats) if round_floats else s[2]
+                graph.add_edge(Edge(state.state_id, s[0].state_id, label=_wrap_label(f'{i}/{s[1]}:{prob}')))
+
+
+def visualize_automaton(automaton, path="LearnedModel", file_type="pdf", display_same_state_trans=True):
+    """
+    Create a graphical representation of the automaton.
+    Function is round in the separate thread in the background.
+    If possible, it will be opened by systems default program.
+
+    Args:
+
+        automaton: automaton to be visualized
+
+        path: pathlike or str, file in which visualization will be saved (Default value = "LearnedModel.pdf")
+
+        file_type: type of file/visualization. Can be ['png', 'svg', 'pdf'] (Default value = "pdf")
+
+        display_same_state_trans: if True, same state transitions will be displayed (Default value = True)
+
+    """
+    print('Visualization started in the background thread.')
+
+    if len(automaton.states) >= 25:
+        print(f'Visualizing {len(automaton.states)} state automaton could take some time.')
+
+    import threading
+    visualization_thread = threading.Thread(target=save_automaton_to_file, name="Visualization",
+                                            args=(automaton, path, file_type, display_same_state_trans, True, 2))
+    visualization_thread.start()
+
+
+def save_automaton_to_file(automaton, path="LearnedModel", file_type="dot",
+                           display_same_state_trans=True, visualize=False, round_floats=None):
+    """
+    The Standard of the automata strictly follows the syntax found at: https://automata.cs.ru.nl/Syntax/Overview.
+    For non-deterministic and stochastic systems syntax can be found on AALpy's Wiki.
+
+    Args:
+
+        automaton: automaton to be saved to file
+
+        path: pathlike or str, file in which visualization will be saved (Default value = "LearnedModel")
+
+        file_type: type of file/visualization. Can be ['dot', 'png', 'svg', 'pdf'] (Default value = "dot)
+
+        display_same_state_trans: True, should not be set to false except from the visualization method
+            (Default value = True)
+
+        visualize: visualize the automaton
+
+        round_floats: for stochastic automata, round the floating point numbers to defined number of decimal places
+
+    Returns:
+
+    """
+    path = Path(path)
+    file_type = file_type.lower()
+    assert file_type in file_types, f"Filetype {file_type} not in allowed filetypes"
+    path = path.with_suffix(f".{file_type}")
+    if file_type == 'dot' and not display_same_state_trans:
+        print("When saving to file all transitions will be saved")
+        display_same_state_trans = True
+    automaton_type = automaton_types[automaton.__class__]
+
+    graph = Dot(path.stem, graph_type='digraph')
+    for state in automaton.states:
+        graph.add_node(_get_node(state, automaton_type))
+
+    for state in automaton.states:
+        _add_transition_to_graph(graph, state, automaton_type, display_same_state_trans, round_floats)
+
+    # add initial node
+    graph.add_node(Node('__start0', shape='none', label=''))
+    graph.add_edge(Edge('__start0', automaton.initial_state.state_id, label=''))
+
+    if file_type == 'string':
+        return graph.to_string()
+    else:
+        try:
+            graph.write(path=path, format=file_type if file_type != 'dot' else 'raw')
+            print(f'Model saved to {path.as_posix()}.')
+
+            if visualize and file_type in {'pdf', 'png', 'svg'}:
+                try:
+                    import webbrowser
+                    webbrowser.open(path.resolve().as_uri())
+                except OSError:
+                    traceback.print_exc()
+                    print(f'Could not open the file {path.as_posix()}.', file=sys.stderr)
+        except OSError:
+            traceback.print_exc()
+            print(f'Could not write to the file {path.as_posix()}.', file=sys.stderr)
+
+
+def _process_label(label, source, destination, automaton_type):
+    if automaton_type == 'dfa' or automaton_type == 'moore':
+        source.transitions[int(label) if label.isdigit() else label] = destination
+    if automaton_type == 'mealy':
+        inp, out = label.split('/', maxsplit=1)
+        inp = int(inp) if inp.isdigit() else inp
+        out = int(out) if out.isdigit() else out
+        source.transitions[inp] = destination
+        source.output_fun[inp] = out
+    if automaton_type == 'onfsm':
+        inp, out = label.split('/', maxsplit=1)
+        inp = int(inp) if inp.isdigit() else inp
+        out = int(out) if out.isdigit() else out
+        source.transitions[inp].append((out, destination))
+    if automaton_type == 'mc':
+        prob = label
+        source.transitions.append((destination, float(prob)))
+    if automaton_type == 'mdp':
+        inp, prob = label.split(':', maxsplit=1)
+        inp = int(inp) if inp.isdigit() else inp
+        prob = float(prob)
+        source.transitions[inp].append((destination, prob))
+    if automaton_type == 'smm':
+        inp, out_prob = label.split('/', maxsplit=1)
+        out, prob = out_prob.split(':', maxsplit=1)
+        inp = int(inp) if inp.isdigit() else inp
+        out = int(out) if out.isdigit() else out
+        source.transitions[inp].append((destination, out, float(prob)))
+
+
+def _process_node_label(node, label, node_label_dict, node_type, automaton_type):
+    node_name = node.get_name()
+    if automaton_type == 'mdp' or automaton_type == 'mc':
+        node_label_dict[node_name] = node_type(node_name, label)
+    else:
+        if automaton_type == 'moore' and label != "":
+            label_output = _strip_label(label)
+            label, output = label_output.split("|", maxsplit=1)
+            output = output if not output.isdigit() else int(output)
+            node_label_dict[node_name] = node_type(label, output)
+        else:
+            node_label_dict[node_name] = node_type(label)
+        if automaton_type == 'dfa':
+            if 'shape' in node.get_attributes().keys() and 'doublecircle' in node.get_attributes()['shape']:
+                node_label_dict[node_name].is_accepting = True
+
+
+def _strip_label(label: str) -> str:
+    label = label.strip()
+    if label[0] == '\"' and label[-1] == label[0]:
+        label = label[1:-1]
+    if label[0] == '{' and label[-1] == '}':
+        label = label[1:-1]
+    # label = label.replace(" ", "")
+    return label
+
+
+def load_automaton_from_file(path, automaton_type, compute_prefixes=False):
+    """
+    Loads the automaton from the file.
+    Standard of the automatas strictly follows syntax found at: https://automata.cs.ru.nl/Syntax/Overview.
+    For non-deterministic and stochastic systems syntax can be found on AALpy's Wiki.
+
+    Args:
+
+        path: pathlike or str to the file
+
+        automaton_type: type of the automaton, if not specified it will be automatically determined according,
+            one of ['dfa', 'mealy', 'moore', 'mdp', 'smm', 'onfsm', 'mc']
+
+        compute_prefixes: it True, shortest path to reach every state will be computed and saved in the prefix of
+            the state. Useful when loading the model to use them as a equivalence oracle. (Default value = False)
+
+    Returns:
+      automaton
+
+    """
+    path = Path(path)
+    graph = graph_from_dot_file(path)[0]
+
+    assert automaton_type in automaton_types.values()
+
+    id_node_aut_map = {'dfa': (DfaState, Dfa), 'mealy': (MealyState, MealyMachine), 'moore': (MooreState, MooreMachine),
+                       'onfsm': (OnfsmState, Onfsm), 'mdp': (MdpState, Mdp), 'mc': (McState, MarkovChain),
+                       'smm': (StochasticMealyState, StochasticMealyMachine)}
+
+    nodeType, aut_type = id_node_aut_map[automaton_type]
+
+    node_label_dict = dict()
+    for n in graph.get_node_list():
+        if n.get_name() == '__start0' or n.get_name() == '' or n.get_name() == '"\\n"':
+            continue
+        label = None
+        if 'label' in n.get_attributes().keys():
+            label = n.get_attributes()['label']
+            label = _strip_label(label)
+
+        _process_node_label(n, label, node_label_dict, nodeType, automaton_type)
+
+    initial_node = None
+    for edge in graph.get_edge_list():
+        if edge.get_source() == '__start0':
+            initial_node = node_label_dict[edge.get_destination()]
+            continue
+
+        source = node_label_dict[edge.get_source()]
+        destination = node_label_dict[edge.get_destination()]
+
+        label = edge.get_attributes()['label']
+        label = _strip_label(label)
+        _process_label(label, source, destination, automaton_type)
+
+    if initial_node is None:
+        print("No initial state found. \n"
+              "Please follow syntax found at: https://github.com/DES-Lab/AALpy/wiki/"
+              "Loading,Saving,-Syntax-and-Visualization-of-Automata ")
+        assert False
+
+    automaton = aut_type(initial_node, list(node_label_dict.values()))
+    if automaton_type != 'mc' and not automaton.is_input_complete():
+        print('Warning: Loaded automaton is not input complete.')
+    if compute_prefixes and not automaton_type == 'mc':
+        for state in automaton.states:
+            state.prefix = automaton.get_shortest_path(automaton.initial_state, state)
+    return automaton
```

## aalpy/utils/HelperFunctions.py

 * *Ordering differences only*

```diff
@@ -1,298 +1,298 @@
-import string
-from collections import defaultdict
-
-
-def extend_set(list_to_extend: list, new_elements: list) -> list:
-    """
-    Helper function to extend a list while maintaining set property.
-    They are stored as lists, so with this function set property is maintained.
-    :return
-
-    Returns:
-
-        list of elements that were added to the set
-
-    """
-    set_repr = set(list_to_extend)
-    added_elements = [s for s in new_elements if s not in set_repr]
-    list_to_extend.extend(added_elements)
-    return added_elements
-
-
-def all_prefixes(li):
-    """
-    Returns all prefixes of a list.
-
-    Args:
-      li: list from which to compute all prefixes
-
-    Returns:
-      list of all prefixes
-
-    """
-    return [tuple(li[:i + 1]) for i in range(len(li))]
-
-
-def all_suffixes(li):
-    """
-    Returns all suffixes of a list.
-
-    Args:
-      li: list from which to compute all suffixes
-
-    Returns:
-      list of all suffixes
-
-    """
-    return [tuple(li[len(li) - i - 1:]) for i in range(len(li))]
-
-
-def profile_function(function: callable, sort_key='cumtime'):
-    """
-
-    Args:
-      function: callable: 
-      sort_key:  (Default value = 'cumtime')
-
-    Returns:
-        prints the profiling results
-    """
-    import cProfile
-    pr = cProfile.Profile()
-    pr.enable()
-    function()
-    pr.disable()
-    pr.print_stats(sort=sort_key)
-
-
-def random_string_generator(size=10, chars=string.ascii_lowercase + string.digits):
-    """
-
-    Args:
-
-      size:  (Default value = 10)
-      chars:  (Default value = string.ascii_lowercase + string.digits)
-
-    Returns:
-
-        a random string of length size
-    """
-    import random
-    return ''.join(random.choice(chars) for _ in range(size))
-
-
-def print_learning_info(info: dict):
-    """
-    Print learning statistics.
-    """
-    print('-----------------------------------')
-    print('Learning Finished.')
-    print('Learning Rounds:  {}'.format(info['learning_rounds']))
-    print('Number of states: {}'.format(info['automaton_size']))
-    print('Time (in seconds)')
-    print('  Total                : {}'.format(info['total_time']))
-    print('  Learning algorithm   : {}'.format(info['learning_time']))
-    print('  Conformance checking : {}'.format(info['eq_oracle_time']))
-    print('Learning Algorithm')
-    print(' # Membership Queries  : {}'.format(info['queries_learning']))
-    if 'cache_saved' in info.keys():
-        print(' # MQ Saved by Caching : {}'.format(info['cache_saved']))
-    print(' # Steps               : {}'.format(info['steps_learning']))
-    print('Equivalence Query')
-    print(' # Membership Queries  : {}'.format(info['queries_eq_oracle']))
-    print(' # Steps               : {}'.format(info['steps_eq_oracle']))
-    print('-----------------------------------')
-
-
-def print_observation_table(ot, table_type):
-    """
-    Prints the whole observation table.
-
-    Args:
-
-        ot: observation table
-        table_type: 'det', 'non-det', or 'stoc'
-
-    """
-    if table_type == 'det':
-        s_set, extended_s, e_set, table = ot.S, ot.s_dot_a(), ot.E, ot.T
-    elif table_type == 'non-det':
-        s_set, extended_s, e_set = ot.S, ot.get_extended_S(), ot.E
-        table = ot.sul.cache.get_table(s_set + extended_s, e_set)
-    elif table_type == 'abstracted-non-det':
-        s_set, extended_s, e_set, table = ot.S, ot.S_dot_A, ot.E, ot.T
-    else:
-        s_set, extended_s, e_set, table = ot.S, ot.get_extended_s(), ot.E, ot.T
-
-    headers = [str(e) for e in e_set]
-    s_rows = []
-    extended_rows = []
-    headers.insert(0, 'Prefixes / E set')
-    for s in s_set:
-        row = [str(s)]
-        if table_type == 'det':
-            row.extend(str(e) for e in table[s])
-        else:
-            row.extend(str(table[s][e]) for e in e_set)
-        s_rows.append(row)
-    for s in extended_s:
-        row = [str(s)]
-        if table_type == 'det':
-            row.extend(str(e) for e in table[s])
-        else:
-            row.extend(str(table[s][e]) for e in e_set)
-        extended_rows.append(row)
-
-    table = [headers] + s_rows
-    columns = defaultdict(int)
-    for i in table + extended_rows:
-        for index, el in enumerate(i):
-            columns[index] = max(columns[index], len(el))
-
-    row_len = 0
-    for row in table:
-        row = "|".join(element.ljust(columns[ind] + 1) for ind, element in enumerate(row))
-        print("-" * len(row))
-        row_len = len(row)
-        print(row)
-    print('=' * row_len)
-    for row in extended_rows:
-        row = "|".join(element.ljust(columns[ind] + 1) for ind, element in enumerate(row))
-        print("-" * len(row))
-        print(row)
-    print('-' * row_len)
-
-
-def is_suffix_of(suffix, trace) -> bool:
-    """
-
-    Args:
-      suffix: target suffix
-      trace: trace in question
-
-    Returns:
-
-        True if suffix is the suffix of trace.
-    """
-    if len(trace) < len(suffix):
-        return False
-    else:
-        return trace[-len(suffix):] == suffix
-
-
-def get_cex_prefixes(cex, automaton_type):
-    """
-    Returns all prefixes of the stochastic automaton.
-
-    Args:
-        cex: counterexample
-        automaton_type: `mdp` or `smm`
-
-    Returns:
-
-        all prefixes of the counterexample based on the `automaton_type`
-    """
-    if automaton_type == 'mdp':
-        return [tuple(cex[:i + 1]) for i in range(0, len(cex), 2)]
-    return [tuple(cex[:i]) for i in range(0, len(cex) + 1, 2)]
-
-
-def get_available_oracles_and_err_msg():
-    from aalpy.oracles import RandomWalkEqOracle
-    from aalpy.oracles import RandomWordEqOracle
-    available_oracles = {RandomWalkEqOracle, RandomWordEqOracle}
-
-    available_oracles_msg = 'Warning! Only Random Walk and Random Word oracles are supported for non-deterministic and ' \
-                            'stochastic learning. If you have implemented the custom oracle, set the custom_oracle ' \
-                            'flag to True. '
-
-    return available_oracles, available_oracles_msg
-
-
-def make_input_complete(automaton, missing_transition_go_to='self_loop'):
-    """
-    Makes the automaton input complete/enabled. If a input is not defined in a state, it will lead to the self loop.
-    In case of Mealy Machines, Stochastic Mealy machines and ONFSM 'epsilon' is used as output.
-    Self loop simply loops the transition back to state which was not input complete,
-    whereas 'sink_state' leads all transitions to a newly added sink state. If transitions have output
-    (Mealy machines and their derivatives), 'epsilon' is used as an output value. If a state has an output value,
-    it is either False (in case of DFA) or 'sink_state' in case of Moore machines and its derivatives.
-
-    Args:
-
-        automaton: automaton that is potentially not input complete
-        missing_transition_go_to: either 'self_loop' or 'sink_state'.
-
-    Returns:
-
-        an input complete automaton
-    """
-    from aalpy.base import DeterministicAutomaton
-    from aalpy.automata import Dfa, MooreState, MealyMachine, Mdp, StochasticMealyMachine, Onfsm, \
-        DfaState, MealyState, MooreMachine, OnfsmState, MdpState, StochasticMealyState
-
-    assert missing_transition_go_to in {'self_loop', 'sink_state'}
-
-    input_al = automaton.get_input_alphabet()
-
-    if automaton.is_input_complete():
-        return automaton
-
-    sink_state = None
-    sink_state_type_dict = {Dfa: DfaState(state_id='sink', is_accepting=False),
-                            MooreMachine: MooreState(state_id='sink', output='sink_state'),
-                            MealyMachine: MealyState(state_id='sink'),
-                            Onfsm: OnfsmState(state_id='sink'),
-                            Mdp: MdpState(state_id='sink', output='sink_state'),
-                            StochasticMealyMachine: StochasticMealyState(state_id='sink')}
-
-    if missing_transition_go_to == 'sink_state':
-        sink_state = sink_state_type_dict[automaton.__class__]
-        automaton.states.append(sink_state)
-
-    for state in automaton.states:
-        for i in input_al:
-            if i not in state.transitions.keys():
-                if missing_transition_go_to == 'self_loop':
-                    if isinstance(automaton, DeterministicAutomaton):
-                        state.transitions[i] = state
-                        if isinstance(automaton, MealyMachine):
-                            state.output_fun[i] = 'epsilon'
-                    if isinstance(automaton, Onfsm):
-                        state.transitions[i].append(('epsilon', state))
-                    if isinstance(automaton, Mdp):
-                        state.transitions[i].append((state, 1.))
-                    if isinstance(automaton, StochasticMealyMachine):
-                        state.transitions[i].append((state, 'epsilon', 1.))
-                else:
-                    if isinstance(automaton, Dfa):
-                        state.transitions[i] = sink_state
-                    if isinstance(automaton, MooreMachine):
-                        state.transitions[i] = sink_state
-                    if isinstance(automaton, MealyMachine):
-                        state.transitions[i] = sink_state
-                        state.output_fun[i] = 'epsilon'
-                    if isinstance(automaton, Onfsm):
-                        state.transitions[i].append(('epsilon', sink_state))
-                    if isinstance(automaton, Mdp):
-                        state.transitions[i].append((sink_state, 1.))
-                    if isinstance(automaton, StochasticMealyMachine):
-                        state.transitions[i].append((sink_state, 'epsilon', 1.))
-
-    return automaton
-
-
-def convert_i_o_traces_for_RPNI(sequences):
-    """
-    Converts a list of input-output sequences to RPNI format.
-    Eg. [[(1,'a'), (2,'b'), (3,'c')], [(6,'7'), (4,'e'), (3,'c')]] to
-    [((1,), 'a'), ((1, 2), 'b'), ((1, 2, 3), 'c'), ((6,), '7'), ((6, 4), 'e'), ((6, 4, 3), 'c')]
-    """
-    rpni_sequences = set()
-
-    for s in sequences:
-        for i in range(len(s)):
-            inputs = tuple([io[0] for io in s[:i+1]])
-            rpni_sequences.add((inputs, s[i][1]))
-
-    return list(rpni_sequences)
+import string
+from collections import defaultdict
+
+
+def extend_set(list_to_extend: list, new_elements: list) -> list:
+    """
+    Helper function to extend a list while maintaining set property.
+    They are stored as lists, so with this function set property is maintained.
+    :return
+
+    Returns:
+
+        list of elements that were added to the set
+
+    """
+    set_repr = set(list_to_extend)
+    added_elements = [s for s in new_elements if s not in set_repr]
+    list_to_extend.extend(added_elements)
+    return added_elements
+
+
+def all_prefixes(li):
+    """
+    Returns all prefixes of a list.
+
+    Args:
+      li: list from which to compute all prefixes
+
+    Returns:
+      list of all prefixes
+
+    """
+    return [tuple(li[:i + 1]) for i in range(len(li))]
+
+
+def all_suffixes(li):
+    """
+    Returns all suffixes of a list.
+
+    Args:
+      li: list from which to compute all suffixes
+
+    Returns:
+      list of all suffixes
+
+    """
+    return [tuple(li[len(li) - i - 1:]) for i in range(len(li))]
+
+
+def profile_function(function: callable, sort_key='cumtime'):
+    """
+
+    Args:
+      function: callable: 
+      sort_key:  (Default value = 'cumtime')
+
+    Returns:
+        prints the profiling results
+    """
+    import cProfile
+    pr = cProfile.Profile()
+    pr.enable()
+    function()
+    pr.disable()
+    pr.print_stats(sort=sort_key)
+
+
+def random_string_generator(size=10, chars=string.ascii_lowercase + string.digits):
+    """
+
+    Args:
+
+      size:  (Default value = 10)
+      chars:  (Default value = string.ascii_lowercase + string.digits)
+
+    Returns:
+
+        a random string of length size
+    """
+    import random
+    return ''.join(random.choice(chars) for _ in range(size))
+
+
+def print_learning_info(info: dict):
+    """
+    Print learning statistics.
+    """
+    print('-----------------------------------')
+    print('Learning Finished.')
+    print('Learning Rounds:  {}'.format(info['learning_rounds']))
+    print('Number of states: {}'.format(info['automaton_size']))
+    print('Time (in seconds)')
+    print('  Total                : {}'.format(info['total_time']))
+    print('  Learning algorithm   : {}'.format(info['learning_time']))
+    print('  Conformance checking : {}'.format(info['eq_oracle_time']))
+    print('Learning Algorithm')
+    print(' # Membership Queries  : {}'.format(info['queries_learning']))
+    if 'cache_saved' in info.keys():
+        print(' # MQ Saved by Caching : {}'.format(info['cache_saved']))
+    print(' # Steps               : {}'.format(info['steps_learning']))
+    print('Equivalence Query')
+    print(' # Membership Queries  : {}'.format(info['queries_eq_oracle']))
+    print(' # Steps               : {}'.format(info['steps_eq_oracle']))
+    print('-----------------------------------')
+
+
+def print_observation_table(ot, table_type):
+    """
+    Prints the whole observation table.
+
+    Args:
+
+        ot: observation table
+        table_type: 'det', 'non-det', or 'stoc'
+
+    """
+    if table_type == 'det':
+        s_set, extended_s, e_set, table = ot.S, ot.s_dot_a(), ot.E, ot.T
+    elif table_type == 'non-det':
+        s_set, extended_s, e_set = ot.S, ot.get_extended_S(), ot.E
+        table = ot.sul.cache.get_table(s_set + extended_s, e_set)
+    elif table_type == 'abstracted-non-det':
+        s_set, extended_s, e_set, table = ot.S, ot.S_dot_A, ot.E, ot.T
+    else:
+        s_set, extended_s, e_set, table = ot.S, ot.get_extended_s(), ot.E, ot.T
+
+    headers = [str(e) for e in e_set]
+    s_rows = []
+    extended_rows = []
+    headers.insert(0, 'Prefixes / E set')
+    for s in s_set:
+        row = [str(s)]
+        if table_type == 'det':
+            row.extend(str(e) for e in table[s])
+        else:
+            row.extend(str(table[s][e]) for e in e_set)
+        s_rows.append(row)
+    for s in extended_s:
+        row = [str(s)]
+        if table_type == 'det':
+            row.extend(str(e) for e in table[s])
+        else:
+            row.extend(str(table[s][e]) for e in e_set)
+        extended_rows.append(row)
+
+    table = [headers] + s_rows
+    columns = defaultdict(int)
+    for i in table + extended_rows:
+        for index, el in enumerate(i):
+            columns[index] = max(columns[index], len(el))
+
+    row_len = 0
+    for row in table:
+        row = "|".join(element.ljust(columns[ind] + 1) for ind, element in enumerate(row))
+        print("-" * len(row))
+        row_len = len(row)
+        print(row)
+    print('=' * row_len)
+    for row in extended_rows:
+        row = "|".join(element.ljust(columns[ind] + 1) for ind, element in enumerate(row))
+        print("-" * len(row))
+        print(row)
+    print('-' * row_len)
+
+
+def is_suffix_of(suffix, trace) -> bool:
+    """
+
+    Args:
+      suffix: target suffix
+      trace: trace in question
+
+    Returns:
+
+        True if suffix is the suffix of trace.
+    """
+    if len(trace) < len(suffix):
+        return False
+    else:
+        return trace[-len(suffix):] == suffix
+
+
+def get_cex_prefixes(cex, automaton_type):
+    """
+    Returns all prefixes of the stochastic automaton.
+
+    Args:
+        cex: counterexample
+        automaton_type: `mdp` or `smm`
+
+    Returns:
+
+        all prefixes of the counterexample based on the `automaton_type`
+    """
+    if automaton_type == 'mdp':
+        return [tuple(cex[:i + 1]) for i in range(0, len(cex), 2)]
+    return [tuple(cex[:i]) for i in range(0, len(cex) + 1, 2)]
+
+
+def get_available_oracles_and_err_msg():
+    from aalpy.oracles import RandomWalkEqOracle
+    from aalpy.oracles import RandomWordEqOracle
+    available_oracles = {RandomWalkEqOracle, RandomWordEqOracle}
+
+    available_oracles_msg = 'Warning! Only Random Walk and Random Word oracles are supported for non-deterministic and ' \
+                            'stochastic learning. If you have implemented the custom oracle, set the custom_oracle ' \
+                            'flag to True. '
+
+    return available_oracles, available_oracles_msg
+
+
+def make_input_complete(automaton, missing_transition_go_to='self_loop'):
+    """
+    Makes the automaton input complete/enabled. If a input is not defined in a state, it will lead to the self loop.
+    In case of Mealy Machines, Stochastic Mealy machines and ONFSM 'epsilon' is used as output.
+    Self loop simply loops the transition back to state which was not input complete,
+    whereas 'sink_state' leads all transitions to a newly added sink state. If transitions have output
+    (Mealy machines and their derivatives), 'epsilon' is used as an output value. If a state has an output value,
+    it is either False (in case of DFA) or 'sink_state' in case of Moore machines and its derivatives.
+
+    Args:
+
+        automaton: automaton that is potentially not input complete
+        missing_transition_go_to: either 'self_loop' or 'sink_state'.
+
+    Returns:
+
+        an input complete automaton
+    """
+    from aalpy.base import DeterministicAutomaton
+    from aalpy.automata import Dfa, MooreState, MealyMachine, Mdp, StochasticMealyMachine, Onfsm, \
+        DfaState, MealyState, MooreMachine, OnfsmState, MdpState, StochasticMealyState
+
+    assert missing_transition_go_to in {'self_loop', 'sink_state'}
+
+    input_al = automaton.get_input_alphabet()
+
+    if automaton.is_input_complete():
+        return automaton
+
+    sink_state = None
+    sink_state_type_dict = {Dfa: DfaState(state_id='sink', is_accepting=False),
+                            MooreMachine: MooreState(state_id='sink', output='sink_state'),
+                            MealyMachine: MealyState(state_id='sink'),
+                            Onfsm: OnfsmState(state_id='sink'),
+                            Mdp: MdpState(state_id='sink', output='sink_state'),
+                            StochasticMealyMachine: StochasticMealyState(state_id='sink')}
+
+    if missing_transition_go_to == 'sink_state':
+        sink_state = sink_state_type_dict[automaton.__class__]
+        automaton.states.append(sink_state)
+
+    for state in automaton.states:
+        for i in input_al:
+            if i not in state.transitions.keys():
+                if missing_transition_go_to == 'self_loop':
+                    if isinstance(automaton, DeterministicAutomaton):
+                        state.transitions[i] = state
+                        if isinstance(automaton, MealyMachine):
+                            state.output_fun[i] = 'epsilon'
+                    if isinstance(automaton, Onfsm):
+                        state.transitions[i].append(('epsilon', state))
+                    if isinstance(automaton, Mdp):
+                        state.transitions[i].append((state, 1.))
+                    if isinstance(automaton, StochasticMealyMachine):
+                        state.transitions[i].append((state, 'epsilon', 1.))
+                else:
+                    if isinstance(automaton, Dfa):
+                        state.transitions[i] = sink_state
+                    if isinstance(automaton, MooreMachine):
+                        state.transitions[i] = sink_state
+                    if isinstance(automaton, MealyMachine):
+                        state.transitions[i] = sink_state
+                        state.output_fun[i] = 'epsilon'
+                    if isinstance(automaton, Onfsm):
+                        state.transitions[i].append(('epsilon', sink_state))
+                    if isinstance(automaton, Mdp):
+                        state.transitions[i].append((sink_state, 1.))
+                    if isinstance(automaton, StochasticMealyMachine):
+                        state.transitions[i].append((sink_state, 'epsilon', 1.))
+
+    return automaton
+
+
+def convert_i_o_traces_for_RPNI(sequences):
+    """
+    Converts a list of input-output sequences to RPNI format.
+    Eg. [[(1,'a'), (2,'b'), (3,'c')], [(6,'7'), (4,'e'), (3,'c')]] to
+    [((1,), 'a'), ((1, 2), 'b'), ((1, 2, 3), 'c'), ((6,), '7'), ((6, 4), 'e'), ((6, 4, 3), 'c')]
+    """
+    rpni_sequences = set()
+
+    for s in sequences:
+        for i in range(len(s)):
+            inputs = tuple([io[0] for io in s[:i+1]])
+            rpni_sequences.add((inputs, s[i][1]))
+
+    return list(rpni_sequences)
```

## aalpy/utils/ModelChecking.py

```diff
@@ -1,407 +1,420 @@
-import os
-import queue
-import re
-from collections import defaultdict
-from typing import Tuple
-
-import itertools as it
-
-import aalpy.paths
-from aalpy.SULs import MealySUL, DfaSUL, MooreSUL
-from aalpy.automata import Mdp, StochasticMealyMachine, MealyMachine, Dfa, MooreMachine, MooreState, MealyState, \
-    DfaState
-from aalpy.base import DeterministicAutomaton, SUL, AutomatonState
-from random import choices
-
-prism_prob_output_regex = re.compile("Result: (\d+\.\d+)")
-
-
-def get_properties_file(exp_name):
-    property_files = {
-        'first_grid': aalpy.paths.path_to_properties + 'first_eval.props',
-        'second_grid': aalpy.paths.path_to_properties + 'second_eval.props',
-        'shared_coin': aalpy.paths.path_to_properties + 'shared_coin_eval.props',
-        'slot_machine': aalpy.paths.path_to_properties + 'slot_machine_eval.props',
-        'mqtt': aalpy.paths.path_to_properties + 'emqtt_two_client.props',
-        'tcp': aalpy.paths.path_to_properties + 'tcp_eval.props'
-    }
-    return property_files[exp_name]
-
-
-def get_correct_prop_values(exp_name):
-    correct_model_properties = {
-        'first_grid': {'prob1': 0.96217534, 'prob2': 0.6499274956800001, 'prob3': 0.6911765746880001},
-        'second_grid': {'prob1': 0.93480795088125, 'prob2': 0.6711947700000002, 'prob3': 0.9742903305241055,
-                        'prob4': 0.14244219329051103},
-        'shared_coin': {'prob1': 0.10694382182657244, 'prob2': 0.5555528623795738, 'prob3': 0.3333324384052837,
-                        'prob4': 0.42857002816478273, 'prob5': 0.001708984375, 'prob6': 0.266845703125,
-                        'prob7': 0.244384765625, 'prob8': 0.263427734375},
-        'slot_machine': {'prob1': 0.36380049887344645, 'prob2': 0.6445910164135946, 'prob3': 1.0, 'prob4': 0.159,
-                         'prob5': 0.28567, 'prob6': 0.2500000000000001, 'prob7': 0.025445087448668406},
-        'mqtt': {'prob1': 0.9612, 'prob2': 0.34390000000000004, 'prob3': 0.6513215599000001, 'prob4': 0.814697981114816,
-                 'prob5': 0.7290000000000001},
-        'tcp': {'prob1': 0.19, 'prob2': 0.5695327900000001, 'prob3': 0.7712320754503901, 'prob4': 0.8784233454094308}
-    }
-    return list(correct_model_properties[exp_name].values())
-
-
-def _target_string(target, orig_id_to_int_id):
-    target_state = target[0]
-    target_prob = target[1]
-    target_id = orig_id_to_int_id[target_state.state_id]
-    return f"{target_prob} : (loc'={target_id})"
-
-
-def _sanitize_for_prism(symbol):
-    if symbol in ["mdp", "init", "module", "endmodule", "label"]:
-        return "___" + symbol + "___"
-    else:
-        return symbol
-
-
-def mdp_2_prism_format(mdp: Mdp, name: str, output_path=None):
-    """
-    Translates MDP to Prims modelling language.
-
-    Args:
-
-        mdp: markov decision process
-
-        name: name of the mdp/experiment
-
-        output_path: output file (Default value = None)
-
-    """
-    module_string = "mdp"
-    module_string += os.linesep
-    module_string += f"module {name}"
-    module_string += os.linesep
-
-    nr_states = len(mdp.states)
-    orig_id_to_int_id = dict()
-    for i, s in enumerate(mdp.states):
-        orig_id_to_int_id[s.state_id] = i
-    module_string += "loc : [0..{}] init {};".format(nr_states, orig_id_to_int_id[mdp.initial_state.state_id])
-    module_string += os.linesep
-
-    # print transitions
-    for source in mdp.states:
-        source_id = orig_id_to_int_id[source.state_id]
-        for inp in source.transitions.keys():
-            if source.transitions[inp]:
-                target_strings = \
-                    map(lambda target: _target_string(target, orig_id_to_int_id), source.transitions[inp])
-                target_joined = " + ".join(target_strings)
-                module_string += f"[{_sanitize_for_prism(inp)}] loc={source_id} -> {os.linesep} {target_joined};"
-                module_string += os.linesep
-    module_string += "endmodule"
-    module_string += os.linesep
-    # labelling function
-    output_to_state_id = defaultdict(list)
-    for s in mdp.states:
-        joined_output = s.output
-        outputs = joined_output.split("__")
-        for o in outputs:
-            if o:
-                output_to_state_id[o].append(orig_id_to_int_id[s.state_id])
-
-    for output, states in output_to_state_id.items():
-        state_propositions = map(lambda s_id: "loc={}".format(s_id), states)
-        state_disjunction = "|".join(state_propositions)
-        output_string = _sanitize_for_prism(output)
-        module_string += f"label \"{output_string}\" = {state_disjunction};"
-        module_string += os.linesep
-
-    if output_path:
-        with open(output_path, "w") as text_file:
-            text_file.write(module_string)
-    return module_string
-
-
-def evaluate_all_properties(prism_file_name, properties_file_name):
-    import subprocess
-    import io
-    from os import path
-
-    prism_file = aalpy.paths.path_to_prism.split('/')[-1]
-    path_to_prism_file = aalpy.paths.path_to_prism[:-len(prism_file)]
-
-    file_abs_path = path.abspath(prism_file_name)
-    properties_als_path = path.abspath(properties_file_name)
-    results = {}
-    proc = subprocess.Popen(
-        [aalpy.paths.path_to_prism, file_abs_path, properties_als_path],
-        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=path_to_prism_file)
-    for line in io.TextIOWrapper(proc.stdout, encoding="utf-8"):
-        if not line:
-            break
-        else:
-            match = prism_prob_output_regex.match(line)
-            if match:
-                results[f'prop{len(results) + 1}'] = float(match.group(1))
-    proc.kill()
-    return results
-
-
-def model_check_properties(model: Mdp, properties: str):
-    """
-
-    Args:
-        model: Markov Decision Process that serves as a basis for model checking.
-        properties: Properties file. It should point to a file under the path_to_properties folder.
-
-    Returns:
-
-        results of model checking
-    """
-    from os import remove
-    from aalpy.utils import mdp_2_prism_format
-    mdp_2_prism_format(mdp=model, name='mc_exp', output_path=f'mc_exp.prism')
-
-    prism_model_path = f'mc_exp.prism'
-
-    data = evaluate_all_properties(prism_model_path, properties)
-
-    remove(prism_model_path)
-
-    return data
-
-
-def model_check_experiment(path_to_properties, correct_prop_values, mdp, precision=4):
-    """
-    For our stochastic experiments you can use this function.
-    For example, check learn_stochastic_system_and_do_model_checking in Examples.py
-
-    Args:
-        path_to_properties: path to the properties file
-        correct_prop_values: correct values of all properties. In list, where property at index i corresponds to the
-            i-th element of the list.
-        mdp: MDP
-        precision: precision to which round up results
-
-    Returns:
-
-        results of model checking and absolute differance to the correct results
-    """
-    model_checking_results = model_check_properties(mdp, path_to_properties)
-
-    diff_2_correct = dict()
-    for ind, val in enumerate(model_checking_results.values()):
-        diff_2_correct[f'prop{ind + 1}'] = round(abs(correct_prop_values[ind] - val), precision)
-
-    results = {key: round(val, precision) for key, val in model_checking_results.items()}
-    return results, diff_2_correct
-
-
-def stop_based_on_confidence(hypothesis, property_based_stopping, print_level=2):
-    """
-
-    Args:
-
-        hypothesis: Markov decision process
-        property_based_stopping: a tuple (path to properties file, list of correct property values, max allowed error)
-        print_level: 2 or 3 if output of model checking is to be printed during learning
-
-    Returns:
-
-        True if absolute error for all properties is smaller then property_based_stopping[2]
-    """
-    from aalpy.automata.StochasticMealyMachine import smm_to_mdp_conversion
-
-    path_2_prop = property_based_stopping[0]
-    correct_values = property_based_stopping[1]
-    error_bound = property_based_stopping[2]
-
-    model = hypothesis
-    if isinstance(hypothesis, StochasticMealyMachine):
-        model = smm_to_mdp_conversion(hypothesis)
-
-    res, diff = model_check_experiment(path_2_prop, correct_values, model)
-
-    if print_level >= 2:
-        print('Error for each property:', [round(d * 100, 2) for d in diff.values()])
-    if not diff:
-        return False
-
-    for d in diff.values():
-        if d > error_bound:
-            return False
-
-    return True
-
-def bisimilar(a1: DeterministicAutomaton, a2: DeterministicAutomaton):
-    """
-    Checks whether the provided automata are bisimilar
-    """
-
-    if a1.__class__ != a2.__class__:
-        raise ValueError("tried to check bisimilarity of distinct automaton types")
-    supported_automaton_types = (Dfa, MooreMachine, MealyMachine)
-    if not isinstance(a1, supported_automaton_types):
-        raise NotImplementedError(f"bisimilarity is not implemented for {a1.__class__.__name__}. Supported: {', '.join(t.__name__ for t in supported_automaton_types)}")
-
-    to_check = queue.Queue[Tuple[AutomatonState, AutomatonState]]()
-    to_check.put((a1.initial_state, a2.initial_state))
-    requirements = dict()
-    requirements[(a1.initial_state, a2.initial_state)] = []
-
-    while not to_check.empty():
-        s1, s2 = to_check.get()
-        if (isinstance(s1, DfaState)) and s1.is_accepting != s2.is_accepting or \
-           (isinstance(s1, MooreState) and s1.output != s2.output) or \
-           (isinstance(s1, MealyState) and s1.output_fun != s2.output_fun):
-            return requirements[(s1, s2)]
-
-        t1, t2 = s1.transitions, s2.transitions
-        # not using sets -> deterministic but inefficient
-        # could use set to detect and slow only if detection?
-        for t in it.chain(t1.keys(), t2.keys()):
-            if (t in t1.keys()) != (t in t2.keys()):
-                return requirements[(s1, s2)] + [t]
-
-        for t in t1.keys():
-            c1, c2 = t1[t], t2[t]
-            if (c1, c2) not in requirements:
-                requirements[(c1, c2)] = requirements[(s1, s2)] + [t]
-                to_check.put((c1, c2))
-
-def compare_automata(aut_1: DeterministicAutomaton, aut_2: DeterministicAutomaton, num_cex=10):
-    """
-    Finds cases of non-conformance between first and second automaton. This is done by performing RandomW equivalence
-    check. It is possible that number of found counterexamples is smaller than num_cex, as no counterexample will be a
-    suffix of a previously found counterexample.
-
-    Args:
-
-        aut_1: first automaton
-
-        aut_2: second automaton
-
-        num_cex: max. number of searches for counterexamples
-
-    Returns:
-
-        A list of input sequences that revel different behaviour on both automata. Counterexamples are sorted by length.
-    """
-    #
-    from aalpy.oracles import RandomWMethodEqOracle
-
-    type_map = {MooreMachine: MooreSUL, Dfa: DfaSUL, MealyMachine: MealySUL}
-    assert set(aut_1.get_input_alphabet()) == set(aut_2.get_input_alphabet())
-
-    input_al = aut_1.get_input_alphabet()
-    # larger automaton is used as hypothesis, as then test-cases will contain prefixes leading to states
-    # not in smaller automaton
-    base_automaton, test_automaton = (aut_1, aut_2) if aut_1.size < aut_2.size else (aut_2, aut_1)
-    base_sul = type_map[type(base_automaton)](base_automaton)
-
-    # compute prefixes for all states of the test automaton (needed for advanced eq. oracle)
-    for state in test_automaton.states:
-        if not state.prefix:
-            state.prefix = test_automaton.get_shortest_path(test_automaton.initial_state, state)
-
-    # setup  the eq oracle
-    eq_oracle = RandomWMethodEqOracle(input_al, base_sul, walks_per_state=min(100, len(input_al) * 10), walk_len=10)
-
-    found_cex = []
-    # to avoid near "infinite" loops due to while loop and set requirement
-    # that is, if you can only find 1 cex and all other cexs are suffixes of that cex, first while condition will never
-    # be reached
-    failsafe_counter = 0
-    failsafe_stopping = num_cex * 100
-    while len(found_cex) < num_cex or failsafe_counter == failsafe_stopping:
-        cex = eq_oracle.find_cex(test_automaton)
-        # if no counterexample can be found terminate the loop
-        if cex is None:
-            break
-        if cex not in found_cex:
-            found_cex.append(cex)
-        failsafe_counter += 1
-
-    found_cex.sort(key=len)
-
-    return found_cex
-
-
-class TestCaseWrapperSUL(SUL):
-    def __init__(self, sul):
-        super().__init__()
-        self.sul = sul
-        self.test_cases = []
-        self.test_case_inputs = None
-        self.test_case_outputs = None
-
-    def pre(self):
-        self.test_case_inputs = []
-        self.test_case_outputs = []
-        return self.sul.pre()
-
-    def post(self):
-        if self.test_case_inputs and self.test_case_outputs:
-            self.test_cases.append((tuple(self.test_case_inputs), tuple(self.test_case_outputs)))
-        return self.sul.post()
-
-    def step(self, letter):
-        output = self.sul.step(letter)
-        self.test_case_inputs.append(letter)
-        self.test_case_outputs.append(output)
-        return output
-
-
-def generate_test_cases(automaton: DeterministicAutomaton, oracle):
-    """
-    Uses parametrized eq. oracle to construct test cases on the automaton.
-    If automaton are big (200+ states), increase recursion depth if necessary (eg. sys.setrecursionlimit(10000)).
-
-    Args:
-
-        automaton: deterministic automaton that serves as a basis for test case generation
-        oracle: oracle that will construct test-cases and record inputs and outputs
-
-    Returns:
-
-        List of test cases, where each testcase is a tuple containing two elements, and input and an output sequance.
-    """
-    from copy import deepcopy
-
-    type_map = {MooreMachine: MooreSUL, Dfa: DfaSUL, MealyMachine: MealySUL}
-
-    automaton_copy = deepcopy(automaton)
-    base_sul = type_map[type(automaton_copy)](automaton_copy)
-
-    wrapped_sul = TestCaseWrapperSUL(base_sul)
-    oracle.sul = wrapped_sul
-    # no counterexamples can be found
-    cex = oracle.find_cex(automaton)
-    assert cex is None
-    return wrapped_sul.test_cases
-
-
-def statistical_model_checking(model, goals, max_num_steps, num_tests=105967):
-    """
-
-
-    Args:
-        model: model on which model checking is performed
-        goals: set of goal outputs
-        max_num_steps: bounded length of tests
-        num_tests: num of tests that will be performed
-
-    Returns:
-
-        num of tests containing element of goals set / num_tests
-    """
-    def compute_output_sequence(model, seq):
-        model.reset_to_initial()
-        observed_outputs = {model.step(i) for i in seq}
-        return observed_outputs
-
-    goal_reached = 0
-    inputs = model.get_input_alphabet()
-    for _ in range(num_tests):
-        test_sequence = choices(inputs, k=max_num_steps)
-        outputs = compute_output_sequence(model, test_sequence)
-        if goals & outputs:
-            goal_reached += 1
-
-    return goal_reached / num_tests
+import itertools as it
+import os
+import re
+from collections import defaultdict
+from queue import Queue
+from random import choices
+from typing import Tuple, Union
+
+import aalpy.paths
+from aalpy.SULs import MealySUL, DfaSUL, MooreSUL
+from aalpy.automata import Mdp, StochasticMealyMachine, MealyMachine, Dfa, MooreMachine, MooreState, MealyState, \
+    DfaState
+from aalpy.base import DeterministicAutomaton, SUL, AutomatonState
+
+prism_prob_output_regex = re.compile("Result: (\d+\.\d+)")
+
+
+def get_properties_file(exp_name):
+    property_files = {
+        'first_grid': aalpy.paths.path_to_properties + 'first_eval.props',
+        'second_grid': aalpy.paths.path_to_properties + 'second_eval.props',
+        'shared_coin': aalpy.paths.path_to_properties + 'shared_coin_eval.props',
+        'slot_machine': aalpy.paths.path_to_properties + 'slot_machine_eval.props',
+        'mqtt': aalpy.paths.path_to_properties + 'emqtt_two_client.props',
+        'tcp': aalpy.paths.path_to_properties + 'tcp_eval.props',
+        'bluetooth': aalpy.paths.path_to_properties + 'bluetooth.props',
+    }
+    return property_files[exp_name]
+
+
+def get_correct_prop_values(exp_name):
+    correct_model_properties = {
+        'first_grid': {'prob1': 0.96217534, 'prob2': 0.6499274956800001, 'prob3': 0.6911765746880001},
+        'second_grid': {'prob1': 0.93480795088125, 'prob2': 0.6711947700000002, 'prob3': 0.9742903305241055,
+                        'prob4': 0.14244219329051103},
+        'shared_coin': {'prob1': 0.10694382182657244, 'prob2': 0.5555528623795738, 'prob3': 0.3333324384052837,
+                        'prob4': 0.42857002816478273, 'prob5': 0.001708984375, 'prob6': 0.266845703125,
+                        'prob7': 0.244384765625, 'prob8': 0.263427734375},
+        'slot_machine': {'prob1': 0.36380049887344645, 'prob2': 0.6445910164135946, 'prob3': 1.0, 'prob4': 0.159,
+                         'prob5': 0.28567, 'prob6': 0.2500000000000001, 'prob7': 0.025445087448668406},
+        'mqtt': {'prob1': 0.9612, 'prob2': 0.34390000000000004, 'prob3': 0.6513215599000001, 'prob4': 0.814697981114816,
+                 'prob5': 0.7290000000000001},
+        'tcp': {'prob1': 0.19, 'prob2': 0.5695327900000001, 'prob3': 0.7712320754503901, 'prob4': 0.8784233454094308},
+        'bluetooth': {'prop1': 0.16800000000000004, 'prop2': 0.3926480000000001, 'prop3': 0.5572338000000001,
+                      'prop4': 0.6772233874640001, 'prop5': 0.7646958490393682, 'prop6': 0.8284632739463244,
+                      'prop7': 0.36000000000000004, 'prop8': 0.5904, 'prop9': 0.7902848,
+                      'prop10': 0.8926258176000001, 'prop11': 0.9450244186112, 'prop12': 0.9718525023289344,
+                      'prop13': 0.9855884811924145}
+    }
+    return list(correct_model_properties[exp_name].values())
+
+
+def _target_string(target, orig_id_to_int_id):
+    target_state = target[0]
+    target_prob = target[1]
+    target_id = orig_id_to_int_id[target_state.state_id]
+    return f"{target_prob} : (loc'={target_id})"
+
+
+def _sanitize_for_prism(symbol):
+    if symbol in ["mdp", "init", "module", "endmodule", "label"]:
+        return "___" + symbol + "___"
+    else:
+        return symbol
+
+
+def mdp_2_prism_format(mdp: Mdp, name: str, output_path=None):
+    """
+    Translates MDP to Prims modelling language.
+
+    Args:
+
+        mdp: markov decision process
+
+        name: name of the mdp/experiment
+
+        output_path: output file (Default value = None)
+
+    """
+    module_string = "mdp"
+    module_string += os.linesep
+    module_string += f"module {name}"
+    module_string += os.linesep
+
+    nr_states = len(mdp.states)
+    orig_id_to_int_id = dict()
+    for i, s in enumerate(mdp.states):
+        orig_id_to_int_id[s.state_id] = i
+    module_string += "loc : [0..{}] init {};".format(nr_states, orig_id_to_int_id[mdp.initial_state.state_id])
+    module_string += os.linesep
+
+    # print transitions
+    for source in mdp.states:
+        source_id = orig_id_to_int_id[source.state_id]
+        for inp in source.transitions.keys():
+            if source.transitions[inp]:
+                target_strings = \
+                    map(lambda target: _target_string(target, orig_id_to_int_id), source.transitions[inp])
+                target_joined = " + ".join(target_strings)
+                module_string += f"[{_sanitize_for_prism(inp)}] loc={source_id} -> {os.linesep} {target_joined};"
+                module_string += os.linesep
+    module_string += "endmodule"
+    module_string += os.linesep
+    # labelling function
+    output_to_state_id = defaultdict(list)
+    for s in mdp.states:
+        joined_output = s.output
+        outputs = joined_output.split("__")
+        for o in outputs:
+            if o:
+                output_to_state_id[o].append(orig_id_to_int_id[s.state_id])
+
+    for output, states in output_to_state_id.items():
+        state_propositions = map(lambda s_id: "loc={}".format(s_id), states)
+        state_disjunction = "|".join(state_propositions)
+        output_string = _sanitize_for_prism(output)
+        module_string += f"label \"{output_string}\" = {state_disjunction};"
+        module_string += os.linesep
+
+    if output_path:
+        with open(output_path, "w") as text_file:
+            text_file.write(module_string)
+    return module_string
+
+
+def evaluate_all_properties(prism_file_name, properties_file_name):
+    import subprocess
+    import io
+    from os import path
+
+    prism_file = aalpy.paths.path_to_prism.split('/')[-1]
+    path_to_prism_file = aalpy.paths.path_to_prism[:-len(prism_file)]
+
+    file_abs_path = path.abspath(prism_file_name)
+    properties_als_path = path.abspath(properties_file_name)
+    results = {}
+    proc = subprocess.Popen(
+        [aalpy.paths.path_to_prism, file_abs_path, properties_als_path],
+        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=path_to_prism_file)
+    for line in io.TextIOWrapper(proc.stdout, encoding="utf-8"):
+        if not line:
+            break
+        else:
+            match = prism_prob_output_regex.match(line)
+            if match:
+                results[f'prop{len(results) + 1}'] = float(match.group(1))
+    proc.kill()
+    return results
+
+
+def model_check_properties(model: Mdp, properties: str):
+    """
+
+    Args:
+        model: Markov Decision Process that serves as a basis for model checking.
+        properties: Properties file. It should point to a file under the path_to_properties folder.
+
+    Returns:
+
+        results of model checking
+    """
+    from os import remove
+    from aalpy.utils import mdp_2_prism_format
+    mdp_2_prism_format(mdp=model, name='mc_exp', output_path=f'mc_exp.prism')
+
+    prism_model_path = f'mc_exp.prism'
+
+    data = evaluate_all_properties(prism_model_path, properties)
+
+    remove(prism_model_path)
+
+    return data
+
+
+def model_check_experiment(path_to_properties, correct_prop_values, mdp, precision=4):
+    """
+    For our stochastic experiments you can use this function.
+    For example, check learn_stochastic_system_and_do_model_checking in Examples.py
+
+    Args:
+        path_to_properties: path to the properties file
+        correct_prop_values: correct values of all properties. In list, where property at index i corresponds to the
+            i-th element of the list.
+        mdp: MDP
+        precision: precision to which round up results
+
+    Returns:
+
+        results of model checking and absolute differance to the correct results
+    """
+    model_checking_results = model_check_properties(mdp, path_to_properties)
+
+    diff_2_correct = dict()
+    for ind, val in enumerate(model_checking_results.values()):
+        diff_2_correct[f'prop{ind + 1}'] = round(abs(correct_prop_values[ind] - val), precision)
+
+    results = {key: round(val, precision) for key, val in model_checking_results.items()}
+    return results, diff_2_correct
+
+
+def stop_based_on_confidence(hypothesis, property_based_stopping, print_level=2):
+    """
+
+    Args:
+
+        hypothesis: Markov decision process
+        property_based_stopping: a tuple (path to properties file, list of correct property values, max allowed error)
+        print_level: 2 or 3 if output of model checking is to be printed during learning
+
+    Returns:
+
+        True if absolute error for all properties is smaller then property_based_stopping[2]
+    """
+    from aalpy.automata.StochasticMealyMachine import smm_to_mdp_conversion
+
+    path_2_prop = property_based_stopping[0]
+    correct_values = property_based_stopping[1]
+    error_bound = property_based_stopping[2]
+
+    model = hypothesis
+    if isinstance(hypothesis, StochasticMealyMachine):
+        model = smm_to_mdp_conversion(hypothesis)
+
+    res, diff = model_check_experiment(path_2_prop, correct_values, model)
+
+    if print_level >= 2:
+        print('Error for each property:', [round(d * 100, 2) for d in diff.values()])
+    if not diff:
+        return False
+
+    for d in diff.values():
+        if d > error_bound:
+            return False
+
+    return True
+
+
+def bisimilar(a1: DeterministicAutomaton, a2: DeterministicAutomaton, return_cex=False) -> Union[bool, None, list]:
+    """
+    Checks whether the provided automata are bisimilar.
+    If return_cex the function returns a counter example or None, otherwise a Boolean is returned.
+    """
+
+    # TODO allow states as inputs instead of automata
+    if a1.__class__ != a2.__class__:
+        raise ValueError("tried to check bisimilarity of different automaton types")
+    supported_automaton_types = (Dfa, MooreMachine, MealyMachine)
+    if not isinstance(a1, supported_automaton_types):
+        raise NotImplementedError(
+            f"bisimilarity is not implemented for {a1.__class__.__name__}. Supported: {', '.join(t.__name__ for t in supported_automaton_types)}")
+
+    to_check: Queue[Tuple[AutomatonState, AutomatonState]] = Queue()
+    to_check.put((a1.initial_state, a2.initial_state))
+    requirements = dict()
+    requirements[(a1.initial_state, a2.initial_state)] = []
+
+    while not to_check.empty():
+        s1, s2 = to_check.get()
+        if (isinstance(s1, DfaState)) and s1.is_accepting != s2.is_accepting or \
+                (isinstance(s1, MooreState) and s1.output != s2.output) or \
+                (isinstance(s1, MealyState) and s1.output_fun != s2.output_fun):
+            return requirements[(s1, s2)] if return_cex else False
+
+        t1, t2 = s1.transitions, s2.transitions
+        # not using sets -> deterministic but inefficient
+        # could use set to detect and slow only if detection?
+        for t in it.chain(t1.keys(), t2.keys()):
+            if (t in t1.keys()) != (t in t2.keys()):
+                return requirements[(s1, s2)] + [t] if return_cex else False
+
+        for t in t1.keys():
+            c1, c2 = t1[t], t2[t]
+            if (c1, c2) not in requirements:
+                requirements[(c1, c2)] = requirements[(s1, s2)] + [t]
+                to_check.put((c1, c2))
+
+    return None if return_cex else True
+
+
+def compare_automata(aut_1: DeterministicAutomaton, aut_2: DeterministicAutomaton, num_cex=10):
+    """
+    Finds cases of non-conformance between first and second automaton. This is done by performing RandomW equivalence
+    check. It is possible that number of found counterexamples is smaller than num_cex, as no counterexample will be a
+    suffix of a previously found counterexample.
+
+    Args:
+
+        aut_1: first automaton
+
+        aut_2: second automaton
+
+        num_cex: max. number of searches for counterexamples
+
+    Returns:
+
+        A list of input sequences that revel different behaviour on both automata. Counterexamples are sorted by length.
+    """
+    #
+    from aalpy.oracles import RandomWMethodEqOracle
+
+    type_map = {MooreMachine: MooreSUL, Dfa: DfaSUL, MealyMachine: MealySUL}
+    assert set(aut_1.get_input_alphabet()) == set(aut_2.get_input_alphabet())
+
+    input_al = aut_1.get_input_alphabet()
+    # larger automaton is used as hypothesis, as then test-cases will contain prefixes leading to states
+    # not in smaller automaton
+    base_automaton, test_automaton = (aut_1, aut_2) if aut_1.size < aut_2.size else (aut_2, aut_1)
+    base_sul = type_map[type(base_automaton)](base_automaton)
+
+    # compute prefixes for all states of the test automaton (needed for advanced eq. oracle)
+    for state in test_automaton.states:
+        if not state.prefix:
+            state.prefix = test_automaton.get_shortest_path(test_automaton.initial_state, state)
+
+    # setup  the eq oracle
+    eq_oracle = RandomWMethodEqOracle(input_al, base_sul, walks_per_state=min(100, len(input_al) * 10), walk_len=10)
+
+    found_cex = []
+    # to avoid near "infinite" loops due to while loop and set requirement
+    # that is, if you can only find 1 cex and all other cexs are suffixes of that cex, first while condition will never
+    # be reached
+    failsafe_counter = 0
+    failsafe_stopping = num_cex * 100
+    while len(found_cex) < num_cex or failsafe_counter == failsafe_stopping:
+        cex = eq_oracle.find_cex(test_automaton)
+        # if no counterexample can be found terminate the loop
+        if cex is None:
+            break
+        if cex not in found_cex:
+            found_cex.append(cex)
+        failsafe_counter += 1
+
+    found_cex.sort(key=len)
+
+    return found_cex
+
+
+class TestCaseWrapperSUL(SUL):
+    def __init__(self, sul):
+        super().__init__()
+        self.sul = sul
+        self.test_cases = []
+        self.test_case_inputs = None
+        self.test_case_outputs = None
+
+    def pre(self):
+        self.test_case_inputs = []
+        self.test_case_outputs = []
+        return self.sul.pre()
+
+    def post(self):
+        if self.test_case_inputs and self.test_case_outputs:
+            self.test_cases.append((tuple(self.test_case_inputs), tuple(self.test_case_outputs)))
+        return self.sul.post()
+
+    def step(self, letter):
+        output = self.sul.step(letter)
+        self.test_case_inputs.append(letter)
+        self.test_case_outputs.append(output)
+        return output
+
+
+def generate_test_cases(automaton: DeterministicAutomaton, oracle):
+    """
+    Uses parametrized eq. oracle to construct test cases on the automaton.
+    If automaton are big (200+ states), increase recursion depth if necessary (eg. sys.setrecursionlimit(10000)).
+
+    Args:
+
+        automaton: deterministic automaton that serves as a basis for test case generation
+        oracle: oracle that will construct test-cases and record inputs and outputs
+
+    Returns:
+
+        List of test cases, where each testcase is a tuple containing two elements, and input and an output sequance.
+    """
+    from copy import deepcopy
+
+    type_map = {MooreMachine: MooreSUL, Dfa: DfaSUL, MealyMachine: MealySUL}
+
+    automaton_copy = deepcopy(automaton)
+    base_sul = type_map[type(automaton_copy)](automaton_copy)
+
+    wrapped_sul = TestCaseWrapperSUL(base_sul)
+    oracle.sul = wrapped_sul
+    # no counterexamples can be found
+    cex = oracle.find_cex(automaton)
+    assert cex is None
+    return wrapped_sul.test_cases
+
+
+def statistical_model_checking(model, goals, max_num_steps, num_tests=105967):
+    """
+
+
+    Args:
+        model: model on which model checking is performed
+        goals: set of goal outputs
+        max_num_steps: bounded length of tests
+        num_tests: num of tests that will be performed
+
+    Returns:
+
+        num of tests containing element of goals set / num_tests
+    """
+
+    def compute_output_sequence(model, seq):
+        model.reset_to_initial()
+        observed_outputs = {model.step(i) for i in seq}
+        return observed_outputs
+
+    goal_reached = 0
+    inputs = model.get_input_alphabet()
+    for _ in range(num_tests):
+        test_sequence = choices(inputs, k=max_num_steps)
+        outputs = compute_output_sequence(model, test_sequence)
+        if goals & outputs:
+            goal_reached += 1
+
+    return goal_reached / num_tests
```

## aalpy/utils/__init__.py

```diff
@@ -1,10 +1,11 @@
-from .AutomatonGenerators import generate_random_dfa, generate_random_mealy_machine, generate_random_smm, \
-    generate_random_moore_machine, generate_random_markov_chain, dfa_from_state_setup, mealy_from_state_setup, \
-    moore_from_state_setup, generate_random_deterministic_automata
-from .AutomatonGenerators import generate_random_mdp, generate_random_ONFSM
-from .BenchmarkSULs import *
-from .DataHandler import DataHandler, CharacterTokenizer, DelimiterTokenizer, IODelimiterTokenizer
-from .FileHandler import save_automaton_to_file, load_automaton_from_file, visualize_automaton
-from .ModelChecking import model_check_experiment, mdp_2_prism_format, model_check_properties, get_properties_file, \
-    get_correct_prop_values, compare_automata, generate_test_cases, statistical_model_checking
-from .HelperFunctions import make_input_complete, convert_i_o_traces_for_RPNI
+from .AutomatonGenerators import generate_random_dfa, generate_random_mealy_machine, generate_random_smm, \
+    generate_random_moore_machine, generate_random_markov_chain, dfa_from_state_setup, mealy_from_state_setup, \
+    moore_from_state_setup, mdp_from_state_setup, smm_from_state_setup, generate_random_deterministic_automata
+from .AutomatonGenerators import generate_random_mdp, generate_random_ONFSM
+from .BenchmarkSULs import *
+from .DataHandler import DataHandler, CharacterTokenizer, DelimiterTokenizer, IODelimiterTokenizer
+from .FileHandler import save_automaton_to_file, load_automaton_from_file, visualize_automaton
+from .ModelChecking import model_check_experiment, mdp_2_prism_format, model_check_properties, get_properties_file, \
+    get_correct_prop_values, compare_automata, generate_test_cases, statistical_model_checking, \
+    bisimilar
+from .HelperFunctions import make_input_complete, convert_i_o_traces_for_RPNI
```

## Comparing `aalpy-1.3.1.dist-info/LICENCE.txt` & `aalpy-1.3.2.dist-info/LICENCE.txt`

 * *Files 15% similar despite different names*

```diff
@@ -1,22 +1,22 @@
-MIT License
-
-Copyright (c) 2022 TU Graz - SAL Dependable Embedded Systems Lab (DES Lab),
-                   Edi Muskardin <edi.muskardin@silicon-austria.com>
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+MIT License
+
+Copyright (c) 2023 TU Graz - SAL Dependable Embedded Systems Lab (DES Lab),
+                   Edi Muskardin <edi.muskardin@silicon-austria.com>
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
 SOFTWARE.
```

## Comparing `aalpy-1.3.1.dist-info/METADATA` & `aalpy-1.3.2.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: aalpy
-Version: 1.3.1
+Version: 1.3.2
 Summary: An active automata learning library
 Home-page: https://github.com/DES-Lab/AALpy
 Author: Edi Muskardin
 Author-email: edi.muskardin@silicon-austria.com
 License: MIT
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -35,15 +35,15 @@
 You can start learning automata in just a few lines of code.
 
 Whether you work with regular languages or you would like to learn models of 
 (black-box) reactive systems, AALpy supports a wide range of modeling formalisms, including 
 **deterministic**, **non-deterministic**, and **stochastic automata**. 
 
 <div align="center">
-	
+
 | **Automata Type** |                      **Supported Formalisms**                     | **Algorithms**        |                                                       **Features** |
 |-------------------|:-----------------------------------------------------------------:|-----------------------|-------------------------------------------------------------------:|
 | Deterministic     |                 DFAs <br /> Mealy Machines <br /> Moore Machines                 |      L* <br /> KV <br /> RPNI      | Seamless Caching <br /> Counterexample Processing <br /> 11 Equivalence Oracles  |
 | Non-Deterministic |                      ONFSM <br /> Abstracted ONFSM                      |        L*<sub>ONFSM</sub>       |                                 Size Reduction  Trough Abstraction |
 | Stochastic        | Markov Decision Processes <br /> Stochastic Mealy Machines <br /> Markov Chains | L*<sub>MDP</sub> <br /> L*<sub>SMM</sub> <br /> ALERGIA |               Counterexample Processing <br /> Exportable to PRISM format  <br /> Bindings to jALERGIA|
 
 </div>
@@ -51,15 +51,15 @@
 AALpy enables efficient learning by providing a large set of equivalence oracles, implementing various conformance testing strategies. Active learning 
 is mostly based on Angluin's [L* algorithm](https://people.eecs.berkeley.edu/~dawnsong/teaching/s10/papers/angluin87.pdf), for which AALpy supports a 
 selection of optimizations, including efficient counterexample processing caching. However, the recent addition of efficiently implemented 
 [KV](https://mitpress.mit.edu/9780262111935/an-introduction-to-computational-learning-theory/) algorithm
 requires (on average) much less interaction with the system under learning than L*.
 
 AALpy also includes **passive automata learning algorithms**, namely RPNI for deterministic and ALERGIA for stochastic models. Unlike active algorithms which learn by interaction with the system, passive learning algorithms construct a model based on provided data.
- 
+
 ## Installation
 
 Use the package manager [pip](https://pip.pypa.io/en/stable/) to install the latest release of AALpy:
 ```bash
 pip install aalpy
 ```
 To install current version of the master branch.
```

## Comparing `aalpy-1.3.1.dist-info/RECORD` & `aalpy-1.3.2.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,74 +1,74 @@
 aalpy/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aalpy/paths.py,sha256=HuWZtzw_pfWFqSVwBffMn6z3NnFUlrm3b35kNpI05TE,460
-aalpy/SULs/AutomataSUL.py,sha256=P7gc0X9JrC3PFnFRS2bJE3g9Fkm-YggRnPnpEEMcjQE,3561
-aalpy/SULs/PyMethodSUL.py,sha256=zjXzx8EPjN-hcx628P3zOO1jXqLjLXmjpEeDEYm2s3Q,1745
-aalpy/SULs/RegexSUL.py,sha256=gDP4AAATUfs-LSgbJtR9vfND-TBjnAZ6RjOC7gCoIGc,943
-aalpy/SULs/TomitaSUL.py,sha256=jiqlx_pBj3XkPVlo3FkNTQoVcmi0IA7iXjg73LP3CaI,1546
-aalpy/SULs/__init__.py,sha256=IPwbII48Fi1iqkx-gJnWQr2M58_US-ABMrhl46o_Kfc,218
-aalpy/automata/Dfa.py,sha256=9n7D4oRdzduNFuA4KLrPOYiiULAyGxnI_wEqzgrwHkI,2024
-aalpy/automata/MarkovChain.py,sha256=ZGPRezBk1FgwQ0Iz7O4ZsAprpw0YbvfM5MfPSotTTOs,1698
-aalpy/automata/Mdp.py,sha256=CXSpc7jBeZVc8y_dauZR1cr3-KU3UVdv5V5lYxbrs5o,1766
-aalpy/automata/MealyMachine.py,sha256=HjZ0ZN2hRmrwGWvtPcO_Wi9K2UadlH033LmCtRnr5nE,1392
-aalpy/automata/MooreMachine.py,sha256=Qpdz3-mvpN_14fIj9_g6jZLhEi60O3NbcnS-M7YftAE,2049
-aalpy/automata/Onfsm.py,sha256=cjxlYhq0083RGv6yD9xoJyRQb_BgTv2PwwT_8eNnebs,2527
-aalpy/automata/StochasticMealyMachine.py,sha256=Shd-woMG3Kludg3eA3iXL-bbmLh0AWoU1PjQdiDTslA,3674
-aalpy/automata/__init__.py,sha256=wa1bPZP6VBFKp8bzXpW4-DtN0dqCuWG2DKo01xBTBKM,335
-aalpy/base/Automaton.py,sha256=OCfp7trWkDFi0LY2NlAq73MxSYsp-jl4AJcNzD_JPHo,16276
-aalpy/base/CacheTree.py,sha256=3z0yZ_Ce2SGoXDpeNDzjZnZU1sExNybABj1yvMas62Q,5736
-aalpy/base/Oracle.py,sha256=57eNkzlrWE6ETKZxjZfGuFI-kzjqXkMYAjgoKcHglrg,1229
-aalpy/base/SUL.py,sha256=ywlHPQSh5CcnXzkxUrxIMxJr03dou8u2RIBjkJaaJfk,4120
-aalpy/base/__init__.py,sha256=YnCibXYW2b_bz5HbIqeMUWo0ujX8gUoYQF2YpCXHVFc,124
-aalpy/learning_algs/__init__.py,sha256=yqrXRE3yTEnuxNqdbmRzZC4BQdFdedC_SBfT14SDLOM,521
-aalpy/learning_algs/deterministic/ClassificationTree.py,sha256=LCeIJFeqhIJ-EwbHAvuyCBTg8Ul1izmq-Nbvtk3J5kk,16383
-aalpy/learning_algs/deterministic/CounterExampleProcessing.py,sha256=0GChlsP15AkW_9HOHBM5dfTLuF9zUb58umUiCbn_UdI,3261
-aalpy/learning_algs/deterministic/KV.py,sha256=0-Qca3ydJ5c1KD1MouEWEiPEh2CPnLAOdefF0UUn_yk,5797
-aalpy/learning_algs/deterministic/LStar.py,sha256=o-UyCDGTZgJkogbPLTFDQPjj8tbYVWxoYUYOXBHsi7c,7555
-aalpy/learning_algs/deterministic/ObservationTable.py,sha256=oR2zGooJgka3vtGrOxqa3xQB7Bfh21wvi5QshZe9Fwk,8316
+aalpy/paths.py,sha256=6aIoWlVEEsMnWCbBxQVKkkEKbUvC4GnH1wkBrQIJZzQ,448
+aalpy/SULs/AutomataSUL.py,sha256=ph6kE22e1UmInPdmdbTfA3BT8Yfdbg7VB_rSsCkvYBg,3395
+aalpy/SULs/PyMethodSUL.py,sha256=Bp-ReQnT9K40yZdvwGXl3iiQbRbKUvB-ChvpB5Sd0JM,1677
+aalpy/SULs/RegexSUL.py,sha256=b4QfyzcnE2M92CN95cEozLj8sWokWZdia-K6ajCiFrk,906
+aalpy/SULs/TomitaSUL.py,sha256=O3xaGdkXaFjazTM9GsHovxROfaZ5dwGuCzSdg1gxRgU,1478
+aalpy/SULs/__init__.py,sha256=c2iZzPvo-DCcw4U64K2VGDr-ie5X5Xigr3xtzptdyRc,215
+aalpy/automata/Dfa.py,sha256=_ucMomFY2E_k5O9ONhEKy61jEc4RyErdC99bm4XWFJA,2097
+aalpy/automata/MarkovChain.py,sha256=YE9t80X9xUDZYyvVW4eIdHJ_G76jiLf9qvDmvzG1m6Q,1638
+aalpy/automata/Mdp.py,sha256=28tK7RrfjTIALO_I-mzwTUTo4yBvOKeVV-CN7fGht1c,2361
+aalpy/automata/MealyMachine.py,sha256=0MnYKsjReyJQXp5gbTIeCVpjA6OsW4s5lZZ92mY_YG0,1483
+aalpy/automata/MooreMachine.py,sha256=rK96sfwnUCKtk1mW2HRV21K6rPiE6wb_InfWzSkTf9U,2128
+aalpy/automata/Onfsm.py,sha256=ffWEwe_Tb9UhdMlpb1Zw1qAy4rwpRWy-txGk3LuhcGg,2428
+aalpy/automata/StochasticMealyMachine.py,sha256=gf22M2OrKIUr8iAO2So2J6wY8zxI4YUe9nBMfZ4ElBE,4212
+aalpy/automata/__init__.py,sha256=ShVr9L_hduiWbgui1-ezNwWsBIcNb01H8rhfnZ1p4_Q,328
+aalpy/base/Automaton.py,sha256=AfM63WcnLm8SthE5Kn0RS7NVIm6IrByPfGMNEV6EiK0,15912
+aalpy/base/CacheTree.py,sha256=VMOjXlhHZZn1n-je8nBUcAidEDzNwLtlJsuEmiFZ4lM,5567
+aalpy/base/Oracle.py,sha256=NQH6EqMAATISHaegIkiXEfRA8EesaM1mFwK3dFcfGlU,1178
+aalpy/base/SUL.py,sha256=8wiAJkfutmKrJPFHOyf-QhditQJID0j8udRMxHCDRc0,3978
+aalpy/base/__init__.py,sha256=n9-0MEorB3OczbK-p90_NVa4DUgy7QlYsiODZeOsc-s,121
+aalpy/learning_algs/__init__.py,sha256=7_x040Wrb1Gi2o9EhFus2divJcUzmEvYnN_8IzPVdM4,576
+aalpy/learning_algs/deterministic/ClassificationTree.py,sha256=hLqDig5Kr6KAppiwjx6Ko8k8X0kp4cAXvlZXJHq2r1Y,15997
+aalpy/learning_algs/deterministic/CounterExampleProcessing.py,sha256=sTeCvv1mkXpygG8iuKS-_63GpAsMJALhFXU38LX73vA,3160
+aalpy/learning_algs/deterministic/KV.py,sha256=e68-0u6FfD9vxOvBcVFjyCQOYSuYAlzmt5O47klW6Cg,5639
+aalpy/learning_algs/deterministic/LStar.py,sha256=pRqyCjk0PnmqI1SxodiJIJQtycC2gXlWGGSn-Sf5n4A,7374
+aalpy/learning_algs/deterministic/ObservationTable.py,sha256=e8yl7eRHpXdF-BbpNWWUXZn4GP-9-vy8_W7gInzWGUo,8097
 aalpy/learning_algs/deterministic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aalpy/learning_algs/deterministic_passive/GeneralizedStateMerging.py,sha256=r9H2UVqkd3Yy4EZD4cyzYavmQLugFiu7i_KV1SYMWqQ,5859
-aalpy/learning_algs/deterministic_passive/RPNI.py,sha256=nTkz-nhAFFbpfoZK6TWsLrk6XUwvicAFyG5Q-6fKNFI,7843
+aalpy/learning_algs/deterministic_passive/GeneralizedStateMerging.py,sha256=722NaCDO8Z-PkomCdepKkHMpXEA54rgPhXk33efXzOI,3954
+aalpy/learning_algs/deterministic_passive/RPNI.py,sha256=28_Yeh0EYLZxvmYtlZffsCbvg5a00ROgrMNpizNa3_c,7619
 aalpy/learning_algs/deterministic_passive/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aalpy/learning_algs/deterministic_passive/active_RPNI.py,sha256=4V3qFjwgAeW2YD65Q-lOftLD4vN29_Zp2PrDBa0obtw,3789
-aalpy/learning_algs/deterministic_passive/rpni_helper_functions.py,sha256=B74DQDfB1FeFgy_ySV91la7gQHKiPYhVKDULqYdVHng,9298
-aalpy/learning_algs/non_deterministic/AbstractedOnfsmLstar.py,sha256=AMUGrQ7OCXnaX5LjnDjQJybaK8WVtG4EI4rTU2N0V0Q,6426
-aalpy/learning_algs/non_deterministic/AbstractedOnfsmObservationTable.py,sha256=8RQA2st0UsxOKkKuscKQg8S6wzgE1yvo4tfkKE37Gys,15947
-aalpy/learning_algs/non_deterministic/NonDeterministicSULWrapper.py,sha256=LnpY_uL7d-fIt2FEEy_prOGsYUGJrLf-E4gqxe83JV0,659
-aalpy/learning_algs/non_deterministic/OnfsmLstar.py,sha256=jBWVMaSYLv-rlKv2MnVhzgHVTPRsPmDlYm4RKn1nCFk,5020
-aalpy/learning_algs/non_deterministic/OnfsmObservationTable.py,sha256=87kyd4YvFQJc7AV86fAKDSTmrxb8dWYtr7Gkh_ht5r0,7032
-aalpy/learning_algs/non_deterministic/TraceTree.py,sha256=6aSS1344Wa3Aq_tQZLVIB3OL7M_rWGJye_1hx6qEdsA,6011
+aalpy/learning_algs/deterministic_passive/active_RPNI.py,sha256=f7WakEOC34mQn6bqPzwdC78g1h9zW-snSdpoFCp9rCE,1818
+aalpy/learning_algs/deterministic_passive/rpni_helper_functions.py,sha256=B4rMmozRUUGexTUIQM9s7pj0pJ3Slbcj4oZ8nO6EzFY,9033
+aalpy/learning_algs/non_deterministic/AbstractedOnfsmLstar.py,sha256=17z73iXQrYwMwDhRprqnJs9AdMg_BVWUmgGeGKYsaN8,6280
+aalpy/learning_algs/non_deterministic/AbstractedOnfsmObservationTable.py,sha256=IFNwi9wALmS4So6PUpz8WVaFi7JUBDytnMijME-Wg2w,15505
+aalpy/learning_algs/non_deterministic/NonDeterministicSULWrapper.py,sha256=uUzaFJQ8K6GwsWyj6PaunMaL08JVOkYNhHoR1jiUnVQ,634
+aalpy/learning_algs/non_deterministic/OnfsmLstar.py,sha256=G_nvZBrNAO-W5GoDBRRzUj8Cm9myct-0n1W5j76Wuz4,4870
+aalpy/learning_algs/non_deterministic/OnfsmObservationTable.py,sha256=I64eN2XQ0i7vdJABSyekzUkeZL0zM7kFbTpma285zp4,6828
+aalpy/learning_algs/non_deterministic/TraceTree.py,sha256=N_DpH2NTFk57wtSATAEbq_7mHNJ6zjPzkXMtsIo9EOE,5808
 aalpy/learning_algs/non_deterministic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aalpy/learning_algs/stochastic/DifferenceChecker.py,sha256=PXwzjCfGJhx4iXoeCbYSOIZ6OHwWyqV-fC0BI1x1Yw0,7764
-aalpy/learning_algs/stochastic/SamplingBasedObservationTable.py,sha256=piQNN4NmfEfeBG-zymlO-rtePvW0P6CX5cm0MwdyRws,25536
-aalpy/learning_algs/stochastic/StochasticCexProcessing.py,sha256=BlVwtzCfkd3fuRsPbqjEWw_SBQP2URjX7GjDuT3BAxY,3426
-aalpy/learning_algs/stochastic/StochasticLStar.py,sha256=k9S8BFtLmo2ukMzVQh5Wx6e33apB9qP1KHDt3xjfaHM,10167
-aalpy/learning_algs/stochastic/StochasticTeacher.py,sha256=9lSGe4ABHlJkhj4QY7xEuIDnw2VSx9dHyO-_jTJCAuk,13239
+aalpy/learning_algs/stochastic/DifferenceChecker.py,sha256=A6XBlZYbqv2omOMoGdaFS9q1E9LvHfxl6-jnjiX7t4U,7587
+aalpy/learning_algs/stochastic/SamplingBasedObservationTable.py,sha256=Ur-MD9DlbsIISMzeuAPmNmW-ExB4KJZb_2epOHJasak,24894
+aalpy/learning_algs/stochastic/StochasticCexProcessing.py,sha256=CERL0cmypQ6PuS2LwgQfBBoYeA-OHaPTtm2bedRdf-c,3296
+aalpy/learning_algs/stochastic/StochasticLStar.py,sha256=KImK-9pM9aztqf2GYbuuNxcBB5aJw_sIMuOePZ66ef0,9953
+aalpy/learning_algs/stochastic/StochasticTeacher.py,sha256=eyEA3M6lA5vEGJ4UGZViG3bsBm4Ts-nNs61t12ZM6xc,12846
 aalpy/learning_algs/stochastic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aalpy/learning_algs/stochastic_passive/ActiveAleriga.py,sha256=qXJd5Fd9wcPRW3gxxHLBGv73l-c0PrSEI9Kbzypk81c,2893
-aalpy/learning_algs/stochastic_passive/Alergia.py,sha256=Es1QNMRkA6zGi6x7oiHC9-WJeDDOCuQtFEBwfO2dTNE,11065
-aalpy/learning_algs/stochastic_passive/CompatibilityChecker.py,sha256=utGblZVS6ZEF5K4x416Fg9B82VFVGM6kwy92L8SFQZk,1178
-aalpy/learning_algs/stochastic_passive/FPTA.py,sha256=-ko9bb85leZNQdD2C51ivOnNULwivfGTMyP4k6SOyuk,3990
+aalpy/learning_algs/stochastic_passive/ActiveAleriga.py,sha256=apEGmY3conflLdllGkrCVXdLDnr8d_GqQhA4wuSYNv4,2805
+aalpy/learning_algs/stochastic_passive/Alergia.py,sha256=iX1RQXmOa3qc_Ss82g1aaYtIcd7_g3ufwv4c0714AP0,10794
+aalpy/learning_algs/stochastic_passive/CompatibilityChecker.py,sha256=dG2r34tTC6lzT5fxYYpN6Ylgtd2SaZtJL9DrMYrN4yo,1847
+aalpy/learning_algs/stochastic_passive/FPTA.py,sha256=VedP5myHywGg6siPdCtg46DGuHXbUShd5s1yjU3yt7k,3984
 aalpy/learning_algs/stochastic_passive/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-aalpy/oracles/BreadthFirstExplorationEqOracle.py,sha256=yI3ITVkuZ2Hzs65mStBmVf4HxvcLS7fy7sTqYqbJCJ0,1433
-aalpy/oracles/CacheBasedEqOracle.py,sha256=dNwoi4Lzxvuh1GwGe9Q9YFdFWdIxwL-cLRkUvBeiPcg,3135
+aalpy/oracles/BreadthFirstExplorationEqOracle.py,sha256=k2RWXIFFbBgeL1G2aCCR-FyTR7EnRr9qhICl8nVSF9A,1382
+aalpy/oracles/CacheBasedEqOracle.py,sha256=hi5eQiOf93NbyUdsB7oCaqkiDycQ4gNGpqJqKquX3I0,3038
 aalpy/oracles/PacOracle.py,sha256=7Wg4_WeFis2qw-JKlgv2WrFcbCcCdCSUZIVPWsi55LI,1670
-aalpy/oracles/RandomWalkEqOracle.py,sha256=G60D_tPQjFI_FM3BbSuMBODkd00DjLGNG0LWJAEY0Pg,2986
-aalpy/oracles/RandomWordEqOracle.py,sha256=evhAt6i5aCQnzzR2SRX82idXQs9W8o8WbY-PxmhT7J0,3438
-aalpy/oracles/StatePrefixEqOracle.py,sha256=6dq60ZRLoHQD_OXX3UN3ym2h2jSV4jhCXWvh8DYmbs4,2816
-aalpy/oracles/TransitionFocusOracle.py,sha256=xuZKrHAkE_lMuaH-RVABanewuhqZANbusGof5o8gXfU,2023
-aalpy/oracles/UserInputEqOracle.py,sha256=kwpFdX3CcyQuQTWWHpwoGcZXr8-C5AsFNOr__qvDsr0,2636
-aalpy/oracles/WMethodEqOracle.py,sha256=UgKNee-oPz7xL72iNMGJha0B07vpHqILB-V-o5x6utk,4673
-aalpy/oracles/__init__.py,sha256=MkWtwSKOrLTvG6EPYzchvFBkRtVlLdoYD_ZqjLgiB_Y,641
-aalpy/oracles/kWayStateCoverageEqOracle.py,sha256=tXgTEXDpkUFibPeBSo_m8EMqQgNe-4wvSbs8k4q4Z0k,2676
-aalpy/oracles/kWayTransitionCoverageEqOracle.py,sha256=5OrwPikjFc_KnQGTTQVpSYNah1XDWpsBAJ2sN5dYZ6g,6429
-aalpy/utils/AutomatonGenerators.py,sha256=lE1OIDDm6JUbDvKcyOLaWjPqJJmusAqUkaSME5une3w,21072
-aalpy/utils/BenchmarkSULs.py,sha256=T-VgUyqBME7s8OxrRMlZenSzw5CpXehAbGPp6m897cQ,11845
-aalpy/utils/DataHandler.py,sha256=qmvn7RGKeiMkufrF-ccCYWsRrhO5dX9ZfdTuSTMSNZ0,2226
-aalpy/utils/FileHandler.py,sha256=G6woywmTA_MXOa_p0zT4k7jFgHV4N1H2e04IzgPTcLU,13312
-aalpy/utils/HelperFunctions.py,sha256=aJCx3LQUW3qxGwj51-cFC1wkXxDVo9rmwmeeE5KNBOY,10371
-aalpy/utils/ModelChecking.py,sha256=55SxqPgZsQvuKa7i2TYM5Pao3_R-UuP03XfKXmgiTmo,15343
-aalpy/utils/__init__.py,sha256=XIqrVkZX3X-ZSdAKK3Z0UGEERxxyilmV17Ovz9DK7HA,888
-aalpy-1.3.1.dist-info/LICENCE.txt,sha256=ypkqcoqn1UXjFwSXbuH4CpAROUNAkLtFJ9-qoBpsvlc,1202
-aalpy-1.3.1.dist-info/METADATA,sha256=F7wj3s6EAVC1ko5kf6bG3b1HfqA_6CTE80JQ7V2GbM4,10112
-aalpy-1.3.1.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-aalpy-1.3.1.dist-info/top_level.txt,sha256=Y20GlNzDowGtNjqyE09gUKkpPEoWwOzqrC_RzAAX0Hg,6
-aalpy-1.3.1.dist-info/RECORD,,
+aalpy/oracles/RandomWalkEqOracle.py,sha256=DgPnuYYEmnp-3BeyKiifv44AyMS65leKxf1-vYyY32I,2898
+aalpy/oracles/RandomWordEqOracle.py,sha256=KQHxwpet4LpL3d1p7fFAi-aeYkl_CYpW1bZs9tgwDuo,3343
+aalpy/oracles/StatePrefixEqOracle.py,sha256=NfgZ_8sTIaLf7nqMf814GS-xbHC9YTsVKMBuBnP4m7Q,2738
+aalpy/oracles/TransitionFocusOracle.py,sha256=DwMSyqKCAD5dqYGEH-WbGe4IVtS_fIm7xa-gAACexvw,1970
+aalpy/oracles/UserInputEqOracle.py,sha256=1Evzm_98E4J0i-U8TJQRHLQI6vaBquvSIcyWYx3Ponc,2567
+aalpy/oracles/WMethodEqOracle.py,sha256=g0n-zI7SyzBp6WxlQnpRo9bakVtTJt1pUyBIiZy9Yqk,4542
+aalpy/oracles/__init__.py,sha256=Fo3ihh6WVoDxfNBOusxGavoJFqFBFP4xtq2xiP_3gBs,630
+aalpy/oracles/kWayStateCoverageEqOracle.py,sha256=oeDRw9TD_A43XiJTG37eD9MoSRtHJa1fzeqmyVvvqR8,2814
+aalpy/oracles/kWayTransitionCoverageEqOracle.py,sha256=k1WkyNN_0EjYII8tEicCmXfNcgXMuT1V1B0MRdoL8WA,6264
+aalpy/utils/AutomatonGenerators.py,sha256=FbQXPmVHD0r8KpIA-tJS93Q_kZCfVrGTUr1MLiamqhs,21678
+aalpy/utils/BenchmarkSULs.py,sha256=ukY6NdIVkV5WuqYwbkg43xfqxwWFzkjOGYZlntS7sgQ,17637
+aalpy/utils/DataHandler.py,sha256=jscODbOQRuWMKtoR7ou6RKy8hNe0aWCI3lor_M8Viq4,2153
+aalpy/utils/FileHandler.py,sha256=6097FiojI--FD4SYfGGBQp6K75cbNqFIb3lyHLT_D30,13013
+aalpy/utils/HelperFunctions.py,sha256=8UHTyaPHjddeiQ6N5izk7D7vxAp66GZ0AgFWHHoVrd4,10073
+aalpy/utils/ModelChecking.py,sha256=r6_1E37HQqDGODddZjbyQVtSRhHFeJJSmAdxid2tlqQ,15817
+aalpy/utils/__init__.py,sha256=m8fSGBOJd8bJTS6XVdbxvihnbZsXLAz4J-gQdKaAAr0,939
+aalpy-1.3.2.dist-info/LICENCE.txt,sha256=hmCaiFE4zWi9MU9UXcA0EzF92Oq0YuyXpa2TGZMqLKo,1181
+aalpy-1.3.2.dist-info/METADATA,sha256=Lawxxy9awwLMEvblQM3vfVd-5WTNwNs3N0y_YKJBjX8,10110
+aalpy-1.3.2.dist-info/WHEEL,sha256=D1Wh14kWDxPnrM-5t_6UCB-UuQNrEODtRa3vF4OsvQY,97
+aalpy-1.3.2.dist-info/top_level.txt,sha256=Y20GlNzDowGtNjqyE09gUKkpPEoWwOzqrC_RzAAX0Hg,6
+aalpy-1.3.2.dist-info/RECORD,,
```

