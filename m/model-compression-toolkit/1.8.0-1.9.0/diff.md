# Comparing `tmp/model_compression_toolkit-1.8.0.tar.gz` & `tmp/model_compression_toolkit-1.9.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "model_compression_toolkit-1.8.0.tar", last modified: Wed Feb  8 14:05:57 2023, max compression
+gzip compressed data, was "model_compression_toolkit-1.9.0.tar", last modified: Mon Jun 19 07:13:13 2023, max compression
```

## Comparing `model_compression_toolkit-1.8.0.tar` & `model_compression_toolkit-1.9.0.tar`

### file list

```diff
@@ -1,559 +1,529 @@
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.717884 model_compression_toolkit-1.8.0/
--rw-r--r--   0 runner    (1001) docker     (123)    10174 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/LICENSE.md
--rw-r--r--   0 runner    (1001) docker     (123)    12036 2023-02-08 14:05:57.717884 model_compression_toolkit-1.8.0/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    10198 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/README.md
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.645884 model_compression_toolkit-1.8.0/model_compression_toolkit/
--rw-r--r--   0 runner    (1001) docker     (123)     4229 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.645884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2943 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/analyzer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.649884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/
--rw-r--r--   0 runner    (1001) docker     (123)     1511 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.649884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/back2framework/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/back2framework/base_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/base_substitutions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.649884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2588 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/base_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     6864 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/histogram_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     3900 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/mean_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     5219 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     7929 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/statistics_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/statistics_collector_generator.py
--rw-r--r--   0 runner    (1001) docker     (123)     4027 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     4017 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/data_loader.py
--rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/defaultdict.py
--rw-r--r--   0 runner    (1001) docker     (123)    22674 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/framework_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6430 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.649884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/fusion/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/fusion/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5411 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/fusion/layer_fusing.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.653884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/
--rw-r--r--   0 runner    (1001) docker     (123)      773 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    28855 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/base_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    18516 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/base_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     3733 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/edge.py
--rw-r--r--   0 runner    (1001) docker     (123)     2922 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/functional_node.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     4732 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/graph_matchers.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     5128 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/graph_searches.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.653884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3892 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     2612 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
--rw-r--r--   0 runner    (1001) docker     (123)     2470 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/cut.py
--rw-r--r--   0 runner    (1001) docker     (123)    17057 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
--rw-r--r--   0 runner    (1001) docker     (123)     3961 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
--rw-r--r--   0 runner    (1001) docker     (123)     7175 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     9272 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py
--rw-r--r--   0 runner    (1001) docker     (123)     1723 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/immutable.py
--rw-r--r--   0 runner    (1001) docker     (123)     4673 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/logger.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.653884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/
--rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3091 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/base_graph_filter.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2210 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/base_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     3706 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/edge_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1773 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/function.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     2745 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/node_matcher.py
--rwxr-xr-x   0 runner    (1001) docker     (123)     1111 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/walk_matcher.py
--rw-r--r--   0 runner    (1001) docker     (123)     1217 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/memory_computation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.657884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6804 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2222 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.657884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4297 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py
--rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py
--rw-r--r--   0 runner    (1001) docker     (123)     7100 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py
--rw-r--r--   0 runner    (1001) docker     (123)     1602 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py
--rw-r--r--   0 runner    (1001) docker     (123)    19342 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py
--rw-r--r--   0 runner    (1001) docker     (123)     8046 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     6773 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
--rw-r--r--   0 runner    (1001) docker     (123)    34634 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.657884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/search_methods/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    15272 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
--rw-r--r--   0 runner    (1001) docker     (123)    25266 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
--rw-r--r--   0 runner    (1001) docker     (123)     6333 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
--rw-r--r--   0 runner    (1001) docker     (123)     1324 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/model_builder_mode.py
--rw-r--r--   0 runner    (1001) docker     (123)     5012 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/model_collector.py
--rw-r--r--   0 runner    (1001) docker     (123)     1209 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/model_validation.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.657884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/
--rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    17948 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/actions.py
--rw-r--r--   0 runner    (1001) docker     (123)     1811 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/edit_network.py
--rw-r--r--   0 runner    (1001) docker     (123)     3149 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/node_filters.py
--rw-r--r--   0 runner    (1001) docker     (123)     1769 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/node_prior_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.661884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3034 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/core_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1482 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/debug_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     4427 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
--rw-r--r--   0 runner    (1001) docker     (123)    16262 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3462 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_analyzer.py
--rw-r--r--   0 runner    (1001) docker     (123)     7244 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     2335 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4085 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.661884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/
--rw-r--r--   0 runner    (1001) docker     (123)     1608 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    16499 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     2939 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py
--rw-r--r--   0 runner    (1001) docker     (123)     7294 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
--rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
--rw-r--r--   0 runner    (1001) docker     (123)     8479 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     4560 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
--rw-r--r--   0 runner    (1001) docker     (123)     4323 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
--rw-r--r--   0 runner    (1001) docker     (123)    41697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
--rw-r--r--   0 runner    (1001) docker     (123)     5095 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
--rw-r--r--   0 runner    (1001) docker     (123)     9684 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     7874 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
--rw-r--r--   0 runner    (1001) docker     (123)     2963 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
--rw-r--r--   0 runner    (1001) docker     (123)     3626 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantize_node.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.661884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2362 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2786 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    14176 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
--rw-r--r--   0 runner    (1001) docker     (123)     5166 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)    10673 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     7469 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/similarity_analyzer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.665884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3429 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     6103 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)    10260 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     5584 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.665884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/apply_substitutions.py
--rw-r--r--   0 runner    (1001) docker     (123)     6434 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (123)     5880 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (123)     9969 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     9105 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)     2296 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
--rw-r--r--   0 runner    (1001) docker     (123)     4866 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    10978 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (123)    26858 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     2625 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     3412 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (123)     4240 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/weights_activation_split.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.665884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/
--rw-r--r--   0 runner    (1001) docker     (123)     1463 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/current_tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     2319 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/fusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     9107 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/op_quantization_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3040 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/operators.py
--rw-r--r--   0 runner    (1001) docker     (123)     8783 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/target_platform_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     1375 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/target_platform_model_component.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.669884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/
--rw-r--r--   0 runner    (1001) docker     (123)     1428 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8771 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/attribute_filter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2046 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/current_tpc.py
--rw-r--r--   0 runner    (1001) docker     (123)     4001 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/layer_filter_params.py
--rw-r--r--   0 runner    (1001) docker     (123)     6015 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/operations_to_layers.py
--rw-r--r--   0 runner    (1001) docker     (123)     9490 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/target_platform_capabilities.py
--rw-r--r--   0 runner    (1001) docker     (123)     1013 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/target_platform_capabilities_component.py
--rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/user_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.669884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6371 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/final_config_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5955 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/nn_visualizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    20094 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/tensorboard_writer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4233 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.669884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.673884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/
--rw-r--r--   0 runner    (1001) docker     (123)      808 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2231 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2439 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/float_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3837 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/instance_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    15658 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     7162 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    15305 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/model_gradients.py
--rw-r--r--   0 runner    (1001) docker     (123)     2476 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     5007 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/default_framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.673884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.673884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3952 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)     3655 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (123)     3168 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2478 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     5610 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
--rw-r--r--   0 runner    (1001) docker     (123)     5937 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    26902 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)     3827 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
--rw-r--r--   0 runner    (1001) docker     (123)     2354 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
--rw-r--r--   0 runner    (1001) docker     (123)     3188 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)     5542 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     7714 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)    10776 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1623 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     1462 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1814 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (123)    27514 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/keras_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/keras_model_validation.py
--rw-r--r--   0 runner    (1001) docker     (123)     3936 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/keras_node_prior_info.py
--rw-r--r--   0 runner    (1001) docker     (123)     8881 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/kpi_data_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.673884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1900 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/mixed_precision/set_layer_to_bitwidth.py
--rw-r--r--   0 runner    (1001) docker     (123)    18051 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.677884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/base_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6175 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3132 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/input_layer_quantize_transform.py
--rw-r--r--   0 runner    (1001) docker     (123)     4524 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.677884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2837 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/quantization_config_factory.py
--rw-r--r--   0 runner    (1001) docker     (123)     6854 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10914 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_quantize_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     7621 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.677884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2639 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/common.py
--rw-r--r--   0 runner    (1001) docker     (123)    11418 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/connectivity_handler.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.677884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7906 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
--rw-r--r--   0 runner    (1001) docker     (123)     2760 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py
--rw-r--r--   0 runner    (1001) docker     (123)     2107 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py
--rw-r--r--   0 runner    (1001) docker     (123)     2408 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
--rw-r--r--   0 runner    (1001) docker     (123)     5951 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/node_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     8109 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/reader.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.677884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3055 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/tf_tensor_numpy.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.677884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/visualization/
--rw-r--r--   0 runner    (1001) docker     (123)      698 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/visualization/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.681884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.681884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/
--rw-r--r--   0 runner    (1001) docker     (123)      813 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2279 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     3414 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     1680 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5152 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    17808 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
--rw-r--r--   0 runner    (1001) docker     (123)    12685 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.681884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5773 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3704 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     2442 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     4219 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/default_framework_info.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.681884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3038 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
--rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2162 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
--rw-r--r--   0 runner    (1001) docker     (123)     4804 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
--rw-r--r--   0 runner    (1001) docker     (123)     5809 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)    38525 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1953 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
--rw-r--r--   0 runner    (1001) docker     (123)     5563 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
--rw-r--r--   0 runner    (1001) docker     (123)     3492 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
--rw-r--r--   0 runner    (1001) docker     (123)     2911 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
--rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
--rw-r--r--   0 runner    (1001) docker     (123)     9833 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
--rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
--rw-r--r--   0 runner    (1001) docker     (123)     1375 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
--rw-r--r--   0 runner    (1001) docker     (123)     1616 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
--rw-r--r--   0 runner    (1001) docker     (123)     8782 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/kpi_data_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11340 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     1678 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/set_layer_to_bitwidth.py
--rw-r--r--   0 runner    (1001) docker     (123)    25965 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/pytorch_implementation.py
--rw-r--r--   0 runner    (1001) docker     (123)     3245 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
--rw-r--r--   0 runner    (1001) docker     (123)    17587 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6481 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     4464 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    12011 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/graph_builders.py
--rw-r--r--   0 runner    (1001) docker     (123)     1789 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/node_holders.py
--rw-r--r--   0 runner    (1001) docker     (123)     5801 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/reader.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/statistics_correction/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3256 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
--rw-r--r--   0 runner    (1001) docker     (123)     2959 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/utils.py
--rw-r--r--   0 runner    (1001) docker     (123)    23032 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.685884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1406 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4176 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6029 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3797 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     3213 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6565 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4273 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     3811 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7867 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4570 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4148 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/
--rw-r--r--   0 runner    (1001) docker     (123)      721 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8057 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     4583 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4161 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7867 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5325 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4666 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/
--rw-r--r--   0 runner    (1001) docker     (123)      721 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8057 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5429 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4679 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.689884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7864 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5325 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4666 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/tpc_pytorch.py
--rw-r--r--   0 runner    (1001) docker     (123)     3459 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/get_target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1414 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1924 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7859 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     5331 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4673 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1402 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1930 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     5923 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     3150 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     2894 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/latest/
--rw-r--r--   0 runner    (1001) docker     (123)     1397 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/latest/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1925 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/target_platform_capabilities.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/
--rw-r--r--   0 runner    (1001) docker     (123)      717 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     7770 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tp_model.py
--rw-r--r--   0 runner    (1001) docker     (123)     6066 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tpc_keras.py
--rw-r--r--   0 runner    (1001) docker     (123)     4937 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tpc_pytorch.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/
--rw-r--r--   0 runner    (1001) docker     (123)     1082 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.693884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      699 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     6022 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2989 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      699 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     1600 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3301 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     2892 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3925 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3006 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/fakely_quant_tflite_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     8148 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/int8_tflite_exporter.py
--rw-r--r--   0 runner    (1001) docker     (123)     3338 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/tflite_export_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/
--rw-r--r--   0 runner    (1001) docker     (123)     1314 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/common/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2876 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/common/exporter_get_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2494 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6745 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2077 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     3310 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2041 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     6216 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2086 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
--rw-r--r--   0 runner    (1001) docker     (123)     1464 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.697884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.701884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    10978 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_config.py
--rw-r--r--   0 runner    (1001) docker     (123)      687 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     2053 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_graph.py
--rw-r--r--   0 runner    (1001) docker     (123)     3389 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_quantizer_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    15210 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_training.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.701884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/gptq_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     4064 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/gptq_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    14276 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/gptq_training.py
--rw-r--r--   0 runner    (1001) docker     (123)     4684 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/graph_info.py
--rw-r--r--   0 runner    (1001) docker     (123)    14341 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.701884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      815 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2925 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/config_factory.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.701884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2417 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/base_quantizer_gptq_config.py
--rw-r--r--   0 runner    (1001) docker     (123)    10300 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/weight_quantizer_gptq_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     1913 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/kernel_functions.py
--rw-r--r--   0 runner    (1001) docker     (123)     6347 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.701884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14824 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11462 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (123)     2026 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3187 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/gptq_graph_info.py
--rw-r--r--   0 runner    (1001) docker     (123)     2719 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/gptq_loss.py
--rw-r--r--   0 runner    (1001) docker     (123)     4729 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/gptq_model_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)    10835 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/gptq_training.py
--rw-r--r--   0 runner    (1001) docker     (123)    13255 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2574 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/gptq_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5844 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4232 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/quantizer_wrapper.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     4436 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/ste_weights_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5484 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9681 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     8364 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/pytorch/quantization_facade.py
--rw-r--r--   0 runner    (1001) docker     (123)     2552 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/runner.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/
--rw-r--r--   0 runner    (1001) docker     (123)      802 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)      830 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     3133 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/qat_config.py
--rw-r--r--   0 runner    (1001) docker     (123)     3326 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/qat_get_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6082 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/qat_get_quantizer_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.705884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    14790 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      856 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2275 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2123 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4083 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    13547 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (123)    10969 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)    11875 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantization_facade.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/
--rw-r--r--   0 runner    (1001) docker     (123)      859 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2255 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3950 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
--rw-r--r--   0 runner    (1001) docker     (123)     5004 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     9572 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
--rw-r--r--   0 runner    (1001) docker     (123)     8553 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/
--rw-r--r--   0 runner    (1001) docker     (123)     2149 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.709884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3141 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/base_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     6333 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/base_trainable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1630 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/constants.py
--rw-r--r--   0 runner    (1001) docker     (123)     1222 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/get_all_subclasses.py
--rw-r--r--   0 runner    (1001) docker     (123)     2151 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/quant_utils.py
--rw-r--r--   0 runner    (1001) docker     (123)     4774 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/trainable_quantizer_config.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.713884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/
--rw-r--r--   0 runner    (1001) docker     (123)     1618 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3361 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/base_keras_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4012 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/config_serialization.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.713884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)     2014 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.713884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3141 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/activation_pot_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3891 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/activation_symmetric_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5152 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/activation_uniform_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2168 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/base_keras_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)      944 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.713884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3404 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/weights_pot_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4306 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     8605 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/weights_uniform_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3563 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/load_model.py
--rw-r--r--   0 runner    (1001) docker     (123)    13847 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/quantize_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     2942 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/validation_functions.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.713884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/
--rw-r--r--   0 runner    (1001) docker     (123)      696 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2241 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/base_pytorch_quantizer.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.713884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)     2109 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.717884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     2878 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/activation_pot_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3582 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/activation_symmetric_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4806 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/activation_uniform_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     1879 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/base_pytorch_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     3020 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/base_symmetric_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     2451 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/base_uniform_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)      920 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/constants.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.717884 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/
--rw-r--r--   0 runner    (1001) docker     (123)      697 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (123)     3097 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_pot_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     4940 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)     5340 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_uniform_inferable_quantizer.py
--rw-r--r--   0 runner    (1001) docker     (123)    10469 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/quantize_wrapper.py
--rw-r--r--   0 runner    (1001) docker     (123)     4460 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/quantizer_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-02-08 14:05:57.645884 model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/
--rw-r--r--   0 runner    (1001) docker     (123)    12036 2023-02-08 14:05:57.000000 model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (123)    34381 2023-02-08 14:05:57.000000 model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (123)        1 2023-02-08 14:05:57.000000 model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (123)      117 2023-02-08 14:05:57.000000 model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (123)       26 2023-02-08 14:05:57.000000 model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/top_level.txt
--rw-r--r--   0 runner    (1001) docker     (123)      133 2023-02-08 14:05:57.717884 model_compression_toolkit-1.8.0/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (123)     2040 2023-02-08 14:05:11.000000 model_compression_toolkit-1.8.0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/
+-rw-r--r--   0 runner    (1001) docker     (123)    10174 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/LICENSE.md
+-rw-r--r--   0 runner    (1001) docker     (123)    11724 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    10054 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.922651 model_compression_toolkit-1.9.0/model_compression_toolkit/
+-rw-r--r--   0 runner    (1001) docker     (123)     3608 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3949 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/constants.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.922651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/
+-rw-r--r--   0 runner    (1001) docker     (123)     2008 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2975 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/analyzer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.922651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/
+-rw-r--r--   0 runner    (1001) docker     (123)     1447 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.922651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2023 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/back2framework/base_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1666 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/base_substitutions.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.926651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2576 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/base_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6864 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/histogram_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3888 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/mean_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5207 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7929 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/statistics_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2102 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/statistics_collector_generator.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4017 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/data_loader.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2281 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/defaultdict.py
+-rw-r--r--   0 runner    (1001) docker     (123)    22391 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6424 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.926651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/fusion/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/fusion/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5479 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/fusion/layer_fusing.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.926651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      773 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    28860 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/base_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20579 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/base_node.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3733 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/edge.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2922 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/functional_node.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     4732 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/graph_matchers.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     5128 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/graph_searches.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.926651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3880 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2612 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2470 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/cut.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17045 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3961 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7175 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9265 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.926651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/
+-rwxr-xr-x   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3091 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/base_graph_filter.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2210 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/base_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     3706 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/edge_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1773 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/function.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     2745 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/node_matcher.py
+-rwxr-xr-x   0 runner    (1001) docker     (123)     1111 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/walk_matcher.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1205 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/memory_computation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.926651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6704 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2222 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.930651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4297 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3920 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7046 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1602 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (123)    19323 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8041 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6822 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py
+-rw-r--r--   0 runner    (1001) docker     (123)    34622 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.930651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/search_methods/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15453 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25266 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6326 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1324 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/model_builder_mode.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5005 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/model_collector.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1214 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/model_validation.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.930651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/
+-rw-r--r--   0 runner    (1001) docker     (123)     1307 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17994 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/actions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1756 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/edit_network.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3149 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/node_filters.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1769 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.930651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3022 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/core_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1482 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/debug_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4352 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16271 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3462 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_analyzer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7247 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2352 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4090 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.930651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/
+-rw-r--r--   0 runner    (1001) docker     (123)     1608 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16505 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2927 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7270 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1772 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8484 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4558 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4315 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    41685 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5090 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9689 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7879 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2939 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3614 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantize_node.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.930651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2350 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2774 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14210 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5486 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10683 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7450 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/similarity_analyzer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.934651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3434 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5956 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10228 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5584 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.934651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1390 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/apply_substitutions.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6434 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5892 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9962 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9093 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2250 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4866 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10978 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26835 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2625 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3400 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4228 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/weights_activation_split.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1631 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/user_info.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.934651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6371 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/final_config_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5955 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/nn_visualizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    20099 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/tensorboard_writer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4233 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.934651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.934651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (123)      808 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2226 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2444 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/float_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4078 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/instance_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16452 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7150 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15316 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/model_gradients.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2481 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2664 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4999 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/default_framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.934651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3940 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3655 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3168 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2478 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5598 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5925 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    26778 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3872 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2387 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3176 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5542 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7714 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10781 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1623 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1462 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1814 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py
+-rw-r--r--   0 runner    (1001) docker     (123)    27197 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/keras_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1722 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/keras_model_validation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3941 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/keras_node_prior_info.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8600 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/kpi_data_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1900 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/mixed_precision/set_layer_to_bitwidth.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1733 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/base_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6151 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3120 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/input_layer_quantize_transform.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4507 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2837 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/quantization_config_factory.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6854 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10902 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_quantize_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7621 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2627 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/common.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11418 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/connectivity_handler.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7906 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2760 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2107 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2408 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5951 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/node_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8109 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/reader.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3060 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2022 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/tf_tensor_numpy.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.938651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/visualization/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/visualization/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/
+-rw-r--r--   0 runner    (1001) docker     (123)      813 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2274 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3419 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1848 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4976 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18214 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16443 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5773 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1640 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3456 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2472 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4224 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/default_framework_info.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3038 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2822 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2162 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4804 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5797 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)    38353 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1953 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5601 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4148 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2899 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3303 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9838 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1588 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1375 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1616 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8482 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/kpi_data_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11345 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1678 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/set_layer_to_bitwidth.py
+-rw-r--r--   0 runner    (1001) docker     (123)    25698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3250 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6464 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4452 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12113 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/graph_builders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1789 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/node_holders.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5801 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/reader.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.942651 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/statistics_correction/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3261 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2959 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)    23009 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/core/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/
+-rw-r--r--   0 runner    (1001) docker     (123)     1189 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/
+-rw-r--r--   0 runner    (1001) docker     (123)      698 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2017 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      699 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1495 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      966 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
+-rw-r--r--   0 runner    (1001) docker     (123)    11100 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3043 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8048 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6016 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      699 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4020 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)      967 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3298 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2897 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5866 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/
+-rw-r--r--   0 runner    (1001) docker     (123)     1187 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4287 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8318 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2077 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3543 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3921 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7582 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2086 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3449 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/validate_layer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/
+-rw-r--r--   0 runner    (1001) docker     (123)     1276 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.946651 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9508 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)      611 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1266 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_framework_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2826 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_graph.py
+-rw-r--r--   0 runner    (1001) docker     (123)    15167 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_training.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1248 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/gptq_keras_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6241 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/gptq_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17319 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/gptq_training.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4564 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/graph_info.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14791 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      963 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4751 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5055 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4136 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2087 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3962 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12163 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10381 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8370 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2719 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/gptq_loss.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1268 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py
+-rw-r--r--   0 runner    (1001) docker     (123)    14668 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/gptq_training.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3955 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/graph_info.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12768 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      968 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4171 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3893 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3996 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2089 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4132 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12351 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9103 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8782 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5534 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    18116 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/keras_quantization_facade.py
+-rw-r--r--   0 runner    (1001) docker     (123)    17664 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/pytorch_quantization_facade.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4863 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/logger.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/
+-rw-r--r--   0 runner    (1001) docker     (123)      930 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9868 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8580 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/pytorch/quantization_facade.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2552 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/runner.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/
+-rw-r--r--   0 runner    (1001) docker     (123)     1091 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.950652 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      829 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3295 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/common/qat_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    16051 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      856 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2138 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2123 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5635 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    13436 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (123)    10708 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)    12461 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantization_facade.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/
+-rw-r--r--   0 runner    (1001) docker     (123)      859 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2213 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5491 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5004 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9629 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8651 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)      920 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1723 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/immutable.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/
+-rw-r--r--   0 runner    (1001) docker     (123)     1574 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2010 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2353 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8538 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3108 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
+-rw-r--r--   0 runner    (1001) docker     (123)      787 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py
+-rw-r--r--   0 runner    (1001) docker     (123)     9206 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1392 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.954651 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/
+-rw-r--r--   0 runner    (1001) docker     (123)     1513 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8759 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2046 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4019 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6040 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8713 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1030 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/
+-rw-r--r--   0 runner    (1001) docker     (123)        0 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (123)     1514 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4625 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6310 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3845 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3261 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6846 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4321 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3859 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8148 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4618 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4196 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/
+-rw-r--r--   0 runner    (1001) docker     (123)      721 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8332 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4631 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4209 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8148 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5372 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4714 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/
+-rw-r--r--   0 runner    (1001) docker     (123)      721 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8332 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5477 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4727 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8145 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5373 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4714 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_pytorch.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3526 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (123)     1522 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2084 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8140 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5379 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4721 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.958652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (123)     1510 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2090 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6204 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3198 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2942 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/
+-rw-r--r--   0 runner    (1001) docker     (123)      697 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/
+-rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     2085 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/
+-rw-r--r--   0 runner    (1001) docker     (123)      717 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     8048 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6131 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py
+-rw-r--r--   0 runner    (1001) docker     (123)     5002 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/
+-rw-r--r--   0 runner    (1001) docker     (123)     1104 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     7564 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)      875 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/constants.py
+-rw-r--r--   0 runner    (1001) docker     (123)     6351 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3558 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1505 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4791 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     4212 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3991 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3480 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/load_model.py
+-rw-r--r--   0 runner    (1001) docker     (123)     1797 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/pytorch/
+-rw-r--r--   0 runner    (1001) docker     (123)      696 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (123)     3083 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
+drwxr-xr-x   0 runner    (1001) docker     (123)        0 2023-06-19 07:13:13.922651 model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (123)    11724 2023-06-19 07:13:13.000000 model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (123)    32878 2023-06-19 07:13:13.000000 model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (123)        1 2023-06-19 07:13:13.000000 model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      140 2023-06-19 07:13:13.000000 model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (123)       26 2023-06-19 07:13:13.000000 model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/top_level.txt
+-rw-r--r--   0 runner    (1001) docker     (123)      133 2023-06-19 07:13:13.962652 model_compression_toolkit-1.9.0/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (123)     2040 2023-06-19 07:12:32.000000 model_compression_toolkit-1.9.0/setup.py
```

### Comparing `model_compression_toolkit-1.8.0/LICENSE.md` & `model_compression_toolkit-1.9.0/LICENSE.md`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/PKG-INFO` & `model_compression_toolkit-1.9.0/PKG-INFO`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: model_compression_toolkit
-Version: 1.8.0
+Version: 1.9.0
 Summary: A Model Compression Toolkit for neural networks
 Home-page: UNKNOWN
 License: UNKNOWN
 Description: # Model Compression Toolkit (MCT)
         
         Model Compression Toolkit (MCT) is an open-source project for neural network model optimization under efficient, constrained hardware.
         
@@ -16,128 +16,107 @@
         
         MCT is developed by researchers and engineers working at Sony Semiconductor Israel.
         
         
         
         ## Table of Contents
         
-        - [Supported features](#supported-features)
         - [Getting Started](#getting-started)
+        - [Supported features](#supported-features)
         - [Results](#results)
         - [Contributions](#contributions)
         - [License](#license)
         
-        ## Supported Features
-        
-        MCT supports different quantization methods:
-        * Post training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
-        * Gradient-based post training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
-        * Quantization aware training (QAT)[*](#experimental-features)
-        
-        
-        | Quantization Method | Complexity                                    | Computational Cost          |
-        |---------------------|-----------------------------------------------|-----------------------------|
-        | PTQ                 | Low                                           | Low (order of minutes)      |
-        | GPTQ                | Mild (parameters fine-tuning using gradients) | Mild (order of 2-3 hours)   |
-        | QAT                 | High                                          | High (order of 12-36 hours) |
-        
-        
-        In addition, MCT supports different quantization schemes for quantizing weights and activations:
-        * Power-Of-Two (hardware-friendly quantization [1])
-        * Symmetric
-        * Uniform
-        
-        Core features:
-        * <ins>Graph optimizations:</ins> Transforming the model to an equivalent (yet, more efficient) model (for example, batch-normalization layer folding to its preceding linear layer).
-        * <ins>Quantization parameter search:</ins> Different methods can be used to minimize the expected added quantization-noise during thresholds search (by default, we use Mean-Square-Errorm but other metrics can be used such as No-Clipping, Mean-Average-Error, and more).
-        * <ins>Advanced quantization algorithms:</ins> To prevent a performance degradation some algorithms are applied such as: 
-          * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
-          * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
-        * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
-        * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
-        * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
-        
-        
-        #### Experimental features 
-        
-        Some features are experimental and subject to future changes. 
-         
-        For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
-        
         
         ## Getting Started
         
-        This section provides a quick starting guide. We begin with installation via source code or pip server. Then, we provide a short usage example.
+        This section provides an installation and a quick starting guide.
         
         ### Installation
-        See the MCT install guide for the pip package, and build from the source.
-        
         
-        #### From Source
-        ```
-        git clone https://github.com/sony/model_optimization.git
-        python setup.py install
-        ```
-        #### From PyPi - latest stable release
+        To install the latest stable release of MCT, run the following command:
         ```
         pip install model-compression-toolkit
         ```
         
-        A nightly package is also available (unstable):
-        ```
-        pip install mct-nightly
-        ```
+        For installing the nightly version or installing from source, refer to the [installation guide](INSTALLATION.md).
         
-        ### Requierments
         
-        To run MCT, one of the supported frameworks, Tenosflow/Pytorch, needs to be installed.
+        ### Quick start & tutorials 
         
-        For using with Tensorflow please install the packages: 
-        [tensorflow](https://www.tensorflow.org/install), 
-        [tensorflow-model-optimization](https://www.tensorflow.org/model_optimization/guide/install)
+        For an example of how to use MCT with TensorFlow or PyTorch on various models and tasks,
+        check out the [quick-start page](tutorials/quick_start/README.md) and
+        the [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
         
-        For using with PyTorch please install the packages: 
-        [torch](https://pytorch.org/)
-        
-        Also, a [requirements](requirements.txt) file can be used to set up your environment.
+        In addition, a set of [notebooks](tutorials/notebooks) are provided for an easy start. For example:
+        * [MobileNet with Tensorflow](tutorials/notebooks/example_keras_mobilenet.py).
+        * [MobileNetV2 with PyTorch](tutorials/notebooks/example_pytorch_mobilenet_v2.py).
         
         
         ### Supported Python Versions
         
         Currently, MCT is being tested on various Python versions:
         
-        
-        
         | Python Version                                                                                                                                                                                                                                                            |
         |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
         | [![Run Tests - Python 3.10](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python310.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python310.yml) |
         | [![Run Tests - Python 3.9](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python39.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python39.yml)   |
         | [![Run Tests - Python 3.8](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python38.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python38.yml)   |
         | [![Run Tests - Python 3.7](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python37.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python37.yml)   |
         
         
         ### Supported NN-Frameworks Versions
         
-        Currently, MCT supports compressing models of TensorFlow and PyTorch, and
-        is tested on various versions:
+        MCT supports compressing models built with the TensorFlow or PyTorch frameworks, and is tested on various python versions:
         
         | TensorFlow Version                                                                                   | PyTorch Version                                                                                          |
         |------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
         | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf211.yml/badge.svg) | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_13.yml/badge.svg) |
         | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf210.yml/badge.svg) | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_12.yml/badge.svg) |
         | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf29.yml/badge.svg)  | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_11.yml/badge.svg) |
         
         
-        ### Usage Example 
-        For an example of how to use the post-training quantization, using Keras,
-        please use this [link](tutorials/example_keras_mobilenet.py).
+        ## Supported Features
+        
+        MCT supports different quantization methods:
+        * Post-training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
+        * Gradient-based post-training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
+        * Quantization-aware training (QAT)[*](#experimental-features)
+        
+        
+        | Quantization Method                           | Complexity | Computational Cost          |
+        |-----------------------------------------------|------------|-----------------------------|
+        | PTQ                                           | Low        | Low (order of minutes)      |
+        | GPTQ (parameters fine-tuning using gradients) | Mild       | Mild (order of 2-3 hours)   |
+        | QAT                                           | High       | High (order of 12-36 hours) |
         
-        For an example using PyTorch, please use this [link](tutorials/example_pytorch_mobilenet_v2.py).
         
-        For more examples please see the [tutorials' directory](https://github.com/sony/model_optimization/tree/main/tutorials).
+        In addition, MCT supports different quantization schemes for quantizing weights and activations:
+        
+        * Power-Of-Two (hardware-friendly quantization [1])
+        * Symmetric
+        * Uniform
+        
+        Main features:
+        * <ins>Graph optimizations:</ins> Transforming the model to an equivalent (yet, more efficient) model (for example, batch-normalization layer folding to its preceding linear layer).
+        * <ins>Quantization parameter search:</ins> Different methods can be used to minimize the expected added quantization-noise during thresholds search (by default, we use Mean-Square-Error, but other metrics can be used such as No-Clipping, Mean-Average-Error, and more).
+        * <ins>Advanced quantization algorithms:</ins> To prevent a performance degradation some algorithms are applied such as: 
+          * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
+          * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
+        * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
+        * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
+        * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
+        * <ins>Target Platform Capabilities:</ins> The Target Platform Capabilities (TPC) describes the target platform (an edge device with dedicated hardware). For more details, please read the [TPC README](model_compression_toolkit/target_platform_capabilities/README.md).   
+        
+        
+        #### Experimental features 
+        
+        Some features are experimental and subject to future changes. 
+         
+        For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
         
         
         ## Results
         ### Keras
         Graph of [MobileNetV2](https://keras.io/api/applications/mobilenet/) accuracy on ImageNet vs average bit-width of weights, using 
         single-precision quantization, mixed-precision quantization, and mixed-precision quantization with GPTQ.
         
@@ -151,15 +130,15 @@
         
         | Network Name              | Float Accuracy  | 8Bit Accuracy   | 
         | --------------------------| ---------------:| ---------------:| 
         | MobileNet V2 [3]          | 71.886          | 71.444           |                                      
         | ResNet-18 [3]             | 69.86           | 69.63           |                                      
         | SqueezeNet 1.1 [3]        | 58.128          | 57.678          |                                      
         
-        
+        For more results, please refer to [quick start](https://github.com/sony/model_optimization/tree/main/tutorials/quick_start).
         
         ## Contributions
         MCT aims at keeping a more up-to-date fork and welcomes contributions from anyone.
         
         *You will find more information about contributions in the [Contribution guide](CONTRIBUTING.md).
```

### Comparing `model_compression_toolkit-1.8.0/README.md` & `model_compression_toolkit-1.9.0/README.md`

 * *Files 5% similar despite different names*

```diff
@@ -10,128 +10,107 @@
 
 MCT is developed by researchers and engineers working at Sony Semiconductor Israel.
 
 
 
 ## Table of Contents
 
-- [Supported features](#supported-features)
 - [Getting Started](#getting-started)
+- [Supported features](#supported-features)
 - [Results](#results)
 - [Contributions](#contributions)
 - [License](#license)
 
-## Supported Features
-
-MCT supports different quantization methods:
-* Post training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
-* Gradient-based post training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
-* Quantization aware training (QAT)[*](#experimental-features)
-
-
-| Quantization Method | Complexity                                    | Computational Cost          |
-|---------------------|-----------------------------------------------|-----------------------------|
-| PTQ                 | Low                                           | Low (order of minutes)      |
-| GPTQ                | Mild (parameters fine-tuning using gradients) | Mild (order of 2-3 hours)   |
-| QAT                 | High                                          | High (order of 12-36 hours) |
-
-
-In addition, MCT supports different quantization schemes for quantizing weights and activations:
-* Power-Of-Two (hardware-friendly quantization [1])
-* Symmetric
-* Uniform
-
-Core features:
-* <ins>Graph optimizations:</ins> Transforming the model to an equivalent (yet, more efficient) model (for example, batch-normalization layer folding to its preceding linear layer).
-* <ins>Quantization parameter search:</ins> Different methods can be used to minimize the expected added quantization-noise during thresholds search (by default, we use Mean-Square-Errorm but other metrics can be used such as No-Clipping, Mean-Average-Error, and more).
-* <ins>Advanced quantization algorithms:</ins> To prevent a performance degradation some algorithms are applied such as: 
-  * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
-  * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
-* <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
-* <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
-* <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
-
-
-#### Experimental features 
-
-Some features are experimental and subject to future changes. 
- 
-For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
-
 
 ## Getting Started
 
-This section provides a quick starting guide. We begin with installation via source code or pip server. Then, we provide a short usage example.
+This section provides an installation and a quick starting guide.
 
 ### Installation
-See the MCT install guide for the pip package, and build from the source.
-
 
-#### From Source
-```
-git clone https://github.com/sony/model_optimization.git
-python setup.py install
-```
-#### From PyPi - latest stable release
+To install the latest stable release of MCT, run the following command:
 ```
 pip install model-compression-toolkit
 ```
 
-A nightly package is also available (unstable):
-```
-pip install mct-nightly
-```
+For installing the nightly version or installing from source, refer to the [installation guide](INSTALLATION.md).
 
-### Requierments
 
-To run MCT, one of the supported frameworks, Tenosflow/Pytorch, needs to be installed.
+### Quick start & tutorials 
 
-For using with Tensorflow please install the packages: 
-[tensorflow](https://www.tensorflow.org/install), 
-[tensorflow-model-optimization](https://www.tensorflow.org/model_optimization/guide/install)
+For an example of how to use MCT with TensorFlow or PyTorch on various models and tasks,
+check out the [quick-start page](tutorials/quick_start/README.md) and
+the [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
 
-For using with PyTorch please install the packages: 
-[torch](https://pytorch.org/)
-
-Also, a [requirements](requirements.txt) file can be used to set up your environment.
+In addition, a set of [notebooks](tutorials/notebooks) are provided for an easy start. For example:
+* [MobileNet with Tensorflow](tutorials/notebooks/example_keras_mobilenet.py).
+* [MobileNetV2 with PyTorch](tutorials/notebooks/example_pytorch_mobilenet_v2.py).
 
 
 ### Supported Python Versions
 
 Currently, MCT is being tested on various Python versions:
 
-
-
 | Python Version                                                                                                                                                                                                                                                            |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | [![Run Tests - Python 3.10](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python310.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python310.yml) |
 | [![Run Tests - Python 3.9](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python39.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python39.yml)   |
 | [![Run Tests - Python 3.8](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python38.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python38.yml)   |
 | [![Run Tests - Python 3.7](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python37.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python37.yml)   |
 
 
 ### Supported NN-Frameworks Versions
 
-Currently, MCT supports compressing models of TensorFlow and PyTorch, and
-is tested on various versions:
+MCT supports compressing models built with the TensorFlow or PyTorch frameworks, and is tested on various python versions:
 
 | TensorFlow Version                                                                                   | PyTorch Version                                                                                          |
 |------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
 | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf211.yml/badge.svg) | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_13.yml/badge.svg) |
 | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf210.yml/badge.svg) | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_12.yml/badge.svg) |
 | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf29.yml/badge.svg)  | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_11.yml/badge.svg) |
 
 
-### Usage Example 
-For an example of how to use the post-training quantization, using Keras,
-please use this [link](tutorials/example_keras_mobilenet.py).
+## Supported Features
+
+MCT supports different quantization methods:
+* Post-training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
+* Gradient-based post-training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
+* Quantization-aware training (QAT)[*](#experimental-features)
+
+
+| Quantization Method                           | Complexity | Computational Cost          |
+|-----------------------------------------------|------------|-----------------------------|
+| PTQ                                           | Low        | Low (order of minutes)      |
+| GPTQ (parameters fine-tuning using gradients) | Mild       | Mild (order of 2-3 hours)   |
+| QAT                                           | High       | High (order of 12-36 hours) |
 
-For an example using PyTorch, please use this [link](tutorials/example_pytorch_mobilenet_v2.py).
 
-For more examples please see the [tutorials' directory](https://github.com/sony/model_optimization/tree/main/tutorials).
+In addition, MCT supports different quantization schemes for quantizing weights and activations:
+
+* Power-Of-Two (hardware-friendly quantization [1])
+* Symmetric
+* Uniform
+
+Main features:
+* <ins>Graph optimizations:</ins> Transforming the model to an equivalent (yet, more efficient) model (for example, batch-normalization layer folding to its preceding linear layer).
+* <ins>Quantization parameter search:</ins> Different methods can be used to minimize the expected added quantization-noise during thresholds search (by default, we use Mean-Square-Error, but other metrics can be used such as No-Clipping, Mean-Average-Error, and more).
+* <ins>Advanced quantization algorithms:</ins> To prevent a performance degradation some algorithms are applied such as: 
+  * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
+  * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
+* <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
+* <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
+* <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
+* <ins>Target Platform Capabilities:</ins> The Target Platform Capabilities (TPC) describes the target platform (an edge device with dedicated hardware). For more details, please read the [TPC README](model_compression_toolkit/target_platform_capabilities/README.md).   
+
+
+#### Experimental features 
+
+Some features are experimental and subject to future changes. 
+ 
+For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
 
 
 ## Results
 ### Keras
 Graph of [MobileNetV2](https://keras.io/api/applications/mobilenet/) accuracy on ImageNet vs average bit-width of weights, using 
 single-precision quantization, mixed-precision quantization, and mixed-precision quantization with GPTQ.
 
@@ -145,15 +124,15 @@
 
 | Network Name              | Float Accuracy  | 8Bit Accuracy   | 
 | --------------------------| ---------------:| ---------------:| 
 | MobileNet V2 [3]          | 71.886          | 71.444           |                                      
 | ResNet-18 [3]             | 69.86           | 69.63           |                                      
 | SqueezeNet 1.1 [3]        | 58.128          | 57.678          |                                      
 
-
+For more results, please refer to [quick start](https://github.com/sony/model_optimization/tree/main/tutorials/quick_start).
 
 ## Contributions
 MCT aims at keeping a more up-to-date fork and welcomes contributions from anyone.
 
 *You will find more information about contributions in the [Contribution guide](CONTRIBUTING.md).
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -9,52 +9,43 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType, GradientPTQConfigV2
-from model_compression_toolkit.gptq.common.gptq_quantizer_config import GPTQQuantizerConfig, SoftQuantizerConfig
+from model_compression_toolkit.target_platform_capabilities import target_platform
+from model_compression_toolkit.target_platform_capabilities.tpc_models.get_target_platform_capabilities import get_target_platform_capabilities
+from model_compression_toolkit import core
+from model_compression_toolkit.logger import set_log_folder
+from model_compression_toolkit.legacy.keras_quantization_facade import keras_post_training_quantization, keras_post_training_quantization_mixed_precision
+from model_compression_toolkit.legacy.pytorch_quantization_facade import pytorch_post_training_quantization, pytorch_post_training_quantization_mixed_precision
+from model_compression_toolkit import trainable_infrastructure
+from model_compression_toolkit import ptq
+from model_compression_toolkit import qat
+from model_compression_toolkit import exporter
+from model_compression_toolkit import gptq
+from model_compression_toolkit.trainable_infrastructure.keras.load_model import keras_load_quantized_model
+
+
+# Old API (will not be accessible in future releases)
+from model_compression_toolkit.core.common import network_editors as network_editor
 from model_compression_toolkit.core.common.quantization import quantization_config
 from model_compression_toolkit.core.common.mixed_precision import mixed_precision_quantization_config
-from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, \
-    QuantizationErrorMethod, DEFAULTCONFIG
-from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
-from model_compression_toolkit.core.common import target_platform
-from model_compression_toolkit.core.tpc_models.get_target_platform_capabilities import get_target_platform_capabilities
+from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
+from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, QuantizationErrorMethod, DEFAULTCONFIG
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfig, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.qat.common.qat_config import QATConfig, TrainingMethod
-from model_compression_toolkit.core.common.logger import set_log_folder
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfig
+from model_compression_toolkit.logger import set_log_folder
 from model_compression_toolkit.core.common.data_loader import FolderImageLoader
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
 from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common import network_editors as network_editor
-
-from model_compression_toolkit.core.keras.quantization_facade import keras_post_training_quantization, \
-    keras_post_training_quantization_mixed_precision
-from model_compression_toolkit.ptq.keras.quantization_facade import keras_post_training_quantization_experimental
-from model_compression_toolkit.gptq.keras.quantization_facade import \
-    keras_gradient_post_training_quantization_experimental
+from model_compression_toolkit.legacy.keras_quantization_facade import keras_post_training_quantization, keras_post_training_quantization_mixed_precision
+from model_compression_toolkit.legacy.pytorch_quantization_facade import pytorch_post_training_quantization, pytorch_post_training_quantization_mixed_precision
+from model_compression_toolkit.core.keras.kpi_data_facade import keras_kpi_data
+from model_compression_toolkit.core.pytorch.kpi_data_facade import pytorch_kpi_data
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
+from model_compression_toolkit.gptq.common.gptq_config import RoundingType
 from model_compression_toolkit.gptq.keras.quantization_facade import get_keras_gptq_config
-from model_compression_toolkit.qat.keras.quantization_facade import keras_quantization_aware_training_init, \
-    keras_quantization_aware_training_finalize
-from model_compression_toolkit.qat.pytorch.quantization_facade import pytorch_quantization_aware_training_init, \
-    pytorch_quantization_aware_training_finalize
-from model_compression_toolkit.core.pytorch.quantization_facade import pytorch_post_training_quantization, \
-    pytorch_post_training_quantization_mixed_precision
-from model_compression_toolkit.ptq.pytorch.quantization_facade import pytorch_post_training_quantization_experimental
-from model_compression_toolkit.gptq.pytorch.quantization_facade import \
-    pytorch_gradient_post_training_quantization_experimental
 from model_compression_toolkit.gptq.pytorch.quantization_facade import get_pytorch_gptq_config
 
-from model_compression_toolkit.core.keras.kpi_data_facade import keras_kpi_data, keras_kpi_data_experimental
-from model_compression_toolkit.core.pytorch.kpi_data_facade import pytorch_kpi_data, pytorch_kpi_data_experimental
-
-from model_compression_toolkit.quantizers_infrastructure.keras.load_model import keras_load_quantized_model
-
-from model_compression_toolkit.exporter.model_exporter import tflite_export_model, TFLiteExportMode, keras_export_model, KerasExportMode, pytorch_export_model, PyTorchExportMode
-
-__version__ = "1.8.0"
+__version__ = "1.9.0"
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/analyzer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/analyzer.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,22 +13,23 @@
 # limitations under the License.
 # ==============================================================================
 
 
 from typing import Callable
 
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-from model_compression_toolkit.core.common import FrameworkInfo, Logger
-from model_compression_toolkit.core.common.constants import NUM_SAMPLES_DISTANCE_TENSORBOARD
+from model_compression_toolkit.core.common import FrameworkInfo
+from model_compression_toolkit.constants import NUM_SAMPLES_DISTANCE_TENSORBOARD
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 
 from model_compression_toolkit.core.common.similarity_analyzer import compute_cs
 from model_compression_toolkit.core.common.visualization.nn_visualizer import NNVisualizer
 
 from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
+from model_compression_toolkit.logger import Logger
 
 
 def analyzer_model_quantization(representative_data_gen: Callable,
                                 tb_w: TensorboardWriter,
                                 tg: Graph,
                                 fw_impl: FrameworkImplementation,
                                 fw_info: FrameworkInfo):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,12 +13,11 @@
 # limitations under the License.
 # ==============================================================================
 from model_compression_toolkit.core.common.quantization import quantization_params_generation
 from model_compression_toolkit.core.common.base_substitutions import BaseSubstitution
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.logger import Logger
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, DEFAULTCONFIG
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import max_power_of_two
 from model_compression_toolkit.core.common.collectors.statistics_collector import StatsCollector, NoStatsCollector
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/back2framework/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/back2framework/base_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/back2framework/base_model_builder.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/base_substitutions.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/base_substitutions.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/base_collector.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/base_collector.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 class BaseCollector(object):
     """
     Base class for statistics collection object.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/histogram_collector.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/histogram_collector.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/mean_collector.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/mean_collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import numpy as np
 
 from model_compression_toolkit.core.common.collectors.base_collector import BaseCollector
-from model_compression_toolkit.core.common.constants import LAST_AXIS
+from model_compression_toolkit.constants import LAST_AXIS
 
 
 class MeanCollector(BaseCollector):
     """
         Class to collect observed per channel mean values of tensors that goes through it (passed to update).
         The mean is calculated using a exponential moving average with bias correction.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/min_max_per_channel_collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
 
 from model_compression_toolkit.core.common.collectors.base_collector import BaseCollector
-from model_compression_toolkit.core.common.constants import LAST_AXIS
+from model_compression_toolkit.constants import LAST_AXIS
 
 
 class MinMaxPerChannelCollector(BaseCollector):
     """
     Class to collect observed mix/max values of tensors that goes through it (passed to update).
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/statistics_collector.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/statistics_collector.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/collectors/statistics_collector_generator.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/collectors/statistics_collector_generator.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/constants.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/constants.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,14 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+
 import importlib
 
 # Supported frameworks in MCT:
 TENSORFLOW = 'tensorflow'
 PYTORCH = 'pytorch'
 FOUND_TF = importlib.util.find_spec(TENSORFLOW) is not None and importlib.util.find_spec(
     "tensorflow_model_optimization") is not None
@@ -43,17 +44,14 @@
 REUSE_GROUP = 'reuse_group'
 LAST_AXIS = -1
 
 # Data types:
 DATA_TYPE = 'dtype'
 FLOAT_32 = 'float32'
 
-# Version
-LATEST = 'latest'
-
 # Number of Tensorboard cosine-similarity plots to add:
 NUM_SAMPLES_DISTANCE_TENSORBOARD = 20
 
 # num bits for shift negative non linear node
 SHIFT_NEGATIVE_NON_LINEAR_NUM_BITS = 16
 
 # Default bitwidth for disabled quantization candidate
@@ -115,16 +113,14 @@
 WEIGHTS_QUANT_PARAMS_FN = 'weights_quantization_params_fn'
 WEIGHTS_CHANNELS_AXIS = 'weights_channels_axis'
 
 # Memory graph constants
 DUMMY_NODE = 'dummy_node'
 DUMMY_TENSOR = 'dummy_tensor'
 
-# TP Model constants
-OPS_SET_LIST = 'ops_set_list'
 
 # TF Input node base name
 INPUT_BASE_NAME = 'base_input'
 
 # Jacobian-weights constants
 MIN_JACOBIANS_ITER = 10
-JACOBIANS_COMP_TOLERANCE = 1e-3
+JACOBIANS_COMP_TOLERANCE = 1e-3
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/data_loader.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/data_loader.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/defaultdict.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/defaultdict.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/framework_implementation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/framework_implementation.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 from abc import ABC, abstractmethod
 from typing import Callable, Any, List, Tuple, Dict
 
 import numpy as np
 
-from model_compression_toolkit import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfigV2
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
@@ -284,21 +284,14 @@
         Returns:
             A list of the framework substitutions used after we apply second moment statistics.
         """
         raise NotImplemented(f'{self.__class__.__name__} have to implement the '
                              f'framework\'s get_substitutions_after_second_moment_correction '
                              f'method.')  # pragma: no cover
 
-    @abstractmethod
-    def get_gptq_trainer_obj(self):
-        """
-        Returns: GPTQTrainer object
-        """
-        raise NotImplemented(f'{self.__class__.__name__} have to implement the '
-                             f'framework\'s get_gptq_trainer method.')  # pragma: no cover
 
     @abstractmethod
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
                                   quant_config: MixedPrecisionQuantizationConfigV2,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/framework_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/framework_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 from enum import Enum
 from typing import Dict, Any, List
 
 
 
 from model_compression_toolkit.core.common.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 class ChannelAxis(Enum):
     """
 
     Index of output channels axis:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/fusion/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/fusion/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/fusion/layer_fusing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/fusion/layer_fusing.py`

 * *Files 3% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 from typing import Any, List
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.layer_filter_params import LayerFilterParams
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.layer_filter_params import LayerFilterParams
 
 
 def filter_fusing_patterns(fusing_patterns: List[List[Any]], node: BaseNode, idx: int = 0) -> List[List[Any]]:
     """
     Update relevant fusing patterns object if layer number 'idx' inside the fusion matches the node
     Args:
         fusing_patterns: supported fusings
@@ -29,15 +29,15 @@
         idx: index of layer in the fusion
     Returns:
         fusing_patterns after filtering non-relevant fusions
     """
     valid_fusing_patterns = []
     for i,fusing_pattern in enumerate(fusing_patterns):
         if idx < len(fusing_pattern):
-            if (type(fusing_pattern[idx]) == LayerFilterParams and fusing_pattern[idx].match(node)) or fusing_pattern[idx] == node.type:
+            if (type(fusing_pattern[idx]) == LayerFilterParams and node.is_match_filter_params(fusing_pattern[idx])) or fusing_pattern[idx] == node.type:
                 valid_fusing_patterns.append(fusing_pattern)
 
     # Return only valid patterns for this node
     return valid_fusing_patterns
 
 
 def is_valid_fusion(fusing_patterns: List[List[Any]], nodes: List[BaseNode]) -> bool:
@@ -53,15 +53,15 @@
     if fusion_depth <= 1:
         return False
     for fusing_pattern in fusing_patterns:
         if fusion_depth != len(fusing_pattern):
             continue
         counter = 0
         for i,layer in enumerate(fusing_pattern):
-            if (type(layer) == LayerFilterParams and layer.match(nodes[i])) or layer == nodes[i].type:
+            if (type(layer) == LayerFilterParams and nodes[i].is_match_filter_params(layer)) or layer == nodes[i].type:
                 counter += 1
         if counter == fusion_depth:
             return True
     return False
 
 
 def disable_nodes_activation_quantization(nodes: List[BaseNode]):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/base_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/base_graph.py`

 * *Files 1% similar despite different names*

```diff
@@ -26,16 +26,16 @@
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX, EDGE_SOURCE_INDEX
 from model_compression_toolkit.core.common.graph.edge import Edge, convert_to_edge
 from model_compression_toolkit.core.common.graph.graph_searches import GraphSearches
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.collectors.statistics_collector import scale_statistics, shift_statistics
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 
 OutTensor = namedtuple('OutTensor', 'node node_out_index')
 
 
 class Graph(nx.MultiDiGraph, GraphSearches):
     """
     Base graph representing a model to be optimized.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/base_node.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/base_node.py`

 * *Files 18% similar despite different names*

```diff
@@ -14,16 +14,19 @@
 # ==============================================================================
 
 import copy
 from typing import Dict, Any, Tuple, List
 
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import WEIGHTS_NBITS_ATTRIBUTE, CORRECTED_BIAS_ATTRIBUTE, \
+from model_compression_toolkit.constants import WEIGHTS_NBITS_ATTRIBUTE, CORRECTED_BIAS_ATTRIBUTE, \
     ACTIVATION_NBITS_ATTRIBUTE
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationConfigOptions, \
+    TargetPlatformCapabilities, LayerFilterParams
 
 
 class BaseNode:
     """
     Class to represent a node in a graph that represents the model.
     """
 
@@ -425,7 +428,60 @@
         Checks whether the node has quantization configuration candidates that enable activation quantization.
 
         Returns: True if the node has at list one quantization configuration candidate with activation quantization enabled.
         """
 
         return len(self.candidates_quantization_cfg) > 0 and \
                any([c.activation_quantization_cfg.enable_activation_quantization for c in self.candidates_quantization_cfg])
+
+    def get_qco(self, tpc: TargetPlatformCapabilities) -> QuantizationConfigOptions:
+        """
+        Get the QuantizationConfigOptions of the node according
+        to the mappings from layers/LayerFilterParams to the OperatorsSet in the TargetPlatformModel.
+
+        Args:
+            tpc: TPC to extract the QuantizationConfigOptions for the node
+
+        Returns:
+            QuantizationConfigOptions of the node.
+        """
+
+        if tpc is None:
+            Logger.error(f'Can not retrieve QC options for None TPC')  # pragma: no cover
+
+        for fl, qco in tpc.filterlayer2qco.items():
+            if self.is_match_filter_params(fl):
+                return qco
+        if self.type in tpc.layer2qco:
+            return tpc.layer2qco.get(self.type)
+        return tpc.tp_model.default_qco
+
+
+    def is_match_filter_params(self, layer_filter_params: LayerFilterParams) -> bool:
+        """
+        Check if the node matches a LayerFilterParams according to its
+        layer, conditions and keyword-arguments.
+
+        Args:
+            layer_filter_params: LayerFilterParams to check if the node matches its properties.
+
+        Returns:
+            Whether the node matches to the LayerFilterParams properties.
+        """
+        # Check the node has the same type as the layer in LayerFilterParams
+        if layer_filter_params.layer != self.type:
+            return False
+
+        # Get attributes from node to filter
+        layer_config = self.framework_attr
+        if hasattr(self, "op_call_kwargs"):
+            layer_config.update(self.op_call_kwargs)
+
+        for attr, value in layer_filter_params.kwargs.items():
+            if layer_config.get(attr) != value:
+                return False
+
+        for c in layer_filter_params.conditions:
+            if not c.match(layer_config):
+                return False
+
+        return True
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/edge.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/edge.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/functional_node.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/functional_node.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/graph_matchers.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/graph_matchers.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/graph_searches.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/graph_searches.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/bipartite_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Any, List, Tuple
 import networkx as nx
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 class DirectedBipartiteGraph(nx.DiGraph):
     """
     Directed Bipartite graph representation.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/compute_graph_max_cut.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/cut.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/cut.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/max_cut_astar.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 from typing import List, Tuple, Dict
 
 from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.constants import DUMMY_TENSOR, DUMMY_NODE
+from model_compression_toolkit.constants import DUMMY_TENSOR, DUMMY_NODE
 from model_compression_toolkit.core.common.graph.memory_graph.cut import Cut
 from model_compression_toolkit.core.common.graph.memory_graph.memory_element import MemoryElements
 from model_compression_toolkit.core.common.graph.memory_graph.memory_graph import ActivationMemoryTensor, MemoryGraph
 
 
 class DummyType:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/memory_element.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/memory_graph/memory_graph.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/graph/virtual_activation_weights_node.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,16 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Dict, Any, Tuple
 
-from model_compression_toolkit import FrameworkInfo
-from model_compression_toolkit.core.common.constants import VIRTUAL_ACTIVATION_WEIGHTS_NODE_PREFIX, \
+from model_compression_toolkit.core import FrameworkInfo
+from model_compression_toolkit.constants import VIRTUAL_ACTIVATION_WEIGHTS_NODE_PREFIX, \
     VIRTUAL_WEIGHTS_SUFFIX, VIRTUAL_ACTIVATION_SUFFIX, FLOAT_BITWIDTH
 
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 import numpy as np
 
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/immutable.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/immutable.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/logger.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/logger.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,14 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import logging
 import os
 from datetime import datetime
-from os import path
 from pathlib import Path
 
 LOGGER_NAME = 'Constrained Model Optimization'
 
 
 class Logger:
     # Logger has levels of verbosity.
@@ -39,15 +38,15 @@
         """
         Create a path if not exist. Otherwise, do nothing.
         Args:
             log_path: Path to create or verify that exists.
 
         """
 
-        if not path.exists(log_path):
+        if not os.path.exists(log_path):
             Path(log_path).mkdir(parents=True, exist_ok=True)
 
     @staticmethod
     def set_logger_level(log_level=logging.INFO):
         """
         Set log level to determine the logger verbosity.
         Args:
@@ -89,14 +88,23 @@
 
         fh = logging.FileHandler(log_name)
         fh.setLevel(logging.DEBUG)
         logger.addHandler(fh)
 
         print(f'log file is in {log_name}')
 
+    @staticmethod
+    def shutdown():
+        """
+        An orderly command to shutdown by flushing and closing all logging handlers.
+
+        """
+        Logger.LOG_PATH = None
+        logging.shutdown()
+
     ########################################
     # Delegating methods to wrapped logger
     ########################################
 
     @staticmethod
     def critical(msg: str):
         """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/base_graph_filter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/base_graph_filter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/base_matcher.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/base_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/edge_matcher.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/edge_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/function.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/function.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/node_matcher.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/node_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/matchers/walk_matcher.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/matchers/walk_matcher.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/memory_computation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/constants.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,29 +1,20 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.constants import BITS_TO_BYTES
 
-
-def compute_quantize_tensor_memory_bytes(tensor_size: float, n_bits: int) -> float:
-    """
-    A utility function to compute the actual memory size of a tensor for a given bit-width.
-
-    Args:
-        tensor_size: The number of parameters in the tensor.
-        n_bits: The bit-width in which the tensor values are represented.
-
-    Returns: The size of the tensor in memory in bytes.
-
-    """
-    return tensor_size * n_bits / BITS_TO_BYTES
+# Quantizers constants (for GPTQ, QAT, etc.)
+FQ_MIN = "min"
+FQ_MAX = "max"
+THRESHOLD_TENSOR = "ptq_threshold_tensor"
+WEIGHTS_QUANTIZATION_PARAMS = 'weights_quantization_params'
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/bit_width_setter.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,33 +14,31 @@
 # ==============================================================================
 
 import copy
 
 from typing import Any, List
 
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def set_bit_widths(mixed_precision_enable: bool,
-                   graph_to_set_bit_widths: Graph,
+                   graph: Graph,
                    bit_widths_config: List[int] = None) -> Graph:
     """
     Set bit widths configuration to nodes in a graph. For each node, use the desired index
     in bit_widths_config to finalize the node weights and activation quantization configuration.
 
     Args:
         mixed_precision_enable: Is mixed precision enabled.
-        graph_to_set_bit_widths: A prepared for quantization graph to set its bit widths.
+        graph: A prepared for quantization graph to set its bit widths.
         bit_widths_config: MP configuration (a list of indices: one for each node's candidate
         quantization configuration).
 
     """
-    graph = copy.deepcopy(graph_to_set_bit_widths)
-
     if mixed_precision_enable:
         assert all([len(n.candidates_quantization_cfg) > 0 for n in graph.get_configurable_sorted_nodes()]), \
             "All configurable nodes in graph should have at least one candidate configuration in mixed precision mode"
 
         Logger.info(f'Set bit widths from configuration: {bit_widths_config}')
         # Get a list of nodes' names we need to finalize (that they have at least one weight qc candidate).
         sorted_nodes_names = graph.get_configurable_sorted_nodes_names()
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/distance_weighting.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_aggregation_methods.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_data.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,22 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable, Any
 import numpy as np
 
-from model_compression_toolkit import FrameworkInfo, KPI, CoreConfig
+from model_compression_toolkit.core import FrameworkInfo, KPI, CoreConfig
 from model_compression_toolkit.core.common import Graph
-from model_compression_toolkit.core.common.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 from model_compression_toolkit.core.runner import read_model_to_graph, get_finalized_graph
-from model_compression_toolkit.core.common.logger import Logger
 
 
 def compute_kpi_data(in_model: Any,
                      representative_data_gen: Callable,
                      core_config: CoreConfig,
                      tpc: TargetPlatformCapabilities,
                      fw_info: FrameworkInfo,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_functions_mapping.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/kpi_tools/kpi_methods.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,22 +14,22 @@
 # ==============================================================================
 from enum import Enum
 from functools import partial
 from typing import List
 
 import numpy as np
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.constants import BITS_TO_BYTES, FLOAT_BITWIDTH
+from model_compression_toolkit.constants import BITS_TO_BYTES, FLOAT_BITWIDTH
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode, \
     VirtualSplitWeightsNode, VirtualSplitActivationNode
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def weights_size_kpi(mp_cfg: List[int],
                      graph: Graph,
                      fw_info: FrameworkInfo,
                      fw_impl: FrameworkImplementation) -> np.ndarray:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_quantization_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from enum import Enum
 from typing import List, Callable, Tuple
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.mixed_precision.distance_weighting import get_average_weights
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, DEFAULTCONFIG
 from model_compression_toolkit.core.common.similarity_analyzer import compute_mse
 
 
 class MixedPrecisionQuantizationConfigV2:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_facade.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,26 +14,27 @@
 # ==============================================================================
 
 import copy
 from enum import Enum
 import numpy as np
 from typing import List, Callable, Dict
 
-from model_compression_toolkit import MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common import Graph, Logger
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core.common import Graph
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_functions_mapping import kpi_functions_mapping
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import MixedPrecisionSearchManager
 from model_compression_toolkit.core.common.mixed_precision.search_methods.linear_programming import \
     mp_integer_programming_search
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.solution_refinement_procedure import \
     greedy_solution_refinement_procedure
 from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
+from model_compression_toolkit.logger import Logger
 
 
 class BitWidthSearchMethod(Enum):
     # When adding a new search_methods MP configuration method, these enum and factory dictionary
     # should be updated with it's kind and a search_method implementation.
     INTEGER_PROGRAMMING = 0
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/mixed_precision_search_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 # ==============================================================================
 
 from typing import Callable, Tuple
 from typing import Dict, List
 import numpy as np
 
 from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode, \
     VirtualSplitWeightsNode, VirtualSplitActivationNode
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPITarget, KPI
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_methods import MpKpiMetric
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/search_methods/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/search_methods/linear_programming.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,18 +14,20 @@
 # ==============================================================================
 
 import numpy as np
 from pulp import *
 from tqdm import tqdm
 from typing import Dict, List, Tuple, Callable
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import MixedPrecisionSearchManager
 
+# Limit ILP solver runtime in seconds
+SOLVER_TIME_LIMIT = 60
 
 def mp_integer_programming_search(search_manager: MixedPrecisionSearchManager,
                                   target_kpi: KPI = None) -> List[int]:
     """
     Searching and returning a mixed-precision configuration using an ILP optimization solution.
     It first builds a mapping from each layer's index (in the model) to a dictionary that maps the
     bitwidth index to the observed sensitivity of the model when using that bitwidth for that layer.
@@ -60,15 +62,18 @@
     # Add all equations and inequalities that define the problem.
     lp_problem = _formalize_problem(layer_to_indicator_vars_mapping,
                                     layer_to_metrics_mapping,
                                     layer_to_objective_vars_mapping,
                                     target_kpi,
                                     search_manager)
 
-    lp_problem.solve()  # Try to solve the problem.
+    # Use default PULP solver. Limit runtime in seconds
+    solver = PULP_CBC_CMD(timeLimit=SOLVER_TIME_LIMIT)
+    lp_problem.solve(solver=solver)  # Try to solve the problem.
+
     assert lp_problem.status == LpStatusOptimal, Logger.critical(
         "No solution was found during solving the LP problem")
     Logger.info(LpStatus[lp_problem.status])
 
     # Take the bitwidth index only if its corresponding indicator is one.
     config = np.asarray(
         [[nbits for nbits, indicator in nbits_to_indicator.items() if indicator.varValue == 1.0] for
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/sensitivity_evaluation.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,18 +13,18 @@
 # limitations under the License.
 # ==============================================================================
 import copy
 
 import numpy as np
 from typing import Callable, Any, List
 
-from model_compression_toolkit import FrameworkInfo, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import FrameworkInfo, MixedPrecisionQuantizationConfigV2
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 
 
 class SensitivityEvaluation:
     """
     Class to wrap and manage the computation on distance metric for Mixed-Precision quantization search.
     It provides a function that evaluates the sensitivity of a bit-width configuration for the MP model.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/mixed_precision/solution_refinement_procedure.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,20 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import List
 
-from model_compression_toolkit import KPI
+from model_compression_toolkit.core import KPI
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_search_manager import \
     MixedPrecisionSearchManager
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 import numpy as np
 
 
 def greedy_solution_refinement_procedure(mp_solution: List[int],
                                          search_manager: MixedPrecisionSearchManager,
                                          target_kpi: KPI) -> List[int]:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/model_builder_mode.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/model_builder_mode.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/model_collector.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/model_collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,19 +13,19 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import numpy as np
 from typing import List
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.graph.base_graph import Graph
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 
 
 class ModelCollector:
     """
     Build a Keras model from a graph for statistics collection purposes.
     The ModelCollector builds a float model that its outputs are all layers outputs, so after
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/model_validation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/model_validation.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from abc import abstractmethod
 from typing import Any
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 
 
 class ModelValidation:
     """
     Class to define validation methods in order to validate the received model to quantize.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/actions.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/actions.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,18 @@
 # limitations under the License.
 # ==============================================================================
 
 from abc import ABC, abstractmethod
 from collections import namedtuple
 from typing import Callable
 
-from model_compression_toolkit.core.common import Graph, Logger
+from model_compression_toolkit.core.common import Graph
+from model_compression_toolkit.logger import Logger
+
+
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
     get_activation_quantization_params_fn, get_weights_quantization_params_fn
 from model_compression_toolkit.core.common.quantization.quantization_fn_selection import \
     get_weights_quantization_fn
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/edit_network.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/edit_network.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import copy
 from typing import List
 
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.network_editors import EditRule
 
 
@@ -32,13 +31,12 @@
         groups of layers by how they should be quantized, etc.)
         network_editor: List of edit rules to apply to the graph.
 
     Returns:
         The graph after it has been applied the edit rules from the network editor list.
 
     """
-    # graph = copy.deepcopy(graph_to_edit)
     for edit_rule in network_editor:
         filtered_nodes = graph.filter(edit_rule.filter)
         for node in filtered_nodes:
             edit_rule.action.apply(node, graph, fw_info)
     # return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/network_editors/node_filters.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/network_editors/node_filters.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/node_prior_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/node_prior_info.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/candidate_node_quantization_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.constants import ACTIVATION_QUANTIZATION_CFG, WEIGHTS_QUANTIZATION_CFG, QC, \
+from model_compression_toolkit.constants import ACTIVATION_QUANTIZATION_CFG, WEIGHTS_QUANTIZATION_CFG, QC, \
     OP_CFG, ACTIVATION_QUANTIZATION_FN, WEIGHTS_QUANTIZATION_FN, ACTIVATION_QUANT_PARAMS_FN, WEIGHTS_QUANT_PARAMS_FN, \
     WEIGHTS_CHANNELS_AXIS
 from model_compression_toolkit.core.common.quantization.node_quantization_config import BaseNodeQuantizationConfig, \
     NodeWeightsQuantizationConfig, NodeActivationQuantizationConfig
 
 
 ##########################################
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/core_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/core_config.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/debug_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/debug_config.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/filter_nodes_candidates.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,30 +12,29 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 from typing import List
 
 from model_compression_toolkit.core.common import Graph, BaseNode
-from model_compression_toolkit.core.common.constants import FLOAT_BITWIDTH
+from model_compression_toolkit.constants import FLOAT_BITWIDTH
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 
 
-def filter_nodes_candidates(graph_to_filter: Graph):
+def filter_nodes_candidates(graph: Graph):
     """
     Filters the graph's nodes candidates configuration list.
     We apply this after mark activation operation to eliminate nodes that their activation are no longer being quantized
     from the mixed-precision search.
     Updating the lists is preformed inplace on the graph object.
 
     Args:
-        graph_to_filter: Graph for which to add quantization info to each node.
+        graph: Graph for which to add quantization info to each node.
     """
-    graph = copy.deepcopy(graph_to_filter)
     nodes = list(graph.nodes)
     for n in nodes:
         n.candidates_quantization_cfg = filter_node_candidates(node=n)
 
     return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/node_quantization_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/node_quantization_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,21 +14,21 @@
 # ==============================================================================
 
 
 from typing import Callable, Any
 
 import numpy as np
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
     get_activation_quantization_params_fn, get_weights_quantization_params_fn
 
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig, \
     QuantizationErrorMethod
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig
 
 
 ##########################################
 # Every node holds a quantization configuration
 # for its weights and activations quantization, and a different quantization
 # configuration for its activation quantization configuration.
 ##########################################
@@ -252,15 +252,15 @@
         self.weights_channels_axis = weights_channels_axis
         self.weights_quantization_params = {}
         self.weights_quantization_method = op_cfg.weights_quantization_method
         self.weights_error_method = qc.weights_error_method
         self.weights_n_bits = op_cfg.weights_n_bits
         self.weights_bias_correction = qc.weights_bias_correction
         self.weights_second_moment_correction = qc.weights_second_moment_correction
-        self.weights_per_channel_threshold = qc.weights_per_channel_threshold
+        self.weights_per_channel_threshold = op_cfg.weights_per_channel_threshold
         self.enable_weights_quantization = op_cfg.enable_weights_quantization
         self.min_threshold = qc.min_threshold
         self.l_p_value = qc.l_p_value
 
 
     @property
     def weights_error_method(self) -> QuantizationErrorMethod:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_analyzer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_analyzer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_config.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import math
 from enum import Enum
 
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD
+from model_compression_toolkit.constants import MIN_THRESHOLD
 
 
 class QuantizationErrorMethod(Enum):
     """
     Method for quantization threshold selection:
 
     NOCLIPPING - Use min/max values as thresholds.
@@ -90,15 +90,15 @@
             One may create a quantization configuration to quantize a model according to.
             For example, to quantize a model's weights and activation using thresholds, such that
             weights threshold selection is done using MSE, activation threshold selection is done using NOCLIPPING (min/max),
             enabling relu_bound_to_power_of_2, weights_bias_correction, and quantizing the weights per-channel,
             one can instantiate a quantization configuration:
 
             >>> import model_compression_toolkit as mct
-            >>> qc = mct.QuantizationConfig(activation_error_method=mct.QuantizationErrorMethod.NOCLIPPING,weights_error_method=mct.QuantizationErrorMethod.MSE,relu_bound_to_power_of_2=True,weights_bias_correction=True,weights_per_channel_threshold=True)
+            >>> qc = mct.core.QuantizationConfig(activation_error_method=mct.core.QuantizationErrorMethod.NOCLIPPING,weights_error_method=mct.core.QuantizationErrorMethod.MSE,relu_bound_to_power_of_2=True,weights_bias_correction=True,weights_per_channel_threshold=True)
 
 
             The QuantizationConfig instanse can then be passed to
             :func:`~model_compression_toolkit.ptq.keras_post_training_quantization`
 
         """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_fn_selection.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from collections.abc import Callable
 from functools import partial
 
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.core.common.quantization.quantizers.kmeans_quantizer import kmeans_quantizer
 from model_compression_toolkit.core.common.quantization.quantizers.lut_kmeans_quantizer import lut_kmeans_quantizer
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import power_of_two_quantizer, \
     symmetric_quantizer, uniform_quantizer
 
 
 def get_weights_quantization_fn(weights_quantization_method: QuantizationMethod) -> Callable:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_fn_selection.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from collections.abc import Callable
 from functools import partial
 
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.kmeans_params import kmeans_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.lut_kmeans_params import \
     lut_kmeans_tensor, lut_kmeans_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import \
     symmetric_selection_tensor, symmetric_selection_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.uniform_selection import \
     uniform_selection_histogram, uniform_selection_tensor
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/error_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,18 +13,19 @@
 # limitations under the License.
 # ==============================================================================
 from copy import deepcopy
 from typing import Tuple, Callable
 import numpy as np
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
 from model_compression_toolkit.core.common.similarity_analyzer import compute_mse, compute_mae, compute_lp_norm
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.constants import FLOAT_32
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.constants import FLOAT_32
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import uniform_quantize_tensor
 
+
 def _mse_error_histogram(q_bins: np.ndarray,
                          q_count: np.ndarray,
                          bins: np.ndarray,
                          counts: np.ndarray) -> np.float32:
     """
     Compute the error function between a histogram to its quantized version.
     The error is computed based on the mean square error the distributions have.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/kmeans_params.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
 from sklearn.cluster import KMeans
 
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
-from model_compression_toolkit.core.common.constants import CLUSTER_CENTERS, SCALE_PER_CHANNEL, MIN_THRESHOLD, EPS
+from model_compression_toolkit.constants import CLUSTER_CENTERS, SCALE_PER_CHANNEL, MIN_THRESHOLD, EPS
 
 
 def kmeans_tensor(tensor_data: np.ndarray,
                   p: int,
                   n_bits: int,
                   per_channel: bool = False,
                   channel_axis: int = 1,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/lut_kmeans_params.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,24 +13,24 @@
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
 from sklearn.cluster import KMeans
 
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
-from model_compression_toolkit.core.common.constants import CLUSTER_CENTERS, MIN_THRESHOLD, SCALE_PER_CHANNEL, \
+from model_compression_toolkit.constants import CLUSTER_CENTERS, MIN_THRESHOLD, SCALE_PER_CHANNEL, \
     MULTIPLIER_N_BITS, THRESHOLD
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import \
     max_power_of_two, int_quantization_with_threshold
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.symmetric_selection import \
     symmetric_selection_tensor
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.power_of_two_selection import \
     power_of_two_selection_tensor
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def lut_kmeans_tensor(tensor_data: np.ndarray,
                       p: int,
                       n_bits: int,
                       per_channel: bool = False,
                       channel_axis: int = 1,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/power_of_two_selection.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,21 +11,21 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import numpy as np
 
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD, THRESHOLD
+from model_compression_toolkit.constants import MIN_THRESHOLD, THRESHOLD
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_search import \
     qparams_selection_tensor_search, qparams_selection_histogram_search
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import max_power_of_two, get_tensor_max
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.error_functions import \
     get_threshold_selection_tensor_error_function, get_threshold_selection_histogram_error_function
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 def power_of_two_selection_tensor(tensor_data: np.ndarray,
                                   p: int,
                                   n_bits: int,
                                   per_channel: bool = False,
                                   channel_axis: int = 1,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_activations_computation.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,19 +9,19 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import numpy as np
-from typing import Tuple, Dict
+from typing import Dict
 
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
-from model_compression_toolkit.core.common.constants import SIGNED
+from model_compression_toolkit.constants import SIGNED
 from model_compression_toolkit.core.common.quantization import quantization_params_generation
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeActivationQuantizationConfig
 
 
 def get_activations_qparams(activation_quant_cfg: NodeActivationQuantizationConfig,
                             nodes_prior_info: NodePriorInfo,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_computation.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List
 
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common import Graph, BaseNode, Logger
+from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_weights_computation import \
     get_weights_qparams, get_channels_axis
 
 
 def calculate_quantization_params(graph: Graph,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_search.py`

 * *Files 0% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 # ==============================================================================
 import itertools
 from collections.abc import Callable
 from typing import Any, Tuple, Dict
 
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD, DEFAULT_TOL, DEFAULT_DEC_FACTOR, \
+from model_compression_toolkit.constants import MIN_THRESHOLD, DEFAULT_TOL, DEFAULT_DEC_FACTOR, \
     SYMMETRIC_TENSOR_PER_CHANNEL_N_INTERVALS, SYMMETRIC_TENSOR_PER_CHANNEL_N_ITER, SYMMETRIC_TENSOR_DEC_FREQ, \
     SYMMETRIC_TENSOR_PER_CHANNEL_DEC_FREQ, SYMMETRIC_TENSOR_N_INTERVALS, SYMMETRIC_TENSOR_N_ITER, \
     UNIFORM_TENSOR_PER_CHANNEL_N_ITER, UNIFORM_TENSOR_N_ITER, SYMMETRIC_HISTOGRAM_DEC_FREQ, SYMMETRIC_HISTOGRAM_N_ITER, \
     SYMMETRIC_HISTOGRAM_N_INTERVALS, UNIFORM_HISTOGRAM_N_ITER, BOTTOM_FACTOR, UPPER_FACTOR, UNIFORM_TENSOR_N_SAMPLES, \
     UNIFORM_HISTOGRAM_N_SAMPLES, DEC_RANGE_UPPER, DEC_RANGE_BOTTOM
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import quantize_tensor, \
     reshape_tensor_for_per_channel_search, uniform_quantize_tensor, get_output_shape
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/qparams_weights_computation.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Dict, Any, Tuple
 
 import numpy as np
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeWeightsQuantizationConfig
 
 
 # If the quantization config does not contain kernel channel mapping or the weights
 # quantization is not per-channel, we use a dummy channel mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/symmetric_selection.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,23 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import numpy as np
 
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD, THRESHOLD
+from model_compression_toolkit.constants import MIN_THRESHOLD, THRESHOLD
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.error_functions import \
     get_threshold_selection_tensor_error_function, get_threshold_selection_histogram_error_function, _kl_error_histogram
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_search import \
     qparams_symmetric_selection_tensor_search, \
     qparams_symmetric_selection_histogram_search, kl_qparams_symmetric_selection_histogram_search
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import \
     get_tensor_max
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 def symmetric_selection_tensor(tensor_data: np.ndarray,
                                p: int,
                                n_bits: int,
                                per_channel: bool = False,
                                channel_axis: int = 1,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantization_params_generation/uniform_selection.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,22 +11,22 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import numpy as np
 
 import model_compression_toolkit.core.common.quantization.quantization_config as qc
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD, RANGE_MIN, RANGE_MAX
+from model_compression_toolkit.constants import MIN_THRESHOLD, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_search import \
     qparams_uniform_selection_tensor_search, qparams_uniform_selection_histogram_search
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.error_functions import \
     get_threshold_selection_tensor_error_function, get_threshold_selection_histogram_error_function
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import get_tensor_max, \
     get_tensor_min
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 def uniform_selection_tensor(tensor_data: np.ndarray,
                              p: int,
                              n_bits: int,
                              per_channel: bool = False,
                              channel_axis: int = 1,
                              n_iter: int = 10,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantize_graph_weights.py`

 * *Files 8% similar despite different names*

```diff
@@ -16,43 +16,43 @@
 import copy
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_kernel_by_weights_qc
+from model_compression_toolkit.logger import Logger
 
 
-def quantize_graph_weights(graph_to_quantize: Graph,
+def quantize_graph_weights(graph: Graph,
                            fw_info: FrameworkInfo,
                            fw_impl: FrameworkImplementation) -> Graph:
     """
     Get a graph representing a model, and quantize its nodes' weights.
     Each node is quantized according to the passed framework info and quantization configuration.
     If weights bias correction is enabled in the quantization configuration, a bias correction term
     is calculated and subtracted from the original node's bias. The graph is quantized in-place.
 
     Args:
-        graph_to_quantize: Graph to quantize its nodes.
+        graph: Graph to quantize its nodes.
         fw_info: Framework information needed for quantizing the graph's nodes' weights and activations.
         fw_impl: FrameworkImplementation with specific framework implementations.
 
     """
-    graph = copy.deepcopy(graph_to_quantize)
     # Iterate over nodes in the graph and quantize each node's weights and activations
     # (according to operators groups in framework info).
     for n in graph.nodes():
 
         if n.is_weights_quantization_enabled():
             quantized_kernel, io_channels_axes = get_quantized_kernel_by_weights_qc(fw_info,
                                                                                     n,
                                                                                     n.final_weights_quantization_cfg,
                                                                                     fw_impl=fw_impl)
 
-            common.Logger.debug(
+            Logger.debug(
                 f'Node name: {n.name} has the following quantization params: '
                 f'{str(n.final_weights_quantization_cfg.weights_quantization_params)}')
 
             # Set the kernel node to be the quantized kernel.
             n.set_weights_by_keys(fw_impl.constants.KERNEL, quantized_kernel)
 
     return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantize_node.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantize_node.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeWeightsQuantizationConfig
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_weights_computation \
     import \
     get_channels_axis
@@ -42,15 +42,15 @@
     Returns:
         A quantized kernel of the node using a weights quantization configuration.
     """
 
     # If weights should be quantized per-channel but a kernel channels mapping is missing.
     if weights_qc.weights_per_channel_threshold and fw_info.kernel_channels_mapping is \
             None:
-        common.Logger.warning(
+        Logger.warning(
             'Weights Per Channel Quantization requires channel mapping function but framework info '
             'does not contain one')
     output_channels_axis, input_channels_axis = get_channels_axis(weights_qc,
                                                                   fw_info,
                                                                   n.type)
 
     Logger.debug(f'quantizing {n.name} with {weights_qc.weights_n_bits} bits')
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/kmeans_quantizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from sklearn.cluster import KMeans
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import CLUSTER_CENTERS, MIN_THRESHOLD, SCALE_PER_CHANNEL
+from model_compression_toolkit.constants import CLUSTER_CENTERS, MIN_THRESHOLD, SCALE_PER_CHANNEL
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import kmeans_assign_clusters
 
 
 def kmeans_quantizer(tensor_data: np.ndarray,
                         n_bits: int,
                         signed: bool,
                         quantization_params: dict,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/lut_kmeans_quantizer.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import CLUSTER_CENTERS, SCALE_PER_CHANNEL, \
+from model_compression_toolkit.constants import CLUSTER_CENTERS, SCALE_PER_CHANNEL, \
     MULTIPLIER_N_BITS
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import kmeans_assign_clusters, \
     get_quantized_tensor, int_quantization_with_threshold
 
 
 def lut_kmeans_quantizer(tensor_data: np.ndarray,
                         n_bits: int,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/quantizers_helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,16 +13,18 @@
 # limitations under the License.
 # ==============================================================================
 
 
 from typing import Tuple, List
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD, EPS
+from model_compression_toolkit.constants import MIN_THRESHOLD, EPS
 from model_compression_toolkit.core import common
+from model_compression_toolkit.logger import Logger
+
 
 def max_power_of_two(x: np.ndarray,
                      min_threshold: float = MIN_THRESHOLD) -> np.ndarray:
     """
     Compute the max power-of-two threshold for quantizing a tensor x. The threshold
     is determined by the maximal value of the tensor (or min_threshold, the greater one, if a
     minimal value needed to be enforced for the threshold calculation).
@@ -232,15 +234,15 @@
         n_bits: number of bits the tensor will be quantized with
         is_uniform_quantization (bool): Whether the tensor will be quantized with uniform quantization (min-max)
 
     Returns: maximal value (or values).
 
     """
     if n_bits < 1:
-        common.Logger.error("n_bits must be positive")
+        Logger.error("n_bits must be positive")
     if is_uniform_quantization:
         expansion_factor = 1.0
     elif n_bits == 1:
         expansion_factor = 0.0
     else:
         expansion_factor = np.power(2.0, n_bits - 1) / (np.power(2.0, n_bits - 1) - 1)
     if per_channel:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/quantizers/uniform_quantizers.py`

 * *Files 12% similar despite different names*

```diff
@@ -11,16 +11,16 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import numpy as np
 
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.constants import RANGE_MIN, RANGE_MAX, THRESHOLD
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import RANGE_MIN, RANGE_MAX, THRESHOLD
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import uniform_quantize_tensor, \
     quantize_tensor
 
 
 def threshold_is_power_of_two(threshold: np.ndarray, per_channel: bool) -> bool:
     if per_channel:
         thresholds_per_channel = threshold.flatten()
@@ -51,14 +51,17 @@
         Quantized data.
     """
     threshold = quantization_params.get(THRESHOLD)
     if threshold is None:
         Logger.error(f"{THRESHOLD} parameter must be defined in 'quantization_params'")  # pragma: no cover
     if not threshold_is_power_of_two(threshold, per_channel):
         Logger.error(f"Expects {THRESHOLD} parameter to be a power of two, but got {threshold}")  # pragma: no cover
+    if (per_channel and (threshold <= 0).any()) or ((not per_channel) and threshold <= 0):
+        Logger.error(f"{THRESHOLD} parameter must positive")  # pragma: no cover
+
 
     return quantize_tensor(tensor_data,
                            threshold,
                            n_bits,
                            signed)
 
 
@@ -83,14 +86,17 @@
     Returns:
         Quantized data.
     """
     threshold = quantization_params.get(THRESHOLD)
     if threshold is None:
         Logger.error(f"{THRESHOLD} parameter must be defined in 'quantization_params'")  # pragma: no cover
 
+    if (per_channel and np.any(threshold <= 0)) or (not per_channel and threshold <= 0):
+        Logger.error(f"{THRESHOLD} parameter must positive")  # pragma: no cover
+
     return quantize_tensor(tensor_data,
                            threshold,
                            n_bits,
                            signed)
 
 
 def uniform_quantizer(tensor_data: np.ndarray,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/quantization/set_node_quantization_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,27 +13,28 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import copy
 from typing import List
 
-from model_compression_toolkit.core.common import Logger, BaseNode
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeActivationQuantizationConfig
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.quantization.quantization_params_fn_selection import \
     get_activation_quantization_params_fn, get_weights_quantization_params_fn
 from model_compression_toolkit.core.common.quantization.quantization_fn_selection import \
     get_weights_quantization_fn
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import OpQuantizationConfig, \
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, \
     QuantizationConfigOptions
 
 
 def set_quantization_configuration_to_graph(graph: Graph,
                                             quant_config: QuantizationConfig,
                                             mixed_precision_enable: bool = False) -> Graph:
     """
@@ -44,22 +45,21 @@
         quant_config: Quantization configuration containing parameters for how the graph should be quantized.
         mixed_precision_enable: is mixed precision enabled
 
     Returns:
         The graph with quantization configurations attached to each node in it.
     """
 
-    graph_with_qcs = copy.deepcopy(graph)
-    for n in graph_with_qcs.nodes:
+    for n in graph.nodes:
         set_quantization_configs_to_node(node=n,
                                          quant_config=quant_config,
                                          fw_info=graph.fw_info,
                                          tpc=graph.tpc,
                                          mixed_precision_enable=mixed_precision_enable)
-    return graph_with_qcs
+    return graph
 
 
 def set_quantization_configs_to_node(node: BaseNode,
                                      quant_config: QuantizationConfig,
                                      fw_info: FrameworkInfo,
                                      tpc: TargetPlatformCapabilities,
                                      mixed_precision_enable: bool = False):
@@ -69,15 +69,15 @@
     Args:
         node: Node to set its quantization configurations.
         quant_config: Quantization configuration to generate the node's configurations from.
         fw_info: Information needed for quantization about the specific framework.
         tpc: TargetPlatformCapabilities to get default OpQuantizationConfig.
         mixed_precision_enable: is mixed precision enabled
     """
-    node_qc_options = tpc.get_qco_by_node(node)
+    node_qc_options = node.get_qco(tpc)
 
     # Create QC candidates for weights and activation combined
     weight_channel_axis = fw_info.kernel_channels_mapping.get(node.type)[0]
     node.candidates_quantization_cfg = _create_node_candidates_qc(quant_config,
                                                                   fw_info,
                                                                   weight_channel_axis,
                                                                   node_qc_options,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/similarity_analyzer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/similarity_analyzer.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,19 +9,19 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Any, Tuple
+from typing import Any
 
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import EPS
+from model_compression_toolkit.constants import EPS
 
 #########################
 #  Helpful functions
 #########################
 
 
 def validate_before_compute_similarity(float_tensor: Any, fxp_tensor: Any):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/apply_bias_correction_to_graph.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 
 
 def apply_bias_correction_to_graph(graph_to_apply_bias_correction: Graph,
                                    core_config: CoreConfig,
                                    fw_impl: FrameworkImplementation) -> Graph:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/apply_second_moment_correction_to_graph.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,15 +8,14 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import copy
 from typing import Callable, Any
 
 from tqdm import tqdm
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import FrameworkInfo
 from model_compression_toolkit.core.common import Graph
@@ -88,32 +87,31 @@
 
     quantized_model, user_info = fw_impl.model_builder(quantized_tg,
                                                        mode=ModelBuilderMode.FLOAT,
                                                        fw_info=fw_info)
     return quantized_model
 
 
-def apply_second_moment_correction_to_graph(graph_to_apply_second_moment_correction: Graph,
+def apply_second_moment_correction_to_graph(graph: Graph,
                                             representative_data_gen: Callable,
                                             core_config: CoreConfig,
                                             fw_info: FrameworkInfo,
                                             fw_impl: FrameworkImplementation) -> Graph:
     """
      Apply second moment correction on graph.
      Args:
-        graph_to_apply_second_moment_correction: Graph to apply second moment correction.
+        graph: Graph to apply second moment correction.
         representative_data_gen (Callable): Dataset used for calibration.
         core_config (CoreConfig): Configuration object containing parameters of how the model should be
          quantized, including mixed precision parameters.
         fw_info: FrameworkInfo object with information about the specific framework's model.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
      Returns:
          Graph after second moment correction.
      """
-    graph = copy.deepcopy(graph_to_apply_second_moment_correction)
     semi_quantized_model = quantized_model_builder_for_second_moment_correction(graph, fw_info, fw_impl)
     fw_impl.apply_second_moment_correction(semi_quantized_model, core_config, representative_data_gen, graph)
     graph = substitute(graph, fw_impl.get_substitutions_after_second_moment_correction(core_config.quantization_config))
     _collect_and_assign_act_threshold(graph, representative_data_gen, core_config, fw_info, fw_impl)
 
     return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/compute_bias_correction_of_graph.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,43 +14,43 @@
 # ==============================================================================
 
 import copy
 from typing import Any
 
 import numpy as np
 
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common import BaseNode, Logger, Graph
+from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.quantization.quantize_node import get_quantized_kernel_by_weights_qc
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
+from model_compression_toolkit.logger import Logger
 
 
-def compute_bias_correction_of_graph(graph_co_compute_bias: Graph,
+def compute_bias_correction_of_graph(graph: Graph,
                                      core_config: CoreConfig,
                                      fw_info: FrameworkInfo,
                                      fw_impl: FrameworkImplementation) -> Graph:
     """
     For each node in a graph, and for each candidate weights quantization configuration,
     compute the bias-correction term, and store it in the candidate weights quantization configuration.
 
     Args:
-        graph_co_compute_bias: Graph with nodes to compute the bias correction for
+        graph: Graph with nodes to compute the bias correction for
         each node's weights quantization configuration candidates.
         core_config: CoreConfig containing parameters of how the model should be quantized.
         fw_info: Framework info like lists of nodes their kernel should quantized.
         fw_impl: FrameworkImplementation object with a specific framework methods implementation.
 
     Returns:
         Graph with bias correction for each weights quantization configuration candidate
         for each node.
     """
 
-    graph = copy.deepcopy(graph_co_compute_bias)
     for n in graph.nodes:
         if n.is_weights_quantization_enabled() and core_config.quantization_config.weights_bias_correction:
             _compute_bias_correction_per_candidate_qc(n,
                                                       fw_info,
                                                       graph.get_in_stats_collector(n),
                                                       fw_impl=fw_impl)
     return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/statistics_correction/statistics_correction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/apply_substitutions.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/apply_substitutions.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,32 +9,29 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-import copy
-
 from typing import List
 
 from model_compression_toolkit.core import common
 
 
-def substitute(graph_to_substitute: common.Graph,
+def substitute(graph: common.Graph,
                substitutions_list: List[common.BaseSubstitution]) -> common.Graph:
     """
     Apply a list of substitutions on a graph.
     Args:
         graph: Graph to transform.
         substitutions_list: List of substitutions to apply on the graph.
 
     Returns:
         Transformed graph after applying all substitutions in substitutions_list.
     """
 
-    graph = copy.deepcopy(graph_to_substitute)
     for substitution in substitutions_list:
         matched_nodes = graph.filter(substitution.matcher_instance)
         for idn in matched_nodes:
             graph = substitution.substitute(graph, idn)
-    return graph
+    return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/batchnorm_folding.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/batchnorm_reconstruction.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,19 +16,19 @@
 
 import copy
 from typing import Callable
 
 import numpy as np
 
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 class BatchNormalizationReconstruction(common.BaseSubstitution):
     """
     Reconstruct BatchNormalization after linear layers.
     """
     def __init__(self,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/batchnorm_refusing.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 import numpy as np
 from typing import Tuple, Callable
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import EdgeMatcher, NodeOperationMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.constants import THRESHOLD, RANGE_MIN, RANGE_MAX
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.constants import THRESHOLD, RANGE_MIN, RANGE_MAX
+from model_compression_toolkit.logger import Logger
 
 
 class BatchNormalizationRefusing(common.BaseSubstitution):
     """
     Re-fuse BatchNormalization into preceding linear layers.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/linear_collapsing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/linear_collapsing.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 import copy
 import numpy as np
 from typing import Tuple, Callable
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import EdgeMatcher, NodeOperationMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 
 
 class Conv2DCollapsing(common.BaseSubstitution):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,16 +9,14 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-import copy
-
 from model_compression_toolkit.core import common
 
 
 def linear_collapsing_substitute(graph: common.Graph,
                                  linear_collapsing_substitution: common.BaseSubstitution) -> common.Graph:
     """
     Apply a list of linear collapsing substitutions on a graph.
@@ -28,15 +26,14 @@
     the matches are not valid anymore, and we can find new matches.
     Args:
         graph: Graph to transform.
         linear_collapsing_substitution: substitution to apply on the graph.
     Returns:
         Transformed graph after applying all linear collapsing substitutions.
     """
-    graph = copy.deepcopy(graph)
     matched_nodes = graph.filter(linear_collapsing_substitution.matcher_instance)
     matched_nodes_list = []
     match_indicator = True
     while len(matched_nodes) > 0 and match_indicator:
         match_indicator = False
         for matched_node in matched_nodes:
             if matched_node not in matched_nodes_list:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/residual_collapsing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/residual_collapsing.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/scale_equalization.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/shift_negative_activation.py`

 * *Files 0% similar despite different names*

```diff
@@ -12,19 +12,19 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 import numpy as np
 from typing import List, Tuple, Any, Callable
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import FrameworkInfo, Graph, BaseNode
-from model_compression_toolkit.core.common.constants import THRESHOLD, SIGNED, SHIFT_NEGATIVE_NON_LINEAR_NUM_BITS
+from model_compression_toolkit.constants import THRESHOLD, SIGNED, SHIFT_NEGATIVE_NON_LINEAR_NUM_BITS
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 from model_compression_toolkit.core.common.quantization.set_node_quantization_config import create_node_activation_qc, \
     set_quantization_configs_to_node
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.qparams_activations_computation \
     import get_activations_qparams
 from model_compression_toolkit.core.common.quantization.quantization_params_generation.error_functions import \
     _mse_error_histogram
@@ -352,15 +352,15 @@
     if bypass_nodes:
         for bypass_node in bypass_nodes:
             for bypass_candidate_qc in bypass_node.candidates_quantization_cfg:
                 if bypass_candidate_qc.activation_quantization_cfg:
                     bypass_candidate_qc.activation_quantization_cfg.activation_quantization_params[SIGNED] = False
                     graph.shift_stats_collector(bypass_node, np.array(shift_value))
 
-    add_node_qco = graph.tpc.get_qco_by_node(add_node).quantization_config_list
+    add_node_qco = add_node.get_qco(graph.tpc).quantization_config_list
     for op_qc_idx, candidate_qc in enumerate(add_node.candidates_quantization_cfg):
         candidate_qc.weights_quantization_cfg.enable_weights_quantization = False
 
         candidate_qc.activation_quantization_cfg = create_node_activation_qc(core_config.quantization_config,
                                                                              fw_info,
                                                                              add_node_qco[op_qc_idx])
 
@@ -491,15 +491,15 @@
     Returns:
         Graph after applying shift negative on selected activations.
     """
 
     nodes = list(graph.nodes())
     for n in nodes:
         # Skip substitution if QuantizationMethod is uniform.
-        node_qco = graph.tpc.get_qco_by_node(n)
+        node_qco = n.get_qco(graph.tpc)
         if any([op_qc.activation_quantization_method is QuantizationMethod.UNIFORM
                 for op_qc in node_qco.quantization_config_list]):
             continue
 
         if snc_node_types.apply(n):
             linear_node, pad_node, bypass_nodes = get_next_nodes_to_correct(n,
                                                                             graph,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/softmax_shift.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from model_compression_toolkit.core.common import BaseNode, Graph, BaseSubstitution
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualActivationWeightsNode
 
 
 class BaseVirtualActivationWeightsComposition(BaseSubstitution):
     def __init__(self, matcher_instance):
         super().__init__(matcher_instance=matcher_instance)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/substitutions/weights_activation_split.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/substitutions/weights_activation_split.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import itertools
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common import BaseNode, Graph, BaseSubstitution
 from model_compression_toolkit.core.common.graph.virtual_activation_weights_node import VirtualSplitWeightsNode, \
     VirtualSplitActivationNode
 from model_compression_toolkit.core.common.matchers.base_matcher import BaseMatcher
 
 
 class BaseWeightsActivationSplit(BaseSubstitution):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/current_tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 def get_current_tp_model():
     """
 
     Returns: The current TargetPlatformModel that is being used and accessed.
 
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/fusing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py`

 * *Files 7% similar despite different names*

```diff
@@ -12,16 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 
 from typing import Any
 
-from model_compression_toolkit.core.common.target_platform.operators import OperatorSetConcat
-from model_compression_toolkit.core.common.target_platform.target_platform_model_component import TargetPlatformModelComponent
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model_component import TargetPlatformModelComponent
 
 
 class Fusing(TargetPlatformModelComponent):
 
     def __init__(self, operator_groups_list, name=None):
         assert isinstance(operator_groups_list,
                           list), f'List of operator groups should be of type list but is {type(operator_groups_list)}'
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/op_quantization_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py`

 * *Files 3% similar despite different names*

```diff
@@ -10,41 +10,17 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import copy
-from enum import Enum
 from typing import List
 
-
-class QuantizationMethod(Enum):
-    """
-    Method for quantization function selection:
-
-    POWER_OF_TWO - Symmetric, uniform, threshold is power of two quantization.
-
-    KMEANS - k-means quantization.
-
-    LUT_POT_QUANTIZER - quantization using a lookup table and power of 2 threshold.
-
-    SYMMETRIC - Symmetric, uniform, quantization.
-
-    UNIFORM - uniform quantization,
-
-    LUT_SYM_QUANTIZER - quantization using a lookup table and symmetric threshold.
-
-    """
-    POWER_OF_TWO = 0
-    KMEANS = 1
-    LUT_POT_QUANTIZER = 2
-    SYMMETRIC = 3
-    UNIFORM = 4
-    LUT_SYM_QUANTIZER = 5
+from mct_quantizers import QuantizationMethod
 
 
 class OpQuantizationConfig:
     """
     OpQuantizationConfig is a class to configure the quantization parameters of an operator.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/operators.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/operators.py`

 * *Files 14% similar despite different names*

```diff
@@ -10,18 +10,18 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Dict, Any
 
-from model_compression_toolkit.core.common.constants import OPS_SET_LIST
-from model_compression_toolkit.core.common.target_platform.target_platform_model_component import TargetPlatformModelComponent
-from model_compression_toolkit.core.common.target_platform.current_tp_model import _current_tp_model
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import QuantizationConfigOptions
+from model_compression_toolkit.target_platform_capabilities.constants import OPS_SET_LIST
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model_component import TargetPlatformModelComponent
+from model_compression_toolkit.target_platform_capabilities.target_platform.current_tp_model import _current_tp_model
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import QuantizationConfigOptions
 
 
 class OperatorsSetBase(TargetPlatformModelComponent):
     """
     Base class to represent a set of operators.
     """
     def __init__(self, name: str):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/target_platform_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py`

 * *Files 9% similar despite different names*

```diff
@@ -12,24 +12,24 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import pprint
 from typing import Any, Dict
 
-from model_compression_toolkit.core.common.target_platform.current_tp_model import _current_tp_model, \
+from model_compression_toolkit.target_platform_capabilities.target_platform.current_tp_model import _current_tp_model, \
     get_current_tp_model
-from model_compression_toolkit.core.common.target_platform.fusing import Fusing
-from model_compression_toolkit.core.common.target_platform.target_platform_model_component import \
+from model_compression_toolkit.target_platform_capabilities.target_platform.fusing import Fusing
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model_component import \
     TargetPlatformModelComponent
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import OpQuantizationConfig, \
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, \
     QuantizationConfigOptions
-from model_compression_toolkit.core.common.target_platform.operators import OperatorsSetBase
-from model_compression_toolkit.core.common.immutable import ImmutableClass
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorsSetBase
+from model_compression_toolkit.target_platform_capabilities.immutable import ImmutableClass
+from model_compression_toolkit.logger import Logger
 
 
 def get_default_quantization_config_options() -> QuantizationConfigOptions:
     """
 
     Returns: The default QuantizationConfigOptions of the model. This is the options
     to use when a layer's options is queried and it wasn't specified in the TargetPlatformCapabilities.
@@ -219,7 +219,16 @@
     def show(self):
         """
 
         Display the TargetPlatformModel.
 
         """
         pprint.pprint(self.get_info(), sort_dicts=False)
+
+    def set_quantization_format(self,
+                                quantization_format: Any):
+        """
+        Set quantization format.
+        Args:
+            quantization_format: A quantization format (fake-quant, int8 etc.) from enum QuantizationFormat.
+        """
+        self.quantization_format = quantization_format
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/target_platform_model_component.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Any, Dict
 
-from model_compression_toolkit.core.common.target_platform.current_tp_model import _current_tp_model
+from model_compression_toolkit.target_platform_capabilities.target_platform.current_tp_model import _current_tp_model
 
 
 class TargetPlatformModelComponent:
     """
     Component of TargetPlatformModel (Fusing, OperatorsSet, etc.)
     """
     def __init__(self, name: str):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -9,20 +9,21 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.current_tpc import get_current_tpc
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.target_platform_capabilities import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.attribute_filter import \
-    Eq, GreaterEq, NotEq, SmallerEq, Greater, Smaller
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.layer_filter_params import \
-    LayerFilterParams
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.operations_to_layers import \
-    OperationsToLayers, OperationsSetToLayers
-
-
+from model_compression_toolkit.target_platform_capabilities.target_platform.fusing import Fusing
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
+    TargetPlatformCapabilities, OperationsSetToLayers, Smaller, SmallerEq, NotEq, Eq, GreaterEq, Greater, LayerFilterParams, OperationsToLayers, get_current_tpc
+
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model import \
+    get_default_quantization_config_options, TargetPlatformModel
+
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import OpQuantizationConfig, \
+    QuantizationConfigOptions
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorsSet, OperatorSetConcat
 
+from mct_quantizers import QuantizationMethod
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/attribute_filter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import operator
 from typing import Any, Callable, Dict
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 class Filter:
     """
     Filter a layer configuration by its attributes.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/current_tpc.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/layer_filter_params.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,18 +9,16 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Any, Dict
-
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.attribute_filter import AttributeFilter
+from typing import Any
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import AttributeFilter
 
 
 class LayerFilterParams:
     """
     Wrap a layer with filters to filter framework's layers by their attributes.
     """
 
@@ -83,38 +81,38 @@
         conditions and keyword arguments. Used for display and hashing.
 
         """
         params = [f'{k}={v}' for k,v in self.kwargs.items()]
         params.extend([str(c) for c in self.conditions])
         params_str = ', '.join(params)
         return f'{self.layer.__name__}({params_str})'
-
-    def match(self,
-              node: BaseNode) -> bool:
-        """
-        Check if a node matches the layer, conditions and keyword-arguments of
-        the LayerFilterParams.
-
-        Args:
-            node: Node to check if matches to the LayerFilterParams properties.
-
-        Returns:
-            Whether the node matches to the LayerFilterParams properties.
-        """
-        # Check the node has the same type as the layer in LayerFilterParams
-        if self.layer != node.type:
-            return False
-
-        # Get attributes from node to filter
-        layer_config = node.framework_attr
-        if hasattr(node, "op_call_kwargs"):
-            layer_config.update(node.op_call_kwargs)
-
-        for attr, value in self.kwargs.items():
-            if layer_config.get(attr) != value:
-                return False
-
-        for c in self.conditions:
-            if not c.match(layer_config):
-                return False
-
-        return True
+    #
+    # def match(self,
+    #           node: BaseNode) -> bool:
+    #     """
+    #     Check if a node matches the layer, conditions and keyword-arguments of
+    #     the LayerFilterParams.
+    #
+    #     Args:
+    #         node: Node to check if matches to the LayerFilterParams properties.
+    #
+    #     Returns:
+    #         Whether the node matches to the LayerFilterParams properties.
+    #     """
+    #     # Check the node has the same type as the layer in LayerFilterParams
+    #     if self.layer != node.type:
+    #         return False
+    #
+    #     # Get attributes from node to filter
+    #     layer_config = node.framework_attr
+    #     if hasattr(node, "op_call_kwargs"):
+    #         layer_config.update(node.op_call_kwargs)
+    #
+    #     for attr, value in self.kwargs.items():
+    #         if layer_config.get(attr) != value:
+    #             return False
+    #
+    #     for c in self.conditions:
+    #         if not c.match(layer_config):
+    #             return False
+    #
+    #     return True
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/operations_to_layers.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,18 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import List, Any
 
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.current_tpc import  _current_tpc
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.target_platform_capabilities_component import TargetPlatformCapabilitiesComponent
-from model_compression_toolkit.core.common.target_platform.operators import OperatorsSet, OperatorSetConcat, \
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.current_tpc import  _current_tpc
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.target_platform_capabilities_component import TargetPlatformCapabilitiesComponent
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorSetConcat, \
     OperatorsSetBase
 
 
 
 class OperationsSetToLayers(TargetPlatformCapabilitiesComponent):
     """
     Associate an OperatorsSet to a list of framework's layers.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/target_platform_capabilities.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py`

 * *Files 8% similar despite different names*

```diff
@@ -14,26 +14,25 @@
 # ==============================================================================
 
 
 import itertools
 import pprint
 from typing import List, Any, Dict, Tuple
 
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.operations_to_layers import \
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.operations_to_layers import \
     OperationsToLayers, OperationsSetToLayers
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.target_platform_capabilities_component import TargetPlatformCapabilitiesComponent
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.layer_filter_params import LayerFilterParams
-from model_compression_toolkit.core.common.immutable import ImmutableClass
-from model_compression_toolkit.core.common.graph.base_node import BaseNode
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import QuantizationConfigOptions, \
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.target_platform_capabilities_component import TargetPlatformCapabilitiesComponent
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.layer_filter_params import LayerFilterParams
+from model_compression_toolkit.target_platform_capabilities.immutable import ImmutableClass
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import QuantizationConfigOptions, \
     OpQuantizationConfig
-from model_compression_toolkit.core.common.target_platform.operators import OperatorsSet, OperatorsSetBase
-from model_compression_toolkit.core.common.target_platform.target_platform_model import TargetPlatformModel
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.current_tpc import _current_tpc
+from model_compression_toolkit.target_platform_capabilities.target_platform.operators import OperatorsSetBase
+from model_compression_toolkit.target_platform_capabilities.target_platform.target_platform_model import TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.current_tpc import _current_tpc
 
 
 class TargetPlatformCapabilities(ImmutableClass):
     """
     Attach framework information to a modeled hardware.
     """
     def __init__(self,
@@ -159,34 +158,14 @@
 
         Returns: The default OpQuantizationConfig of the TargetPlatformModel that is attached
         to the TargetPlatformCapabilities.
 
         """
         return self.tp_model.get_default_op_quantization_config()
 
-    def get_qco_by_node(self,
-                        node: BaseNode) -> QuantizationConfigOptions:
-        """
-        Get the QuantizationConfigOptions of a node in a graph according
-        to the mappings from layers/LayerFilterParams to the OperatorsSet in the TargetPlatformModel.
-
-        Args:
-            node: Node from graph to get its QuantizationConfigOptions.
-
-        Returns:
-            QuantizationConfigOptions of the node.
-        """
-        if node is None:
-            Logger.error(f'Can not retrieve QC options for None node')  # pragma: no cover
-        for fl, qco in self.filterlayer2qco.items():
-            if fl.match(node):
-                return qco
-        if node.type in self.layer2qco:
-            return self.layer2qco.get(node.type)
-        return self.tp_model.default_qco
 
     def _get_config_options_mapping(self) -> Tuple[Dict[Any, QuantizationConfigOptions],
                                                    Dict[LayerFilterParams, QuantizationConfigOptions]]:
         """
         Build mapping from layers to their QuantizationConfigOptions (and from LayerFilterParams
         to their QuantizationConfigOptions).
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/target_platform/targetplatform2framework/target_platform_capabilities_component.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,20 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from enum import Enum
 
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.current_tpc import  _current_tpc
 
-
-class TargetPlatformCapabilitiesComponent:
-    def __init__(self, name: str):
-        self.name = name
-        _current_tpc.get().append_component(self)
-
-    def get_info(self):
-        return {}
+class QuantizationFormat(Enum):
+    FAKELY_QUANT = 0
+    INT8 = 1
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/user_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/user_info.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/final_config_visualizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/final_config_visualizer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/nn_visualizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/nn_visualizer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/common/visualization/tensorboard_writer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/common/visualization/tensorboard_writer.py`

 * *Files 0% similar despite different names*

```diff
@@ -27,15 +27,15 @@
 from tensorboard.compat.proto.step_stats_pb2 import StepStats, NodeExecStats, DeviceStepStats, AllocatorMemoryUsed
 from tensorboard.compat.proto.summary_pb2 import HistogramProto
 from tensorboard.compat.proto.summary_pb2 import Summary
 from tensorboard.compat.proto.tensor_shape_pb2 import TensorShapeProto
 from tensorboard.summary.writer.event_file_writer import EventFileWriter
 from typing import List, Any, Dict
 from networkx import topological_sort
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 
 DEVICE_STEP_STATS = "/device:CPU:0"
 
 
 def get_node_properties(node_dict_to_log: dict,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/exporter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/exporter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/factory_model_builder.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.keras.back2framework.float_model_builder import FloatKerasModelBuilder
 from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
 from model_compression_toolkit.core.keras.back2framework.mixed_precision_model_builder import \
     MixedPrecisionKerasModelBuilder
 from model_compression_toolkit.core.keras.back2framework.quantized_model_builder import QuantizedKerasModelBuilder
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/float_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/float_model_builder.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.core import common
 from tensorflow.python.util.object_identity import Reference as TFReference
 
 class FloatKerasModelBuilder(KerasModelBuilder):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/instance_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/instance_builder.py`

 * *Files 13% similar despite different names*

```diff
@@ -27,22 +27,30 @@
 
 
 class OperationHandler:
     """
     Class to handle conversions from graph nodes to Keras operators and retrieving them.
     """
 
-    def __init__(self, graph: Graph):
+    def __init__(self, graph: Graph, wrapper: Callable = None):
+        """
+
+        Args:
+            graph: Graph to build its layers based on its nodes.
+            wrapper: Wrapper to use for wrapping the layers.
+        """
+
         # hold nodes after sorting them
         self.node_sort = list(topological_sort(graph))
 
         self.layer_to_node_dict = {}
 
         # hold dictionary from node to its equivalent Keras layer
-        self.node_to_fw_op_dict = instance_builder(self.node_sort)
+        self.node_to_fw_op_dict = instance_builder(self.node_sort,
+                                                   wrapper)
 
     def get_node_op_function(self, n: BaseNode) -> Layer:
         """
         Get the Keras layer that was built from the passed node.
 
         Args:
             n: Node to get its equivalent Keras layer.
@@ -70,42 +78,41 @@
     Args:
         n: Node to build its Keras layer
 
     Returns:
         Keras layer that was built from the node.
     """
     framework_attr = copy.copy(n.framework_attr)
-    if n.layer_class is InputLayer:
-        # replace input node with identity, so can wrap it with QuantizationWrapper
-        _layer_class = Layer  # Identity
-        framework_attr = {}
-    else:
-        _layer_class = n.layer_class
+    _layer_class = n.layer_class
     framework_attr[LAYER_NAME] = n.name  # Overwrite framework name to identical graph node name
     node_instance = _layer_class.from_config(framework_attr)  # Build layer from node's configuration.
     with tf.name_scope(n.name):
         # Add layer name to default weight name to avoid name duplications
         node_instance.build(n.input_shape)
     node_instance.set_weights(n.get_weights_list())
     node_instance.trainable = False  # Set all node as not trainable
     return node_instance
 
 
-def instance_builder(toposort: List[BaseNode]) -> Dict[BaseNode, Layer]:
+def instance_builder(toposort: List[BaseNode],
+                     wrapper: Callable = None) -> Dict[BaseNode, Layer]:
     """
     Build a dictionary of nodes to their corresponding Keras
     layers, given a list of nodes.
 
     Args:
         toposort: List of nodes sorted topological to build their layers.
+        wrapper: Wrapper to use for wrapping the layers.
 
     Returns:
         A dictionary of nodes to their corresponding Keras layers.
     """
 
     nodes_dict = dict()
     for n in toposort:
         if not n.reuse:  # Hold a single node in dictionary for all reused nodes from the same layer.
             keras_node = node_builder(n)
+            if wrapper is not None:
+                keras_node = wrapper(n, keras_node)
             nodes_dict.update({n: keras_node})
 
     return nodes_dict
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/keras_model_builder.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,24 +9,23 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from abc import abstractmethod
-
 import tensorflow as tf
 from keras.engine.input_layer import InputLayer
 from keras.models import Model, clone_model
 from packaging import version
 
+from model_compression_toolkit.constants import INPUT_BASE_NAME
 from model_compression_toolkit.core.common.back2framework.base_model_builder import BaseModelBuilder
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.core.common.constants import INPUT_BASE_NAME
+from mct_quantizers import KerasActivationQuantizationHolder
 
 # As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Input
     from tensorflow.python.keras.layers.core import TFOpLambda
     from tensorflow.python.keras.engine.base_layer import TensorFlowOpLayer
     from tensorflow.python.keras.layers import Layer
@@ -34,15 +33,14 @@
     from keras import Input
     from keras.layers.core import TFOpLambda
     from keras.engine.base_layer import TensorFlowOpLayer, Layer
 
 from typing import Any, Dict, List, Tuple, Callable
 from tensorflow.python.util.object_identity import Reference as TFReference
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
-from model_compression_toolkit.core.common.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.keras.back2framework.instance_builder import OperationHandler
 from model_compression_toolkit.core.keras.reader.connectivity_handler import OutTensor
@@ -91,33 +89,50 @@
     """
 
     def __init__(self,
                  graph: common.Graph,
                  append2output=None,
                  fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
                  return_float_outputs: bool = False,
-                 wrapper: Callable = None):
+                 wrapper: Callable = None,
+                 get_activation_quantizer_holder_fn: Callable=None):
         """
 
         Args:
             graph: Graph to build the model from.
             append2output: Nodes to append to model's output.
             fw_info: Information about the specific framework of the model that is built.
             return_float_outputs: Whether the model returns float tensors or not.
             wrapper: A function wrapper keras Layers.
+            get_activation_quantizer_holder_fn: Function to retrieve a quantization holder for a node.
+
         """
 
         super().__init__(graph,
                          append2output,
                          fw_info,
                          return_float_outputs)
 
         # Build an OperationHandler to handle conversions from graph nodes to Keras operators.
-        self.oh = OperationHandler(self.graph)
+        self.oh = OperationHandler(self.graph,
+                                   wrapper=wrapper)
         self.wrapper = wrapper
+        self.get_activation_quantizer_holder = get_activation_quantizer_holder_fn
+
+    @property
+    def use_activation_holder_during_model_building(self) -> bool:
+        """
+
+        Returns: Whether the model builder uses KerasActivationQuantizationHolder during
+        model building (by adding it as a layer when converting the graph to the Keras model)
+        or not. If so - the model builder expects the activation quantizers to not be wrapped
+        in KerasQuantizeWrapper that was received in its init.
+
+        """
+        return self.get_activation_quantizer_holder is not None
 
     def _quantize_node_activations(self,
                                    node: BaseNode,
                                    input_tensors: List[TFReference]) -> List[TFReference]:
         """
         Quantize node's activation given input tensors.
 
@@ -154,46 +169,41 @@
         # building the model. Initially input nodes with input tensors are added to the dictionary,
         # as they're not added later.
         input_nodes_to_input_tensors = {inode: Input(inode.framework_attr[BATCH_INPUT_SHAPE][1:],
                                                      name=f'{inode.name}_{INPUT_BASE_NAME}')
                                         for
                                         inode in self.graph.get_inputs()}
 
-        # Support adding Layer after input layers require us to store it in layer_to_node_dict
-        # dict offline (unlike other layers which stored during running).
-        for node, layer in self.oh.node_to_fw_op_dict.items():
-            if node.type == InputLayer:
-                self.oh.layer_to_node_dict[layer] = node
 
         # Build a list of the model's input tensors. Switching from a dictionary to a list
         # to keep the tensors input order, since inputs in Graph are ordered by their indices.
         inputs_list = []
         for input_node in self.graph.get_inputs():
             inputs_list.append(input_nodes_to_input_tensors.get(input_node))
 
         # Build a dictionary from node to its output tensors, by applying the layers sequentially.
         for n in self.oh.node_sort:
-            op_func = self.oh.get_node_op_function(n)  # Get node operation function
+            op_func = self.oh.get_node_op_function(n) # Get node operation function
+
             input_tensors = self._build_input_tensors_list(n,
                                                            node_to_output_tensors_dict)  # Fetch Node inputs
             out_tensors_of_n, out_tensors_of_n_float = self._run_operation(n,  # Run node operation and fetch outputs
                                                                            input_tensors,
                                                                            op_func,
                                                                            input_nodes_to_input_tensors)
 
             if isinstance(out_tensors_of_n, (list, tuple)):
                 node_to_output_tensors_dict.update({n: out_tensors_of_n})
                 node_to_output_tensors_dict_float.update({n: out_tensors_of_n_float})
             else:
                 node_to_output_tensors_dict.update({n: [out_tensors_of_n]})
                 node_to_output_tensors_dict_float.update({n: [out_tensors_of_n_float]})
 
-        # convert node_to_output_tensors_dict keys to nodes' names since oh.node_sort contains different objects
-        # than
-        # original graph nodes.
+        # convert node_to_output_tensors_dict keys to nodes' names since oh.node_sort
+        # contains different objects than original graph nodes.
         node_name_to_outtensors = self._convert_node2name(node_to_output_tensors_dict)
         node_name_to_outtensors_float = self._convert_node2name(node_to_output_tensors_dict_float)
 
         for ot in output_list:
             if len(node_name_to_outtensors[ot.node.name]) == 1 or self.append2output is None:
                 if self.return_float_outputs:
                     model_output_tensors.append(node_name_to_outtensors_float[ot.node.name][ot.node_out_index])
@@ -206,27 +216,14 @@
                     model_output_tensors.append(node_name_to_outtensors_float[ot.node.name])
                 else:
                     model_output_tensors.append(node_name_to_outtensors[ot.node.name])
 
         # Build the model.
         model = tf.keras.Model(inputs=inputs_list, outputs=model_output_tensors)
 
-        if self.wrapper is not None:
-            def _wrap(layer):
-                _node = self.oh.layer_to_node_dict.get(layer)
-                if _node is not None:
-                    return self.wrapper(_node, layer)
-                elif is_layer_fake_quant(layer):
-                    return layer
-                raise Exception(  # pragma: no cover
-                    f'Mismatch between keras model and graph cant find node named: '
-                    f'{get_node_name_from_layer(layer)}')
-
-            model = clone_model(model, clone_function=_wrap)
-
         return model, self.graph.user_info
 
     def _convert_node2name(self, in_node_to_output_tensors_dict):
         node_name_to_outtensors = dict()
         for node, tensors in in_node_to_output_tensors_dict.items():
             node_name_to_outtensors[node.name] = tensors
         return node_name_to_outtensors
@@ -274,47 +271,69 @@
 
         Returns:
             A list of references to Keras tensors. The layer's output tensors after applying the
             layer to the input tensors.
         """
         if len(input_tensors) == 0:  # Placeholder handling
             out_tensors_of_n_float = input_nodes_to_input_tensors[n]
-            if self.wrapper is not None:
-                # if a wrapper is defined, add an identity layer for cloning. The Identity will be warpped
-                out_tensors_of_n = op_func(out_tensors_of_n_float)
-            elif n.is_activation_quantization_enabled():
-                out_tensors_of_n = self._quantize_node_activations(n, out_tensors_of_n_float)
-            else:
-                out_tensors_of_n = out_tensors_of_n_float
+            out_tensors_of_n = self._run_operation_activation_quantization(n,
+                                                                           out_tensors_of_n_float)
         else:
             input_tensors = [tensor for tensor_list in input_tensors for tensor in tensor_list]  # flat list of lists
             # Build a functional node using its args
             if isinstance(n, FunctionalNode):
                 if n.inputs_as_list:  # If the first argument should be a list of tensors:
                     out_tensors_of_n_float = op_func(input_tensors, *n.op_call_args, **n.op_call_kwargs)
                 else:  # If the input tensors should not be a list but iterated:
                     out_tensors_of_n_float = op_func(*input_tensors, *n.op_call_args, **n.op_call_kwargs)
             else:
                 # If operator expects a single input tensor, it cannot be a list as it should
                 # have a dtype field.
                 if len(input_tensors) == 1:
                     input_tensors = input_tensors[0]
                 out_tensors_of_n_float = op_func(input_tensors)
-            out_tensors_of_n = out_tensors_of_n_float
 
-            # Add a fake quant node if the node has an activation threshold and a wrapper isn't defined
-            if n.is_activation_quantization_enabled() and self.wrapper is None:
-                out_tensors_of_n = self._quantize_node_activations(n, out_tensors_of_n_float)
+            out_tensors_of_n = self._run_operation_activation_quantization(n,
+                                                                           out_tensors_of_n_float)
 
         # Save a mapping from the layer that created the tensor to the node (as this layer is not the
         # same instance as op_func. We do this to solve an issue that names are different between these
         # layers, thus we can not rely on the op_func name during model cloning (such as GPTQ, MP, etc.)
         if not n.reuse:
             if isinstance(out_tensors_of_n_float, (list, tuple)):
                 # If layer has multiple outputs (e.g., split) we take the layer of the first output (as all outputs are
                 # from the same layer).
                 layer_from_tensor = out_tensors_of_n_float[0].node.layer
             else:
                 layer_from_tensor = out_tensors_of_n_float.node.layer
             self.oh.layer_to_node_dict[layer_from_tensor] = n
 
         return out_tensors_of_n, out_tensors_of_n_float
+
+    def _run_operation_activation_quantization(self,
+                                               node: BaseNode,
+                                               node_outputs: List[TFReference]):
+        """
+        Quantize node's activations
+
+        Args:
+            node: Node to quantize its activations
+            node_outputs: Output tensors of the float node.
+
+        Returns:
+            Quantized node's outputs.
+        """
+        if self.wrapper is not None:
+
+            # In case the activation quantizer is attached out of the wrapper we use get_activation_quantizer_holder
+            # for the activation quantization holder (if the node's activations are quantized)
+            if node.is_activation_quantization_enabled() and self.use_activation_holder_during_model_building:
+                activation_quantizer_holder = self.get_activation_quantizer_holder(node)
+                quantized_node_outputs = activation_quantizer_holder(node_outputs)
+                return quantized_node_outputs
+
+        elif node.is_activation_quantization_enabled():  # Used only when old exporter is used
+            quantized_node_outputs = self._quantize_node_activations(node,
+                                                                     node_outputs)
+            return quantized_node_outputs
+
+        return node_outputs
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,15 +32,15 @@
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.python.keras.layers.core import TFOpLambda, SlicingOpLambda  # pragma: no cover
 else:
     from keras.layers.core import TFOpLambda, SlicingOpLambda
 
 from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from model_compression_toolkit.core.keras.quantizer.mixed_precision.quantization_config_factory import \
     quantization_config_builder_mixed_precision
 from tensorflow.python.util.object_identity import Reference as TFReference
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/model_gradients.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/model_gradients.py`

 * *Files 1% similar despite different names*

```diff
@@ -22,21 +22,21 @@
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.python.keras.layers import Layer  # pragma: no cover
 else:
     from keras.engine.base_layer import Layer
 
 from typing import Any, Dict, List, Tuple
 from tensorflow.python.util.object_identity import Reference as TFReference
-from model_compression_toolkit.core.common.constants import EPS, MIN_JACOBIANS_ITER, JACOBIANS_COMP_TOLERANCE
+from model_compression_toolkit.constants import EPS, MIN_JACOBIANS_ITER, JACOBIANS_COMP_TOLERANCE
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.keras.back2framework.instance_builder import OperationHandler
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def build_input_tensors_list(node: BaseNode,
                              graph: Graph,
                              node_to_output_tensors_dict: Dict[BaseNode, List[TFReference]]) -> List[List[TFReference]]:
     """
     Given a node, build a list of input tensors the node gets. The list is built
@@ -167,16 +167,17 @@
                     # Computing the jacobian approximation by getting the gradient of (output * v)
                     jac_v = g.gradient(f_v, ipt, unconnected_gradients=tf.UnconnectedGradients.ZERO)
                     jac_v = tf.reshape(jac_v, [jac_v.shape[0], -1])
                     jac_trace_approx = tf.reduce_mean(tf.reduce_sum(tf.pow(jac_v, 2.0)))
 
                     # If the change to the mean Jacobian approximation is insignificant we stop the calculation
                     if j > MIN_JACOBIANS_ITER:
-                        delta = np.mean([jac_trace_approx, *trace_jv]) - np.mean(trace_jv)
-                        if np.abs(delta) / (np.abs(np.mean(trace_jv)) + 1e-6) < JACOBIANS_COMP_TOLERANCE:
+                        new_mean = np.mean([jac_trace_approx, *trace_jv])
+                        delta = new_mean - np.mean(trace_jv)
+                        if np.abs(delta) / (np.abs(new_mean) + 1e-6) < JACOBIANS_COMP_TOLERANCE:
                             trace_jv.append(jac_trace_approx)
                             break
 
                     trace_jv.append(jac_trace_approx)
             ipts_jac_trace_approx.append(2 * tf.reduce_mean(trace_jv) / output.shape[-1])  # Get averaged squared jacobian trace approximation
 
         ipts_jac_trace_approx = tf.reduce_mean([ipts_jac_trace_approx], axis=0)  # Just to get one tensor instead of list of tensors with single element
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/back2framework/quantized_model_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
 from tensorflow.python.util.object_identity import Reference as TFReference
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/constants.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/constants.py`

 * *Files 5% similar despite different names*

```diff
@@ -95,17 +95,10 @@
 V_BIAS = '/value/bias'
 OUTPUT_KERNEL = '/attention_output/kernel'
 OUTPUT_BIAS = '/attention_output/bias'
 
 # ReLU bound constants
 RELU_POT_BOUND = 8.0
 
-# Supported TP models names for Tensorflow:
-DEFAULT_TP_MODEL = 'default'
-IMX500_TP_MODEL = 'imx500'
-TFLITE_TP_MODEL = 'tflite'
-QNNPACK_TP_MODEL = 'qnnpack'
-
-
 # TFOpLambda functions:
 ADD = 'add'
 PAD = 'pad'
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/default_framework_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/default_framework_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,17 +21,17 @@
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Conv2DTranspose, Softmax, ELU
 
 from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.constants import SOFTMAX_THRESHOLD
+from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.constants import SOFTMAX_THRESHOLD
 from model_compression_toolkit.core.keras.constants import SOFTMAX, LINEAR, RELU, SWISH, SIGMOID, IDENTITY, TANH, SELU, \
     KERNEL, DEPTHWISE_KERNEL
 from model_compression_toolkit.core.keras.quantizer.fake_quant_builder import power_of_two_quantization, symmetric_quantization, uniform_quantization
 
 """
 Map each layer to a list of its' weights attributes that should get quantized.
 If a layer that is not listed here is queried, [None] is returned.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/activation_decomposition.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 from tensorflow.keras.layers import Dense, DepthwiseConv2D, Conv2D, Conv2DTranspose, Activation, SeparableConv2D
 
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common.constants import FLOAT_32, DATA_TYPE
+from model_compression_toolkit.constants import FLOAT_32, DATA_TYPE
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.keras.constants import LINEAR, ACTIVATION, TRAINABLE, LAYER_NAME
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_folding.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_reconstruction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/batchnorm_refusing.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/input_scaling.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, EdgeMatcher, WalkMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
-from model_compression_toolkit.core.common.constants import THRESHOLD
+from model_compression_toolkit.constants import THRESHOLD
 from model_compression_toolkit.core.keras.constants import KERNEL
 
 input_node = NodeOperationMatcher(InputLayer)
 zeropad_node = NodeOperationMatcher(ZeroPadding2D)
 op2d_node = NodeOperationMatcher(Dense) | \
             NodeOperationMatcher(Conv2D) | \
             NodeOperationMatcher(DepthwiseConv2D) | \
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/linear_collapsing.py`

 * *Files 0% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 import tensorflow as tf
 from tensorflow.keras.layers import Conv2D
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing
 from model_compression_toolkit.core.keras.constants import KERNEL, KERNEL_SIZE, STRIDES, DILATIONS, LINEAR, \
     ACTIVATION, BIAS, USE_BIAS, LAYER_NAME, FILTERS, PADDING, GROUPS, DATA_FORMAT
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def linear_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
     """
     Function generates matchers for matching:
     (Conv2D, Conv2D)[activation=linear] -> Conv2D.
     Returns:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/multi_head_attention_decomposition.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,25 +19,24 @@
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.python.keras.layers.core import TFOpLambda
     from tensorflow.keras.layers import MultiHeadAttention, Conv2D, Softmax, Concatenate, Reshape, Permute
 else:
     from keras.layers.core import TFOpLambda
     from keras.layers import MultiHeadAttention, Conv2D, Softmax, Concatenate, Reshape, Permute
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph, BaseNode, OutTensor
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
-from model_compression_toolkit.core.common.constants import REUSE, REUSE_GROUP
-from model_compression_toolkit.core.keras.reader.node_builder import REUSED_IDENTIFIER
+from model_compression_toolkit.constants import REUSE, REUSE_GROUP
 from model_compression_toolkit.core.keras.constants import KERNEL, BIAS, USE_BIAS, NUM_HEADS, KEY_DIM, VALUE_DIM, \
     QUERY_SHAPE, KEY_SHAPE, VALUE_SHAPE, OUTPUT_SHAPE, ATTENTION_AXES, ACTIVATION, LINEAR, FILTERS, \
     FUNCTION, DIMS, TARGET_SHAPE, F_STRIDED_SLICE, F_STACK, Q_KERNEL, Q_BIAS, K_KERNEL, K_BIAS, V_KERNEL, V_BIAS, \
-    OUTPUT_KERNEL, OUTPUT_BIAS, F_MATMUL, TRANSPOSE_B, KERNEL_SIZE, AXIS, F_STRIDED_SLICE_BEGIN, F_STRIDED_SLICE_END
+    OUTPUT_KERNEL, OUTPUT_BIAS, F_MATMUL, KERNEL_SIZE, AXIS, F_STRIDED_SLICE_BEGIN, F_STRIDED_SLICE_END
 
 
 class MHAParams:
     """
     A data class to hold all relevant parameters from the MHA node framework attributes
     """
     def __init__(self, mha_node):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/relu_bound_to_power_of_2.py`

 * *Files 5% similar despite different names*

```diff
@@ -19,14 +19,15 @@
 import numpy as np
 from tensorflow.keras.layers import ReLU, DepthwiseConv2D, Conv2DTranspose, Conv2D, Dense
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, WalkMatcher
 from model_compression_toolkit.core.keras.constants import KERNEL, BIAS, RELU_MAX_VALUE, RELU_POT_BOUND
+from model_compression_toolkit.logger import Logger
 
 
 class ReLUBoundToPowerOfTwo(common.BaseSubstitution):
     """
     Substitution to scale the weights of two linear nodes, and move the bound of non-linear between them
     (if bounded) in order to use the entire constrained range when activations are quantized.
     """
@@ -77,15 +78,15 @@
         # only act on bound relu with non POT max value
         if max_value is None or np.log2(max_value).astype(int) - np.log2(max_value) == 0:
             return graph
 
         scale_factor = max_value / self.threshold
 
         non_linear_node.framework_attr[RELU_MAX_VALUE] = np.float32(self.threshold)
-        common.Logger.debug(
+        Logger.debug(
             f"Node named:{non_linear_node.name} max value change "
             f"to:{non_linear_node.framework_attr[RELU_MAX_VALUE]}")
 
         w2_fixed = scale_factor * second_op2d_node.get_weights_by_keys(KERNEL)
         w1_fixed = first_op2d_node.get_weights_by_keys(KERNEL) / scale_factor
         b1_fixed = first_op2d_node.get_weights_by_keys(BIAS) / scale_factor
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/remove_relu_upper_bound.py`

 * *Files 12% similar despite different names*

```diff
@@ -16,15 +16,16 @@
 
 from tensorflow.keras.layers import ReLU
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher,NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.keras.constants import RELU_MAX_VALUE
-from model_compression_toolkit.core.common.constants import THRESHOLD
+from model_compression_toolkit.constants import THRESHOLD
+from model_compression_toolkit.logger import Logger
 
 MATCHER = NodeOperationMatcher(ReLU) & NodeFrameworkAttrMatcher(RELU_MAX_VALUE, None).logic_not()
 
 
 class RemoveReLUUpperBound(common.BaseSubstitution):
     """
     Remove ReLU upper bound if its activation threshold bounds it anyway at
@@ -52,9 +53,9 @@
         Returns:
             Graph after applying the substitution.
         """
         if node.final_activation_quantization_cfg and \
                 node.final_activation_quantization_cfg.activation_quantization_params.get(THRESHOLD) == \
                 node.framework_attr.get(RELU_MAX_VALUE):
             node.framework_attr[RELU_MAX_VALUE] = None
-            common.Logger.info(f'Removing upper bound of {node.name}. Threshold and upper bound are equal.')
+            Logger.info(f'Removing upper bound of {node.name}. Threshold and upper bound are equal.')
         return graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/residual_collapsing.py`

 * *Files 1% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 import tensorflow as tf
 from tensorflow.keras.layers import Conv2D, Add
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.substitutions.residual_collapsing import ResidualCollapsing
 from model_compression_toolkit.core.keras.constants import KERNEL, LINEAR, ACTIVATION, LAYER_NAME
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def residual_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
     """
     Function generates matchers for matching:
     (Conv2D, Add)[activation=linear] -> Conv2D.
     Returns:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/separableconv_decomposition.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/shift_negative_activation.py`

 * *Files 0% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 import numpy as np
 import tensorflow as tf
 
 from tensorflow.python.keras.layers.core import TFOpLambda
 from tensorflow.keras.layers import Activation, Conv2D, Dense, DepthwiseConv2D, ZeroPadding2D, Reshape, \
     GlobalAveragePooling2D, Dropout, ReLU, PReLU, ELU
 
-from model_compression_toolkit import CoreConfig, FrameworkInfo
+from model_compression_toolkit.core import CoreConfig, FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, \
     NodeFrameworkAttrMatcher
 from model_compression_toolkit.core.common.substitutions.shift_negative_activation import \
     apply_shift_negative_correction
 from model_compression_toolkit.core.keras.constants import KERNEL_SIZE, STRIDES, ACTIVATION, SWISH, \
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/virtual_activation_weights_composition.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/graph_substitutions/substitutions/weights_activation_split.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/keras_implementation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/keras_implementation.py`

 * *Files 1% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import List, Any, Tuple, Callable, Type, Dict
+from typing import List, Any, Tuple, Callable, Dict
 
 import numpy as np
 import tensorflow as tf
 from tensorflow.keras.models import Model
 from tensorflow.python.layers.base import Layer
 
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
@@ -39,25 +39,23 @@
     from tensorflow.keras.layers import Dense, Activation, Conv2D, DepthwiseConv2D, Conv2DTranspose, Concatenate, Add
     from tensorflow.python.keras.layers.core import TFOpLambda
 else:
     from keras.layers import Dense, Activation, Conv2D, DepthwiseConv2D, Conv2DTranspose, \
         Concatenate, Add
     from keras.layers.core import TFOpLambda
 
-from model_compression_toolkit import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.keras.gptq_training import KerasGPTQTrainer
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.activation_decomposition import \
     ActivationDecomposition
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.softmax_shift import \
     keras_softmax_shift
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_folding import \
     keras_batchnorm_folding
 from model_compression_toolkit.core.keras.graph_substitutions.substitutions.batchnorm_refusing import \
@@ -344,20 +342,14 @@
             A list of the framework substitutions used after we apply second moment statistics.
         """
         substitutions_list = []
         if quant_config.weights_second_moment_correction:
             substitutions_list.append(keras_batchnorm_refusing())
         return substitutions_list
 
-    def get_gptq_trainer_obj(self) -> Type[GPTQTrainer]:
-        """
-        Returns:  Keras object of GPTQTrainer
-        """
-        return KerasGPTQTrainer
-
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
                                   quant_config: MixedPrecisionQuantizationConfigV2,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
                                   disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
         """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/keras_model_validation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/keras_model_validation.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from tensorflow.keras.models import Model
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common.framework_info import ChannelAxis
 from model_compression_toolkit.core.common.model_validation import ModelValidation
 from model_compression_toolkit.core.keras.constants import CHANNELS_FORMAT, CHANNELS_FORMAT_LAST, CHANNELS_FORMAT_FIRST
 
 
 class KerasModelValidation(ModelValidation):
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/keras_node_prior_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/keras_node_prior_info.py`

 * *Files 0% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 from packaging import version
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Activation, ReLU, BatchNormalization
 else:
     from keras.layers import Activation, ReLU, BatchNormalization
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.keras.constants import ACTIVATION, RELU_MAX_VALUE, NEGATIVE_SLOPE, THRESHOLD, \
     GAMMA, BETA, MOVING_MEAN, MOVING_VARIANCE
 from model_compression_toolkit.core.common.graph.base_graph import Graph
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/kpi_data_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/kpi_data_facade.py`

 * *Files 8% similar despite different names*

```diff
@@ -11,27 +11,27 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
-from model_compression_toolkit import MixedPrecisionQuantizationConfig, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import MixedPrecisionQuantizationConfig, CoreConfig, MixedPrecisionQuantizationConfigV2
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import TENSORFLOW
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import TENSORFLOW
+from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_data import compute_kpi_data
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     DEFAULT_MIXEDPRECISION_CONFIG
-from model_compression_toolkit.core.common.constants import FOUND_TF
+from model_compression_toolkit.constants import FOUND_TF
 
 if FOUND_TF:
-    from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
     from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
     from tensorflow.keras.models import Model
 
     from model_compression_toolkit import get_target_platform_capabilities
 
     KERAS_DEFAULT_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
@@ -47,15 +47,15 @@
         Builds the computation graph from the given model and target platform modeling, and uses it to compute the KPI data.
 
         Args:
             in_model (Model): Keras model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
             quant_config (MixedPrecisionQuantizationConfig): MixedPrecisionQuantizationConfig containing parameters of how the model should be quantized.
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to. `Default Keras TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/keras_tp_models/keras_default.py>`_
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
 
         Returns:
             A KPI object with total weights parameters sum, max activation tensor and total kpi.
 
         Examples:
 
             Import a Keras model:
@@ -71,15 +71,15 @@
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 224, 224, 3))]
 
             Import MCT and call for KPI data calculation:
 
             >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.keras_kpi_data(model, repr_datagen)
+            >>> kpi_data = mct.core.keras_kpi_data(model, repr_datagen)
 
 
         """
 
         if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
             Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfig object."
                          "Given quant_config is not of type MixedPrecisionQuantizationConfig.")
@@ -108,15 +108,15 @@
         Builds the computation graph from the given model and hw modeling, and uses it to compute the KPI data.
 
         Args:
             in_model (Model): Keras model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
             core_config (CoreConfig): CoreConfig containing parameters for quantization and mixed precision of how the model should be quantized.
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to. `Default Keras TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/keras_tp_models/keras_default.py>`_
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
 
         Returns:
 
             A KPI object with total weights parameters sum and max activation tensor.
 
         Examples:
 
@@ -129,15 +129,15 @@
 
             >>> import numpy as np
             >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
 
             Import MCT and call for KPI data calculation:
 
             >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.keras_kpi_data(model, repr_datagen)
+            >>> kpi_data = mct.core.keras_kpi_data(model, repr_datagen)
 
         """
 
         if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
             Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfigV2 object."
                          "Given quant_config is not of type MixedPrecisionQuantizationConfigV2.")
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/mixed_precision/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/mixed_precision/set_layer_to_bitwidth.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/mixed_precision/set_layer_to_bitwidth.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/keras_quantization_facade.py`

 * *Files 9% similar despite different names*

```diff
@@ -11,19 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable, List, Tuple
 
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import TENSORFLOW
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import TENSORFLOW
 from model_compression_toolkit.core.common.user_info import UserInformation
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, GradientPTQConfigV2
+from model_compression_toolkit.gptq import GradientPTQConfig, GradientPTQConfigV2
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.network_editors.actions import EditRule
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfig, DEFAULT_MIXEDPRECISION_CONFIG
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
@@ -31,23 +30,23 @@
 from model_compression_toolkit.core.common.quantization.quantization_config import DEFAULTCONFIG
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.ptq.runner import ptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
-from model_compression_toolkit.core.common.constants import FOUND_TF
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.constants import FOUND_TF
 
 if FOUND_TF:
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
     from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
     from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
     from tensorflow.keras.models import Model
-    from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
 
     from model_compression_toolkit import get_target_platform_capabilities
 
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
     def keras_post_training_quantization(in_model: Model,
@@ -77,15 +76,15 @@
             representative_data_gen (Callable): Dataset used for calibration.
             n_iter (int): Number of calibration iterations to run.
             quant_config (QuantizationConfig): QuantizationConfig containing parameters of how the model should be quantized. `Default configuration. <https://github.com/sony/model_optimization/blob/21e21c95ca25a31874a5be7af9dd2dd5da8f3a10/model_compression_toolkit/core/common/quantization/quantization_config.py#L154>`_
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
             network_editor (List[EditRule]): List of EditRules. Each EditRule consists of a node filter and an action to change quantization settings of the filtered nodes.
             gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
             analyze_similarity (bool): Whether to plot similarity figures within TensorBoard (when logger is enabled) or not.
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to. `Default Keras TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/keras_tp_models/keras_default.py>`_
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
 
         Returns:
             A quantized model and information the user may need to handle the quantized model.
 
         Examples:
 
             Import a Keras model:
@@ -100,14 +99,17 @@
 
             Import mct and pass the model with the representative dataset generator to get a quantized model:
 
             >>> import model_compression_toolkit as mct
             >>> quantized_model, quantization_info = mct.keras_post_training_quantization(model, repr_datagen, n_iter=1)
 
         """
+        Logger.warning('keras_post_training_quantization is deprecated and will be removed '
+                       'in the future. Please use mct.ptq.keras_post_training_quantization_experimental instead.')
+
         KerasModelValidation(model=in_model,
                              fw_info=fw_info).validate()
 
         core_config = CoreConfig(quantization_config=quant_config,
                                  debug_config=DebugConfig(analyze_similarity=analyze_similarity,
                                                           network_editor=network_editor)
                                  )
@@ -180,15 +182,15 @@
              target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
              n_iter (int): Number of calibration iterations to run.
              quant_config (MixedPrecisionQuantizationConfig): QuantizationConfig containing parameters of how the model should be quantized.
              fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
              network_editor (List[EditRule]): List of EditRules. Each EditRule consists of a node filter and an action to change quantization settings of the filtered nodes.
              gptq_config (GradientPTQConfig): Configuration for using GPTQ (e.g. optimizer).
              analyze_similarity (bool): Whether to plot similarity figures within TensorBoard (when logger is enabled) or not.
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to. `Default Keras TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/keras_tp_models/keras_default.py>`_
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
 
 
          Returns:
              A quantized model and information the user may need to handle the quantized model.
 
          Examples:
 
@@ -205,39 +207,42 @@
 
              >>> import numpy as np
              >>> def repr_datagen(): return [np.random.random((1,224,224,3))]
 
              Create a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
              The candidates bitwidth for quantization should be defined in the target platform model:
 
-             >>> config = mct.MixedPrecisionQuantizationConfig()
+             >>> config = mct.core.MixedPrecisionQuantizationConfig()
 
              Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
              that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
              while the bias will not):
 
-             >>> kpi = mct.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+             >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
              Pass the model, the representative dataset generator, the configuration and the target KPI to get a
              quantized model:
 
              >>> quantized_model, quantization_info = mct.keras_post_training_quantization_mixed_precision(model,repr_datagen, target_kpi=kpi, n_iter=10, quant_config=config)
 
              For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/experimental_api_docs/modules/mixed_precision_quantization_config.html#model_compression_toolkit.MixedPrecisionQuantizationConfigV2>`_.
 
          """
+        Logger.warning('keras_post_training_quantization_mixed_precision is deprecated and will be removed '
+                       'in the future. Please use mct.ptq.keras_post_training_quantization_experimental instead.')
+
         KerasModelValidation(model=in_model,
                              fw_info=fw_info).validate()
 
         if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+            Logger.error("Given quantization config to mixed-precision facade is not of type "
                                 "MixedPrecisionQuantizationConfig. Please use keras_post_training_quantization API,"
                                 "or pass a valid mixed precision configuration.")
 
-        common.Logger.info("Using experimental mixed-precision quantization. "
+        Logger.info("Using experimental mixed-precision quantization. "
                            "If you encounter an issue please file a bug.")
 
         quantization_config, mp_config = quant_config.separate_configs()
         core_config = CoreConfig(quantization_config=quantization_config,
                                  mixed_precision_config=mp_config,
                                  debug_config=DebugConfig(analyze_similarity=analyze_similarity,
                                                           network_editor=network_editor)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/base_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/base_quantizer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/fake_quant_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,16 +16,16 @@
 
 from typing import Tuple, Callable
 
 import tensorflow as tf
 import numpy as np
 from tensorflow.python.util.object_identity import Reference as TFReference
 
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import threshold_is_power_of_two
 
 
 def quantizer_min_max_calculator(threshold: np.ndarray,
                                  num_bits: int,
                                  signed: bool) -> Tuple[float, float]:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/input_layer_quantize_transform.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/input_layer_quantize_transform.py`

 * *Files 1% similar despite different names*

```diff
@@ -19,15 +19,15 @@
 from tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_transforms import \
     InputLayerQuantize
 from tensorflow_model_optimization.python.core.quantization.keras.graph_transformations import transforms
 from tensorflow_model_optimization.python.core.quantization.keras.quantize_config import QuantizeConfig
 
 
 from model_compression_toolkit.core.common import BaseNode
-from model_compression_toolkit.core.common.constants import INPUT_BASE_NAME
+from model_compression_toolkit.constants import INPUT_BASE_NAME
 
 
 class InputLayerWrapperTransform(InputLayerQuantize):
     """
     Allows to configure an input layer with QuantizeWrapper given a QuantizeConfig object to wrap it.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/lut_fake_quant.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
-from typing import Tuple, Dict, Any, Callable
+from typing import Tuple, Dict, Callable
 
 import numpy as np
 import tensorflow as tf
 from keras.layers import Layer
 from tensorflow.python.util.object_identity import Reference as TFReference
 
-from model_compression_toolkit.core.common.constants import SIGNED, CLUSTER_CENTERS, EPS, \
+from model_compression_toolkit.constants import SIGNED, CLUSTER_CENTERS, EPS, \
     MULTIPLIER_N_BITS, THRESHOLD
 
 
 def activation_lut_kmean_quantizer(activation_n_bits: int,
                                    quantization_params: Dict[str, np.ndarray]) -> Callable:
     """
     Builds a LUT quantizer for layer's activation using the provided params (threshold and clusters).
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/quantization_config_factory.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/quantization_config_factory.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_activation_quantizer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_quantize_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_quantize_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -20,15 +20,15 @@
 import tensorflow as tf
 # As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
 from model_compression_toolkit.core.common.quantization.candidate_node_quantization_config import \
     CandidateNodeQuantizationConfig
 from model_compression_toolkit.core.keras.quantizer.mixed_precision.selective_activation_quantizer import \
     SelectiveActivationQuantizer
 from packaging import version
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.python.keras.layers import Layer  # pragma: no cover
 else:
     from keras.engine.base_layer import Layer
 from tensorflow.python.training.tracking.data_structures import ListWrapper
 from tensorflow_model_optimization.python.core.quantization.keras.quantize_config import QuantizeConfig
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/quantizer/mixed_precision/selective_weights_quantizer.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/common.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/common.py`

 * *Files 2% similar despite different names*

```diff
@@ -25,15 +25,15 @@
     from tensorflow.python.keras.engine.sequential import Sequential
 else:
     from keras.engine.input_layer import InputLayer
     from keras.engine.node import Node as KerasNode
     from keras.engine.functional import Functional
     from keras.engine.sequential import Sequential
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 
 
 def is_node_an_input_layer(node: BaseNode) -> bool:
     """
     Checks if a node represents a Keras input layer.
     Args:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/connectivity_handler.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/connectivity_handler.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/edges_merger.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/nested_model_handler.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/nodes_merger.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/nested_model/outputs_merger.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/node_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/node_builder.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/reader/reader.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/reader/reader.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/statistics_correction/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/statistics_correction/apply_second_moment_correction.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,15 +15,15 @@
 import copy
 from typing import Any, Callable
 
 from tensorflow.keras.layers import BatchNormalization
 from tqdm import tqdm
 
 import model_compression_toolkit.core.keras.constants as keras_constants
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core import common
 
 
 def keras_apply_second_moment_correction(quantized_model: Any,
                                          core_config: CoreConfig,
                                          representative_data_gen: Callable,
                                          graph: common.Graph):
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/tf_tensor_numpy.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/tf_tensor_numpy.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/keras/visualization/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/keras/visualization/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
 from model_compression_toolkit.core.pytorch.back2framework.float_model_builder import FloatPyTorchModelBuilder
 from model_compression_toolkit.core.pytorch.back2framework.mixed_precision_model_builder import \
     MixedPrecisionPyTorchModelBuilder
 from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
 from model_compression_toolkit.core.pytorch.back2framework.quantized_model_builder import QuantizedPyTorchModelBuilder
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import List, Tuple
 
 import torch
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder, \
     PytorchModel
 
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/instance_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -34,18 +34,22 @@
     framework_attr = copy.copy(n.framework_attr)
     node_instance = n.type(**framework_attr)
     node_instance.load_state_dict({k: torch.Tensor(v) for k, v in n.weights.items()}, strict=False)
     set_model(node_instance)
     return node_instance
 
 
-def identity_wrapper(node: BaseNode, module: Module):
+# todo: remove. It is not used anymore
+def identity_wrapper(node: BaseNode,
+                     module: Module,
+                     include_activation_quantizers: bool):
     """
     A function which takes a computational graph node and a pytorch module and return an identity wrapping which return the layer itself
     Args:
         node: A node of mct graph.
         layer: A pytorch module
+        include_activation_quantizers: bool flag.
     Returns: pytorch module
     """
     return module
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import List, Any, Tuple
 
 import torch
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder
 from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder, \
     PytorchModel
@@ -30,34 +30,32 @@
 from model_compression_toolkit.core.pytorch.mixed_precision.mixed_precision_wrapper import PytorchMixedPrecisionWrapper
 
 
 class MixedPrecisionPyTorchModel(PytorchModel):
 
     def __init__(self,
                  graph: common.Graph,
-                 append2output=None,
-                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO):
+                 append2output=None):
         """
 
         Args:
             graph: Graph to build its corresponding Pytorch model.
             append2output: List of nodes or OutTensor objects.
-            fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
         """
 
         super().__init__(graph,
-                         append2output,
-                         fw_info)
+                         append2output)
 
 
     def _add_modules(self):
         configurable_nodes = self.graph.get_configurable_sorted_nodes()
         for n in self.node_sort:
             if n in configurable_nodes:
-                self.add_module(n.name, PytorchMixedPrecisionWrapper(n, self.fw_info))
+                self.add_module(n.name, PytorchMixedPrecisionWrapper(n,
+                                                                     DEFAULT_PYTORCH_INFO))
             else:
                 if not isinstance(n, FunctionalNode):
                     self.add_module(n.name, node_builder(n))
 
     def _quantize_node_activations(self,
                                    node: BaseNode,
                                    input_tensors: List[torch.Tensor]) -> List[torch.Tensor]:
@@ -125,9 +123,8 @@
     def build_model(self) -> Tuple[PytorchModel, UserInformation]:
         """
         Build a PyTorch float model and return it.
         Returns: Float PyTorch model and user information.
 
         """
         return MixedPrecisionPyTorchModel(self.graph,
-                                          self.append2output,
-                                          self.fw_info), self.graph.user_info
+                                          self.append2output), self.graph.user_info
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/model_gradients.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/model_gradients.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,21 +18,22 @@
 import torch.autograd as autograd
 from networkx import topological_sort
 from tqdm import tqdm
 import numpy as np
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
-from model_compression_toolkit.core.common.constants import EPS, MIN_JACOBIANS_ITER, JACOBIANS_COMP_TOLERANCE
+from model_compression_toolkit.constants import EPS, MIN_JACOBIANS_ITER, JACOBIANS_COMP_TOLERANCE
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder
-from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder
-from model_compression_toolkit.core.pytorch.utils import torch_tensor_to_numpy
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.core.pytorch.constants import BUFFER
+from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, BufferHolder
+from model_compression_toolkit.core.pytorch.utils import torch_tensor_to_numpy, get_working_device
+from model_compression_toolkit.logger import Logger
 
 
 def build_input_tensors_list(node: BaseNode,
                              graph: Graph,
                              inputs: Dict[BaseNode, List],
                              node_to_output_tensors_dict: Dict[BaseNode, List]) -> List[List]:
     """
@@ -129,15 +130,21 @@
         self.node_sort = list(topological_sort(graph_float))
         self.interest_points = interest_points
         self.output_list = output_list
         self.interest_points_tensors = []
 
         for n in self.node_sort:
             if not isinstance(n, FunctionalNode):
-                self.add_module(n.name, node_builder(n))
+                if n.type == BufferHolder:
+                    self.add_module(n.name, node_builder(n))
+                    self.get_submodule(n.name). \
+                        register_buffer(n.name,
+                                        torch.Tensor(n.get_weights_by_keys(BUFFER)).to(get_working_device()))
+                else:
+                    self.add_module(n.name, node_builder(n))
 
     def forward(self,
                 *args: Any) -> Any:
         """
         Args:
             args: argument input tensors to model, which is a mappings between an input node and its input tensor.
 
@@ -285,17 +292,17 @@
                                              device=device))
                 break
             jac_v = torch.reshape(jac_v, [jac_v.shape[0], -1])
             jac_trace_approx = torch.mean(torch.sum(torch.pow(jac_v, 2.0)))
 
             # If the change to the mean Jacobian approximation is insignificant we stop the calculation
             if j > MIN_JACOBIANS_ITER:
-                delta = torch.mean(torch.stack([jac_trace_approx, *trace_jv])) - torch.mean(
-                    torch.stack(trace_jv))
-                if torch.abs(delta) / (torch.abs(torch.mean(torch.stack(trace_jv))) + 1e-6) < JACOBIANS_COMP_TOLERANCE:
+                new_mean = torch.mean(torch.stack([jac_trace_approx, *trace_jv]))
+                delta = new_mean - torch.mean(torch.stack(trace_jv))
+                if torch.abs(delta) / (torch.abs(new_mean) + 1e-6) < JACOBIANS_COMP_TOLERANCE:
                     trace_jv.append(jac_trace_approx)
                     break
 
             trace_jv.append(jac_trace_approx)
         ipts_jac_trace_approx.append(2*torch.mean(torch.stack(trace_jv))/output.shape[-1])  # Get averaged jacobian trace approximation
 
     ipts_jac_trace_approx = torch_tensor_to_numpy(torch.Tensor(ipts_jac_trace_approx))  # Just to get one tensor instead of list of tensors with single element
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/pytorch_model_builder.py`

 * *Files 23% similar despite different names*

```diff
@@ -9,31 +9,33 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from abc import abstractmethod
+from functools import partial
 from typing import Tuple, Any, Dict, List, Union, Callable
 
 import torch
 from networkx import topological_sort
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.back2framework.base_model_builder import BaseModelBuilder
 from model_compression_toolkit.core.common.graph.edge import EDGE_SINK_INDEX
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder, identity_wrapper
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
 from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, BufferHolder
 from model_compression_toolkit.core.pytorch.utils import get_working_device
 from model_compression_toolkit.core.pytorch.constants import BUFFER
+from mct_quantizers.common.constants import ACTIVATION_HOLDER_QUANTIZER
 
 
 def _build_input_tensors_list(node: BaseNode,
                               graph: Graph,
                               inputs: Tuple[Any],
                               node_to_output_tensors_dict: Dict[BaseNode, List]) -> List[List]:
     """
@@ -62,44 +64,44 @@
     return input_tensors
 
 
 def _run_operation(n: BaseNode,
                    input_tensors: List,
                    op_func: Any,
                    quantize_node_activation_fn,
-                   is_wrapped: bool) -> Tuple[Union[List,torch.Tensor], Union[List,torch.Tensor]]:
+                   use_activation_quantization: bool) -> Tuple[Union[List, torch.Tensor], Union[List, torch.Tensor]]:
     """
     Applying the layer (op_func) to the input tensors (input_tensors).
     If quantized is set to True, and the layer's corresponding node (n) has quantization
     attributes, an additional fake-quantization node is built and appended to the layer.
 
     Args:
         n: The corresponding node of the layer it runs.
         input_tensors: List of Pytorch tensors that are the layer's inputs.
         op_func: Module/functional to apply to the input tensors.
         quantize_node_activation_fn: quantization function
-        is_wrapped : Flag to indicate if layer is already quantization wrapped so no activation is needed
+        use_activation_quantization: Flag to indicate if we have an activation function.
     Returns:
         A tuple of Pytorch tensors. The Module/functional output tensors after applying the
         Module/functional to the input tensors.
     """
 
     op_call_args = n.op_call_args if isinstance(n, FunctionalNode) else []
     functional_kwargs = n.op_call_kwargs if isinstance(n, FunctionalNode) else {}
     if isinstance(n, FunctionalNode) and n.inputs_as_list:
         out_tensors_of_n_float = op_func(input_tensors, *op_call_args, **functional_kwargs)
     else:
         out_tensors_of_n_float = op_func(*input_tensors + op_call_args, **functional_kwargs)
 
     # Add a fake quant node if the node has an activation threshold.
     out_tensors_of_n = out_tensors_of_n_float
-    if n.is_activation_quantization_enabled() and not is_wrapped:
+    if use_activation_quantization:
         if isinstance(out_tensors_of_n_float, list):
             out_tensors_of_n_float = torch.cat(out_tensors_of_n_float, dim=0)
-        out_tensors_of_n = quantize_node_activation_fn(n, out_tensors_of_n_float)
+        out_tensors_of_n = quantize_node_activation_fn(out_tensors_of_n_float)
 
     return out_tensors_of_n, out_tensors_of_n_float
 
 
 def _find_by_node_name(node_to_output_tensors_dict: dict, node_name: str):
     """
     Args:
@@ -139,37 +141,49 @@
     """
     Class for reconstructing a Pytorch model from a graph
     """
 
     def __init__(self,
                  graph: Graph,
                  append2output: List[Any] = None,
-                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
                  return_float_outputs: bool = False,
-                 wrapper: Callable = identity_wrapper):
+                 wrapper: Callable = None,
+                 get_activation_quantizer_holder_fn: Callable = None):
         """
         Construct a Pytorch model.
 
         Args:
             graph: Graph to build its corresponding Pytorch model.
             append2output: List of nodes or OutTensor objects.
-            fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
             return_float_outputs: Whether the model returns float tensors or not.
             wrapper: A function wrapper Pytorch Layers.
+            get_activation_quantizer_holder_fn: Function to retrieve a quantization holder for a node.
+
         """
         super(PytorchModel, self).__init__()
         self.graph = graph
         self.node_sort = list(topological_sort(graph))
-        self.nodes_dict = {}
+        self.node_to_activation_quantization_holder = {}
         self.append2output = append2output
         self.return_float_outputs = return_float_outputs
-        self.fw_info = fw_info
         self.wrapper = wrapper
+        self.get_activation_quantizer_holder = get_activation_quantizer_holder_fn
         self._add_modules()
 
+    # todo: Move to parent class BaseModelBuilder
+    @property
+    def use_activation_holder_during_model_building(self) -> bool:
+        """
+        Returns: Whether or not the model builder uses a PytorchActivationQuantizationHolder during
+        model building (by adding it as a module when converting the graph to a Pytorch model).
+        If so - the model builder expects the activation quantizers not to be wrapped
+        in a PytorchQuantizeWrapper.
+        """
+        return self.get_activation_quantizer_holder is not None
+
     @abstractmethod
     def _quantize_node_activations(self,
                                    node: BaseNode,
                                    input_tensors: List[torch.Tensor]) -> List[torch.Tensor]:
         """
         Quantize node's activation given input tensors.
 
@@ -180,60 +194,92 @@
         Returns:
             Output of the node.
 
         """
         raise NotImplemented(f'{self.__class__.__name__} '
                              f'have to implement a method for quantization activation nodes.')  # pragma: no cover
 
+    def wrap(self, node):
+        """
+        Wraps a node operation with a wrapper, if one is available.
+
+        Args:
+            node: node to wrap its operation.
+
+        Returns: the node's operation. If a wrapper is available, the operation is wrapped.
+        """
+        if isinstance(node, FunctionalNode):
+            if self.wrapper is None:
+                node_op = node.type
+            else:
+                node_op = self.wrapper(node, node.type)
+        else:
+            if self.wrapper is None or node.type == BufferHolder:
+                node_op = node_builder(node)
+            else:
+                node_op = self.wrapper(node, node_builder(node))
+        return node_op
+
     def _add_modules(self):
-        for n in self.node_sort:
-            if isinstance(n, FunctionalNode):
+        """
+        Build and add the modules and functional nodes from node_sort list as attributes to PytorchModel
+        """
+        for node in self.node_sort:
+            node_op = self.wrap(node)
+            if isinstance(node, FunctionalNode):
                 # for functional layers
-                setattr(self, n.name, self.wrapper(n, n.type))
+                setattr(self, node.name, node_op)
             else:
-                if n.type == BufferHolder:
-                    self.add_module(n.name, node_builder(n))
-                    self.get_submodule(n.name). \
-                        register_buffer(n.name, torch.Tensor(n.get_weights_by_keys(BUFFER)).to(get_working_device()))
-                else:
-                    self.add_module(n.name, self.wrapper(n, node_builder(n)))
+                self.add_module(node.name, node_op)
+                if node.type == BufferHolder:
+                    self.get_submodule(node.name). \
+                        register_buffer(node.name,
+                                        torch.Tensor(node.get_weights_by_keys(BUFFER)).to(get_working_device()))
+
+            # Add activation quantization modules if an activation holder is configured for this node
+            if node.is_activation_quantization_enabled() and self.get_activation_quantizer_holder is not None:
+                activation_quantizer_holder = self.get_activation_quantizer_holder(node)
+                if activation_quantizer_holder is not None:
+                    self.add_module(node.name + '_' + ACTIVATION_HOLDER_QUANTIZER, activation_quantizer_holder)
+                    self.node_to_activation_quantization_holder.update(
+                        {node.name: node.name + '_' + ACTIVATION_HOLDER_QUANTIZER})
 
     def forward(self,
                 *args: Any) -> Any:
         """
         Args:
             args: argument input tensors to model.
         Returns:
             torch Tensor/s which is/are the output of the model logic.
         """
         node_to_output_tensors_dict = dict()
         node_to_output_tensors_dict_float = dict()
         configurable_nodes = self.graph.get_configurable_sorted_nodes_names()
-        for n in self.node_sort:
-            input_tensors = _build_input_tensors_list(n,
+        for node in self.node_sort:
+            input_tensors = _build_input_tensors_list(node,
                                                       self.graph,
                                                       args,
                                                       node_to_output_tensors_dict)
 
-            op_func = self._get_op_func(n, configurable_nodes)
+            op_func = self._get_op_func(node, configurable_nodes)
+            use_activation_quantization, activation_quantization_fn = self._get_activation_quantization_fn(node)
 
             # Run node operation and fetch outputs
-            out_tensors_of_n, out_tensors_of_n_float = _run_operation(n,
+            out_tensors_of_n, out_tensors_of_n_float = _run_operation(node,
                                                                       input_tensors,
                                                                       op_func=op_func,
-                                                                      quantize_node_activation_fn=self._quantize_node_activations,
-                                                                      is_wrapped=self.wrapper is not identity_wrapper)
+                                                                      quantize_node_activation_fn=activation_quantization_fn,
+                                                                      use_activation_quantization=use_activation_quantization)
 
             if isinstance(out_tensors_of_n, list):
-                node_to_output_tensors_dict.update({n: out_tensors_of_n})
-                node_to_output_tensors_dict_float.update({n: out_tensors_of_n_float})
+                node_to_output_tensors_dict.update({node: out_tensors_of_n})
+                node_to_output_tensors_dict_float.update({node: out_tensors_of_n_float})
             else:
-                node_to_output_tensors_dict.update({n: [out_tensors_of_n]})
-                node_to_output_tensors_dict_float.update({n: [out_tensors_of_n_float]})
-
+                node_to_output_tensors_dict.update({node: [out_tensors_of_n]})
+                node_to_output_tensors_dict_float.update({node: [out_tensors_of_n_float]})
 
         if self.append2output:
             outputs = _generate_outputs(self.append2output,
                                         node_to_output_tensors_dict_float if self.return_float_outputs else node_to_output_tensors_dict)
         else:
             outputs = _generate_outputs([ot.node for ot in self.graph.get_outputs()],
                                         node_to_output_tensors_dict_float if self.return_float_outputs else node_to_output_tensors_dict)
@@ -252,46 +298,72 @@
             configurable_nodes_names: A list of names of configurable nodes in the quantized model.
 
         Returns: Module/functional to apply to the input tensors.
 
         """
         return getattr(self, node.name)
 
+    def _get_activation_quantization_fn(self, node) -> Tuple[bool, bool, Callable]:
+        """
+        Get activation quantization parameters for this node.
+
+        Args:
+            node: Node from which to extract the activation quantization parameters.
+
+        Returns: Flag to indicate if we quantize activations, flag to indicate if we quantize activations
+        using a quantization holder and a quantization function to use for the node's activations.
+        """
+        activation_quantization_holder = self.node_to_activation_quantization_holder.get(node.name)
+        use_activation_quantization = node.is_activation_quantization_enabled()
+        if use_activation_quantization:
+            if activation_quantization_holder is None:
+                activation_quantization_fn = partial(self._quantize_node_activations, node)
+                use_activation_quantization = self.wrapper is None
+            else:
+                activation_quantization_fn = getattr(self, activation_quantization_holder)
+        else:
+            activation_quantization_fn = None
+        return use_activation_quantization, activation_quantization_fn
+
 
 class PyTorchModelBuilder(BaseModelBuilder):
     """
     Builder of PyTorch models.
     """
 
     def __init__(self,
                  graph: common.Graph,
                  append2output=None,
                  fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
                  return_float_outputs: bool = False,
-                 wrapper: Callable = identity_wrapper):
+                 wrapper: Callable = None,
+                 get_activation_quantizer_holder_fn: Callable = None):
         """
 
         Args:
             graph: Graph to build the model from.
             append2output: Nodes to append to model's output.
             fw_info: Information about the specific framework of the model that is built.
             return_float_outputs: Whether the model returns float tensors or not.
             wrapper: A function wrapper Pytorch Layers.
+            get_activation_quantizer_holder_fn: Function to retrieve a quantization holder for a node.
         """
 
         super().__init__(graph,
                          append2output,
                          fw_info,
                          return_float_outputs)
 
         self.wrapper = wrapper
+        self.get_activation_quantizer_holder_fn = get_activation_quantizer_holder_fn
 
     def build_model(self) -> Tuple[PytorchModel, UserInformation]:
         """
         Build a PyTorch model and return it.
         Returns: Pytorch model and user information.
 
         """
         return PytorchModel(self.graph,
                             self.append2output,
                             return_float_outputs=self.return_float_outputs,
-                            wrapper=self.wrapper), self.graph.user_info
+                            wrapper=self.wrapper,
+                            get_activation_quantizer_holder_fn=self.get_activation_quantizer_holder_fn), self.graph.user_info
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/quantized_layer_wrapper.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantization_wrapper/wrapper_quantize_config.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/back2framework/quantized_model_builder.py`

 * *Files 8% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import List, Tuple
 
 import torch
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder, \
     PytorchModel
 from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
 
@@ -29,27 +29,24 @@
 class QuantizedPyTorchModel(PytorchModel):
     """
     Quantized PyTorch model.
     """
 
     def __init__(self,
                  graph: common.Graph,
-                 append2output=None,
-                 fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO):
+                 append2output=None):
         """
 
         Args:
             graph: Graph to build its corresponding Pytorch model.
             append2output: List of nodes or OutTensor objects.
-            fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
         """
 
         super().__init__(graph,
-                         append2output,
-                         fw_info)
+                         append2output)
 
     def _quantize_node_activations(self,
                                    node: BaseNode,
                                    input_tensors: List[torch.Tensor]) -> List[torch.Tensor]:
         """
         Quantize node's activation given input tensors.
 
@@ -92,9 +89,8 @@
     def build_model(self) -> Tuple[PytorchModel, UserInformation]:
         """
         Build a PyTorch quantized model and return it.
         Returns: Quantized PyTorch model and user information.
 
         """
         return QuantizedPyTorchModel(self.graph,
-                                     self.append2output,
-                                     self.fw_info), self.graph.user_info
+                                     self.append2output), self.graph.user_info
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/constants.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/constants.py`

 * *Files 11% similar despite different names*

```diff
@@ -65,20 +65,14 @@
 # torch devices
 CUDA = 'cuda'
 CPU = 'cpu'
 
 # ReLU bound constants
 RELU_POT_BOUND = 8.0
 
-# Supported TP models names for Pytorch:
-DEFAULT_TP_MODEL = 'default'
-IMX500_TP_MODEL = 'imx500'
-TFLITE_TP_MODEL = 'tflite'
-QNNPACK_TP_MODEL = 'qnnpack'
-
 # MultiHeadAttention layer attributes:
 EMBED_DIM = 'embed_dim'
 NUM_HEADS = 'num_heads'
 DROPOUT = 'dropout'
 ADD_ZERO_ATTN = 'add_zero_attn'
 KEY_DIM = "kdim"
 VALUE_DIM = 'vdim'
@@ -88,7 +82,11 @@
 V_PROJ_WEIGHT = 'v_proj_weight'
 K_PROJ_WEIGHT = 'k_proj_weight'
 Q_PROJ_WEIGHT = 'q_proj_weight'
 IN_PROJ_WEIGHT = 'in_proj_weight'
 IN_PROJ_BIAS = 'in_proj_bias'
 BIAS_K = 'bias_k'
 BIAS_V = 'bias_v'
+
+# # Batch size value for 'reshape' and 'view' operators,
+# # the value is -1 so the batch size is inferred from the length of the array and remaining dimensions.
+BATCH_DIM_VALUE = -1
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/default_framework_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/default_framework_info.py`

 * *Files 2% similar despite different names*

```diff
@@ -15,16 +15,16 @@
 from torch.nn import Hardsigmoid, ReLU, ReLU6, Softmax, Sigmoid
 from torch.nn.functional import hardsigmoid, relu, relu6, softmax
 from torch.nn import Conv2d, ConvTranspose2d, Linear
 from torch import sigmoid
 
 from model_compression_toolkit.core.common.defaultdict import DefaultDict
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo, ChannelAxis
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.core.common.constants import SOFTMAX_THRESHOLD
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.constants import SOFTMAX_THRESHOLD
 from model_compression_toolkit.core.pytorch.constants import KERNEL
 from model_compression_toolkit.core.pytorch.quantizer.fake_quant_builder import power_of_two_quantization, \
     symmetric_quantization, uniform_quantization
 from model_compression_toolkit.core.pytorch.quantizer.lut_fake_quant import activation_lut_kmean_quantizer
 
 """
 Map each layer to a list of its' weights attributes that should get quantized.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_folding.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_reconstruction.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/batchnorm_refusing.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/const_holder_conv.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/linear_collapsing.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 from torch.nn import Conv2d
 import torch.nn.functional as F
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.substitutions.linear_collapsing import Conv2DCollapsing
 from model_compression_toolkit.core.pytorch.constants import KERNEL, KERNEL_SIZE, STRIDES, DILATIONS, BIAS, USE_BIAS, FILTERS, PADDING, GROUPS
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def linear_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
     """
     Function generates matchers for matching:
     (Conv2D, Conv2D)[activation=linear] -> Conv2D.
     Returns:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/multi_head_attention_decomposition.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 
 import numpy as np
 import torch
 import torch.nn as nn
 import operator
 from typing import List
 
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph, BaseNode, OutTensor
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.pytorch.constants import KERNEL, BIAS, NUM_HEADS, KEY_DIM, VALUE_DIM, \
     EMBED_DIM, BIAS_K, BIAS_V, ADD_ZERO_ATTN, BATCH_FIRST, OUT_PROJ_WEIGHT, OUT_PROJ_BIAS, \
     V_PROJ_WEIGHT, K_PROJ_WEIGHT, Q_PROJ_WEIGHT, IN_PROJ_WEIGHT, IN_PROJ_BIAS, DIM, KERNEL_SIZE, \
@@ -54,29 +54,23 @@
         # Add Zero Attn feature is Not Implemented
         if ADD_ZERO_ATTN in mha_node.framework_attr.keys():
             if mha_node.framework_attr[ADD_ZERO_ATTN] is not False:
                 Logger.error('Add Zero Attn feature is Not Implemented')  # pragma: no cover
 
         # Check if Add Bias KV feature is Active
         if BIAS_K and BIAS_V in mha_node.weights.keys():
-            if mha_node.weights[BIAS_K] and mha_node.weights[BIAS_V] is not None:
+            if mha_node.weights[BIAS_K] is not None and mha_node.weights[BIAS_V] is not None:
                 Logger.error('Add BIAS_KV feature is Not Implemented')  # pragma: no cover
 
         self.embed_dim = mha_node.framework_attr[EMBED_DIM]
         self.num_heads = mha_node.framework_attr[NUM_HEADS]
 
-        if KEY_DIM in mha_node.framework_attr:
-            self.kdim = mha_node.framework_attr[KEY_DIM]
-        else:
-            self.kdim = False
+        self.kdim = mha_node.framework_attr[KEY_DIM]
 
-        if VALUE_DIM in mha_node.framework_attr:
-            self.vdim = mha_node.framework_attr[VALUE_DIM]
-        else:
-            self.vdim = False
+        self.vdim = mha_node.framework_attr[VALUE_DIM]
 
         self.qdim = int(self.embed_dim / self.num_heads)
 
         self.q_input, self.k_input, self.v_input = mha_node.input_shape
 
         # check for input correctness
         assert self.q_input[0] == self.k_input[0] == self.v_input[0], "Batch size must be equal to all inputs"
@@ -704,15 +698,15 @@
             graph: input graph
             mha_node: MHA node to substitute inputs and outputs with
         Returns:
             Graph after applying the substitution.
         """
 
         if mha_node.reuse:
-            raise Exception("MCT doesn't support reuse of MultiHeadAttention layer")
+            raise Exception("MCT doesn't support reuse of MultiHeadAttention layer")  # pragma: no cover
         params = MHAParams(mha_node)
 
         # project
         # (B, q_seq, q_dim*n_h) --> (B, q_dim*n_h, q_seq)
         # (B, kv_seq, k_dim) --> (B, q_dim*n_h, kv_seq)
         # (B, kv_seq, v_dim) --> (B, q_dim*n_h, kv_seq)
         q_transpose_node, k_transpose_node, v_transpose_node, \
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/permute_call_method.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/relu_bound_to_power_of_2.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher, WalkMatcher
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 from model_compression_toolkit.core.pytorch.constants import KERNEL, BIAS, INPLACE, HARDTANH_MIN_VAL, HARDTANH_MAX_VAL, \
     RELU_POT_BOUND
+from model_compression_toolkit.logger import Logger
 
 
 class ReLUBoundToPowerOfTwo(common.BaseSubstitution):
     """
     Substitution to scale the weights of two linear nodes, and move the bound of non-linear between them
     (if bounded) in order to use the entire constrained range when activations are quantized.
     """
@@ -98,16 +99,16 @@
                     (np.log2(non_linear_node.framework_attr[HARDTANH_MAX_VAL]).astype(int) -
                      np.log2(non_linear_node.framework_attr[HARDTANH_MAX_VAL]) == 0):
                 scale_factor = non_linear_node.framework_attr[HARDTANH_MAX_VAL] / self.threshold
                 non_linear_node.functional_op.__defaults__ = (0.0, self.threshold, non_linear_node.framework_attr[INPLACE])
             else:
                 return graph
         else:
-            common.Logger.error(f"In substitution with wrong matched pattern")
-        common.Logger.debug(
+            Logger.error(f"In substitution with wrong matched pattern")
+        Logger.debug(
             f"Node named:{non_linear_node.name} changed "
             f"to:{non_linear_node.type}")
 
         w2_fixed = scale_factor * second_op2d_node.get_weights_by_keys(KERNEL)
         w1_fixed = first_op2d_node.get_weights_by_keys(KERNEL) / scale_factor
         b1_fixed = first_op2d_node.get_weights_by_keys(BIAS) / scale_factor
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/reshape_with_static_shapes.py`

 * *Files 17% similar despite different names*

```diff
@@ -10,18 +10,21 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from torch import reshape
 import torch
+
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
+from model_compression_toolkit.core.pytorch.constants import BATCH_DIM_VALUE
 
 
 class ReshapeWithStaticShapes(common.BaseSubstitution):
     """
     Replace "reshape" or "view" shape attributes. Shape attributes are replaced to static const values.
     """
 
@@ -43,22 +46,33 @@
         Args:
             graph: Graph we apply the substitution on.
             node: node that match the pattern in the substitution init.
 
         Returns:
             Graph after applying the substitution.
         """
+        # we want the batch size value to infer from the length of the array and remaining dimensions
+        if len(node.output_shape) == 1:
+            node.output_shape[0][0] = BATCH_DIM_VALUE
+        else:
+            Logger.error('Reshape or view nodes should have a single output shape')  # pragma: no cover
+
         # configure the new static output shape attribute
         node.op_call_args = node.output_shape
 
         # modify the node input info
         node.input_shape = [node.input_shape[0]]
+
+        # the first input is the tensor to be reshaped, we want his batch size value to infer
+        # from the length of the array and remaining dimensions
+        node.input_shape[0][0] = BATCH_DIM_VALUE
+
         nodes_to_check = []
         for in_edge in graph.incoming_edges(node):
-            if in_edge.sink_index > 0: # the first input is the tensor to be reshaped
+            if in_edge.sink_index > 0:  # the first input is the tensor to be reshaped
                 nodes_to_check.append(in_edge.source_node)
                 graph.remove_edge(in_edge.source_node, node)
         for n in nodes_to_check:
             clean_graph_from_nodes_without_out_edges(graph, n)
         return graph
 
 
@@ -76,8 +90,8 @@
     if len(graph.out_edges(node)) == 0 and node not in output_nodes:
         nodes_to_check = []
         for in_edge in graph.incoming_edges(node):
             nodes_to_check.append(in_edge.source_node)
             graph.remove_edge(in_edge.source_node, node)
         graph.remove_node(node)
         for n in nodes_to_check:
-            clean_graph_from_nodes_without_out_edges(graph, n)
+            clean_graph_from_nodes_without_out_edges(graph, n)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/residual_collapsing.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
 import numpy as np
 import torch
 from torch.nn import Conv2d
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.common.substitutions.residual_collapsing import ResidualCollapsing
 from model_compression_toolkit.core.pytorch.constants import KERNEL
-from model_compression_toolkit.core.common.logger import Logger
+from model_compression_toolkit.logger import Logger
 
 
 def residual_collapsing_node_matchers() -> Tuple[NodeOperationMatcher, NodeOperationMatcher]:
     """
     Function generates matchers for matching:
     (Conv2D, Add)[activation=linear] -> Conv2D.
     Returns:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/scale_equalization.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/shift_negative_activation.py`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 
 import torch.nn.functional
 from torch.nn import Conv2d, Linear, PReLU, ELU, Hardswish, Dropout, ZeroPad2d, SiLU
 from torch import reshape
 from torch.nn.functional import hardswish, silu, prelu, elu
 from torch.nn.functional import avg_pool2d
 
-from model_compression_toolkit import CoreConfig, FrameworkInfo
+from model_compression_toolkit.core import CoreConfig, FrameworkInfo
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.graph.graph_matchers import EdgeMatcher
 from model_compression_toolkit.core.common.graph.graph_matchers import NodeOperationMatcher
 from model_compression_toolkit.core.common.substitutions.shift_negative_activation import apply_shift_negative_correction
 from model_compression_toolkit.core.pytorch.constants import PAD, VALUE, PADDING, BIAS, USE_BIAS
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/softmax_shift.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/virtual_activation_weights_composition.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/graph_substitutions/substitutions/weights_activation_split.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/kpi_data_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/kpi_data_facade.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,29 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import PYTORCH
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import PYTORCH
+from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_data import compute_kpi_data
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfig, DEFAULT_MIXEDPRECISION_CONFIG, MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common.constants import FOUND_TORCH
+from model_compression_toolkit.constants import FOUND_TORCH
 
 if FOUND_TORCH:
     from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
     from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.core.pytorch.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from torch.nn import Module
 
     from model_compression_toolkit import get_target_platform_capabilities
 
     PYTORCH_DEFAULT_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
 
@@ -47,15 +47,15 @@
         Builds the computation graph from the given model and target platform capabilities, and uses it to compute the KPI data.
 
         Args:
             in_model (Model): PyTorch model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
             quant_config (MixedPrecisionQuantizationConfig): MixedPrecisionQuantizationConfig containing parameters of how the model should be quantized.
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to. `Default PyTorch TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/pytorch_tp_models/pytorch_default.py>`_
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
 
         Returns:
             A KPI object with total weights parameters sum, max activation tensor and total kpi.
 
         Examples:
 
             Import a Pytorch model:
@@ -71,15 +71,15 @@
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 3, 224, 224))]
 
             Import mct and call for KPI data calculation:
 
             >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.pytorch_kpi_data(module, repr_datagen)
+            >>> kpi_data = mct.core.pytorch_kpi_data(module, repr_datagen)
 
         """
 
         if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
             Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfig object."
                          "Given quant_config is not of type MixedPrecisionQuantizationConfig.")
 
@@ -107,15 +107,15 @@
         Builds the computation graph from the given model and target platform capabilities, and uses it to compute the KPI data.
 
         Args:
             in_model (Model): PyTorch model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
             core_config (CoreConfig): CoreConfig containing parameters for quantization and mixed precision
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to. `Default PyTorch TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/pytorch_tp_models/pytorch_default.py>`_
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
 
         Returns:
 
             A KPI object with total weights parameters sum and max activation tensor.
 
         Examples:
 
@@ -128,15 +128,15 @@
 
             >>> import numpy as np
             >>> def repr_datagen(): yield [np.random.random((1, 3, 224, 224))]
 
             Import mct and call for KPI data calculation:
 
             >>> import model_compression_toolkit as mct
-            >>> kpi_data = mct.pytorch_kpi_data(module, repr_datagen)
+            >>> kpi_data = mct.core.pytorch_kpi_data(module, repr_datagen)
 
         """
 
         if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
             Logger.error("KPI data computation can't be executed without MixedPrecisionQuantizationConfigV2 object."
                          "Given quant_config is not of type MixedPrecisionQuantizationConfigV2.")
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/mixed_precision_wrapper.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 # ==============================================================================
 
 from typing import Any, List
 
 import torch
 import copy
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.pytorch.utils import set_model, to_torch_tensor
 
 
 class PytorchMixedPrecisionWrapper(torch.nn.Module):
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/mixed_precision/set_layer_to_bitwidth.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/mixed_precision/set_layer_to_bitwidth.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/pytorch_implementation.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/pytorch_implementation.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,24 +9,25 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import operator
+from copy import deepcopy
 from typing import List, Any, Tuple, Callable, Type, Dict
 
 import numpy as np
 import torch
 from torch import sigmoid, softmax, add, cat, argmax
 from torch.nn import Conv2d, ConvTranspose2d, Linear
 from torch.nn import Module, Sigmoid, Softmax
 
 import model_compression_toolkit.core.pytorch.constants as pytorch_constants
-from model_compression_toolkit import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import QuantizationConfig, FrameworkInfo, CoreConfig, MixedPrecisionQuantizationConfigV2
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.collectors.statistics_collector import BaseStatsCollector
 from model_compression_toolkit.core.common.collectors.statistics_collector_generator import \
     create_stats_collector_for_node
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.mixed_precision.sensitivity_evaluation import SensitivityEvaluation
@@ -70,18 +71,15 @@
 from model_compression_toolkit.core.pytorch.graph_substitutions.substitutions.weights_activation_split import \
     WeightsActivationSplit
 from model_compression_toolkit.core.pytorch.mixed_precision.set_layer_to_bitwidth import set_layer_to_bitwidth
 from model_compression_toolkit.core.pytorch.pytorch_node_prior_info import create_node_prior_info
 from model_compression_toolkit.core.pytorch.reader.reader import model_reader
 from model_compression_toolkit.core.pytorch.statistics_correction.apply_second_moment_correction import \
     pytorch_apply_second_moment_correction
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
-from model_compression_toolkit.core.pytorch.utils import torch_tensor_to_numpy
-from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.pytorch.gptq_training import PytorchGPTQTrainer
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy, set_model
 
 
 class PytorchImplementation(FrameworkImplementation):
     """
     An class with implemented methods to support optimizing Pytorch models.
     """
 
@@ -123,15 +121,17 @@
         Convert a framework's module into a graph.
         Args:
             module: Framework's module.
             representative_data_gen (Callable): Dataset used for calibration.
         Returns:
             Graph representing the input module.
         """
-        return model_reader(module, representative_data_gen, self.to_numpy, self.to_tensor)
+        _module = deepcopy(module)
+        _module.eval()
+        return model_reader(_module, representative_data_gen, self.to_numpy, self.to_tensor)
 
     def model_builder(self,
                       graph: Graph,
                       mode: ModelBuilderMode,
                       append2output: List[Any] = None,
                       fw_info: FrameworkInfo = DEFAULT_PYTORCH_INFO,
                       return_float_outputs: bool = False) -> Tuple[Module, UserInformation]:
@@ -319,20 +319,14 @@
             A list of the framework substitutions used after we apply second moment statistics.
         """
         substitutions_list = []
         if quant_config.weights_second_moment_correction:
             substitutions_list.append(pytorch_batchnorm_refusing())
         return substitutions_list
 
-    def get_gptq_trainer_obj(self) -> Type[GPTQTrainer]:
-        """
-        Returns: GPTQTrainer object
-        """
-        return PytorchGPTQTrainer
-
     def get_sensitivity_evaluator(self,
                                   graph: Graph,
                                   quant_config: MixedPrecisionQuantizationConfigV2,
                                   representative_data_gen: Callable,
                                   fw_info: FrameworkInfo,
                                   disable_activation_for_metric: bool = False) -> SensitivityEvaluation:
         """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py`

 * *Files 1% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Any, Tuple
 import numpy as np
 from torch.nn import BatchNorm2d
 
-from model_compression_toolkit import FrameworkInfo
+from model_compression_toolkit.core import FrameworkInfo
 from model_compression_toolkit.core.common import BaseNode, Graph
 from model_compression_toolkit.core.common.node_prior_info import NodePriorInfo
 from model_compression_toolkit.core.pytorch.constants import MOVING_MEAN, MOVING_VARIANCE, GAMMA, BETA
 
 
 def create_node_prior_info(node: BaseNode,
                            fw_info: FrameworkInfo,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/pytorch_quantization_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,40 +10,39 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable, List, Tuple
 
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import PYTORCH
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import PYTORCH
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, GradientPTQConfigV2
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.network_editors.actions import EditRule
 from model_compression_toolkit.core.common.quantization.core_config import CoreConfig
 from model_compression_toolkit.core.common.quantization.debug_config import DebugConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfig, DEFAULT_MIXEDPRECISION_CONFIG
 from model_compression_toolkit.core.common.quantization.quantization_config import QuantizationConfig
 from model_compression_toolkit.core.common.quantization.quantization_config import DEFAULTCONFIG
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.ptq.runner import ptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-from model_compression_toolkit.core.common.constants import FOUND_TORCH
+from model_compression_toolkit.constants import FOUND_TORCH
 
 if FOUND_TORCH:
     from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
     from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.core.pytorch.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from torch.nn import Module
 
     from model_compression_toolkit import get_target_platform_capabilities
     DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
     def pytorch_post_training_quantization(in_module: Module,
                                            representative_data_gen: Callable,
@@ -72,15 +71,15 @@
             representative_data_gen (Callable): Dataset used for calibration.
             n_iter (int): Number of calibration iterations to run.
             quant_config (QuantizationConfig): QuantizationConfig containing parameters of how the module should be quantized. `Default configuration. <https://github.com/sony/model_optimization/blob/21e21c95ca25a31874a5be7af9dd2dd5da8f3a10/model_compression_toolkit/core/common/quantization/quantization_config.py#L154>`_
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
             network_editor (List[EditRule]): List of EditRules. Each EditRule consists of a node filter and an action to change quantization settings of the filtered nodes.
             gptq_config (GradientPTQConfig): Configuration for using gptq (e.g. optimizer).
             analyze_similarity (bool): Whether to plot similarity figures within TensorBoard (when logger is enabled) or not.
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to. `Default PyTorch TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/pytorch_tp_models/pytorch_default.py>`_
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
 
 
         Returns:
             A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
@@ -96,14 +95,16 @@
 
             Import mct and pass the module with the representative dataset generator to get a quantized module:
 
             >>> import model_compression_toolkit as mct
             >>> quantized_module, quantization_info = mct.pytorch_post_training_quantization(module, repr_datagen)
 
         """
+        Logger.warning('pytorch_post_training_quantization is deprecated and will be removed '
+                       'in the future. Please use mct.ptq.pytorch_post_training_quantization_experimental instead.')
 
         core_config = CoreConfig(quant_config,
                                  debug_config=DebugConfig(analyze_similarity=analyze_similarity,
                                                           network_editor=network_editor))
 
         tb_w = _init_tensorboard_writer(fw_info)
 
@@ -171,15 +172,15 @@
              target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
              n_iter (int): Number of calibration iterations to run.
              quant_config (MixedPrecisionQuantizationConfig): QuantizationConfig containing parameters of how the model should be quantized.
              fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default PyTorch info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/pytorch/default_framework_info.py>`_
              network_editor (List[EditRule]): List of EditRules. Each EditRule consists of a node filter and an action to change quantization settings of the filtered nodes.
              gptq_config (GradientPTQConfig): Configuration for using GPTQ (e.g. optimizer).
              analyze_similarity (bool): Whether to plot similarity figures within TensorBoard (when logger is enabled) or not.
-             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to. `Default PyTorch TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/pytorch_tp_models/pytorch_default.py>`_
+             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
 
          Returns:
              A quantized model and information the user may need to handle the quantized model.
 
          Examples:
 
              Import MCT:
@@ -195,37 +196,40 @@
 
              >>> import numpy as np
              >>> def repr_datagen(): return [np.random.random((1,224,224,3))]
 
              Create a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
              The candidates bitwidth for quantization should be defined in the target platform model:
 
-             >>> config = mct.MixedPrecisionQuantizationConfig()
+             >>> config = mct.core.MixedPrecisionQuantizationConfig()
 
              Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
              that should be quantized (for example, the kernel of Conv2D in PyTorch will be affected by this value,
              while the bias will not):
 
-             >>> kpi = mct.KPI(sum(p.numel() for p in module.parameters()) * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+             >>> kpi = mct.core.KPI(sum(p.numel() for p in module.parameters()) * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
              Pass the model, the representative dataset generator, the configuration and the target KPI to get a
              quantized model:
 
              >>> quantized_model, quantization_info = mct.pytorch_post_training_quantization_mixed_precision(module, repr_datagen, n_iter=10, quant_config=config, target_kpi=kpi)
 
              For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/experimental_api_docs/modules/mixed_precision_quantization_config.html#model_compression_toolkit.MixedPrecisionQuantizationConfigV2>`_.
 
          """
 
+        Logger.warning('pytorch_post_training_quantization_mixed_precision is deprecated and will be removed '
+                       'in the future. Please use mct.ptq.pytorch_post_training_quantization_experimental instead.')
+
         if not isinstance(quant_config, MixedPrecisionQuantizationConfig):
-            common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+            Logger.error("Given quantization config to mixed-precision facade is not of type "
                                 "MixedPrecisionQuantizationConfig. Please use pytorch_post_training_quantization API, "
                                 "or pass a valid mixed precision configuration.")
 
-        common.Logger.info("Using experimental mixed-precision quantization. "
+        Logger.info("Using experimental mixed-precision quantization. "
                            "If you encounter an issue please file a bug.")
 
         quantization_config, mp_config = quant_config.separate_configs()
         core_config = CoreConfig(quantization_config=quantization_config,
                                  mixed_precision_config=mp_config,
                                  debug_config=DebugConfig(analyze_similarity=analyze_similarity,
                                                           network_editor=network_editor))
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/fake_quant_builder.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,18 +8,18 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Tuple, Callable
+from typing import Callable
 import torch
 
-from model_compression_toolkit.core.common.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
+from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
 from model_compression_toolkit.core.common.quantization.quantizers.uniform_quantizers import threshold_is_power_of_two
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
 
 
 def get_symmetric_quantization_range_and_scale(activation_is_signed: bool,
                                                activation_n_bits: int,
                                                activation_threshold: float):
@@ -122,15 +122,15 @@
     a = 0 if a > 0 else a
     b = 0 if b < 0 else b
     a, b = fix_range_to_include_zero(a, b, activation_n_bits)
 
     min_value = 0
     max_value = 2 ** activation_n_bits - 1
     scale = (b - a) / ((2 ** activation_n_bits) - 1)
-    zero_point = -int(a / scale)  # zp has to be positive, and a <=0, so we multiply by -1
+    zero_point = -round(a / scale)  # zp has to be positive, and a <=0, so we multiply by -1
 
     return lambda x: q(x, min_value, max_value, scale, zero_point)
 
 
 def q(x: torch.Tensor,
       min_value: int,
       max_value: int,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Dict, Callable
 
 import torch
 import numpy as np
 
-from model_compression_toolkit.core.common.constants import SIGNED, CLUSTER_CENTERS, THRESHOLD, MULTIPLIER_N_BITS, EPS
+from model_compression_toolkit.constants import SIGNED, CLUSTER_CENTERS, THRESHOLD, MULTIPLIER_N_BITS, EPS
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 
 
 def activation_lut_kmean_quantizer(activation_n_bits: int,
                                    quantization_params: Dict[str, np.ndarray]) -> Callable:
     """
     Builds a LUT quantizer for layer's activation using the provided params (threshold and clusters).
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/graph_builders.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/graph_builders.py`

 * *Files 2% similar despite different names*

```diff
@@ -21,14 +21,15 @@
 from model_compression_toolkit.core.common import BaseNode
 from model_compression_toolkit.core.common.graph.base_graph import OutTensor
 from model_compression_toolkit.core.common.graph.edge import Edge
 from model_compression_toolkit.core.common.graph.functional_node import FunctionalNode
 from model_compression_toolkit.core.pytorch.constants import OUTPUT, PLACEHOLDER, TENSOR_META, CALL_FUNCTION, TYPE, \
     CALL_METHOD, BIAS, FUNCTIONAL_OP, OP_CALL_KWARGS, OP_CALL_ARGS, INPUTS_AS_LIST, GET_ATTR, CONSTANT, BUFFER
 from model_compression_toolkit.core.pytorch.reader.node_holders import DummyPlaceHolder, ConstantHolder, BufferHolder
+from model_compression_toolkit.logger import Logger
 
 
 def extract_holder_weights(constant_name, node_target, model, weights, to_numpy):
     """
     Extract layer weights and named buffers for BufferHolder and ConstantHolder.
     Args:
         constant_name: name to write the parameters under, CONSTANT for ConstantHolder and
@@ -60,14 +61,15 @@
                   to_numpy: Callable) -> Tuple[List, List, List, Dict]:
     """
     Build a node from a fx node. A node contains all information to reconstruct the model module or call function
     it's representing in the model: operation, module configuration, weights, input/output shape.
     Args:
         model: Pytorch FX model.
         module_dict: A dictionary of the Pyotrch model's named modules.
+        to_numpy: A function to convert a Tensor to numpy array
 
     Returns:
         A list of Graph nodes that were built from the fx GraphModule nodes.
     """
     # init function variables:
     inputs = []
     outputs = []
@@ -87,15 +89,15 @@
             framework_attr = {k: v for k, v in framework_attr.items() if k in fullargspec}
             if hasattr(node_module, BIAS) and BIAS in fullargspec:
                 framework_attr[BIAS] = False if node_module.bias is None else True
         elif node.op == CALL_FUNCTION:
             node_type = node.target
             if node_type == getattr:
                 node_has_activation = False
-                common.Logger.warning(
+                Logger.warning(
                     'Pytorch model has a parameter or constant Tensor value. This can cause unexpected behaviour when '
                     'converting the model.')
         elif node.op == PLACEHOLDER:
             node_type = DummyPlaceHolder
         elif node.op == OUTPUT:
             output_nodes += node.all_input_nodes
             continue
@@ -108,15 +110,15 @@
                 raise Exception(f'Call method of type \'{node.target}\' is currently not supported.')
         elif node.op == GET_ATTR:
             if node.meta[TYPE] == torch.Tensor:
                 node_type = BufferHolder
             else:
                 node_type = ConstantHolder
             node_has_activation = False
-            common.Logger.warning(
+            Logger.warning(
                 'Pytorch model has a parameter or constant Tensor value. This can cause unexpected behaviour when '
                 'converting the model.')
         else:
             raise Exception(f'Unknown node type: {node.name}')
 
         # extract layer weights and named buffers
         weights = {}
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/node_holders.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/node_holders.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/reader/reader.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/reader/reader.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/statistics_correction/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py`

 * *Files 2% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 # ==============================================================================
 import copy
 from typing import Any, Callable
 
 import torch
 from tqdm import tqdm
 
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.pytorch.constants import GAMMA, BETA, MOVING_MEAN, MOVING_VARIANCE
 from model_compression_toolkit.core.pytorch.utils import set_model, to_torch_tensor
 
 
 def pytorch_apply_second_moment_correction(quantized_model: Any,
                                            core_config: CoreConfig,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/pytorch/utils.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/pytorch/utils.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/runner.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/core/runner.py`

 * *Files 2% similar despite different names*

```diff
@@ -18,15 +18,15 @@
 from typing import Callable, Tuple, Any, List, Dict
 
 import numpy as np
 from tqdm import tqdm
 
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common import FrameworkInfo
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common.fusion.layer_fusing import fusion
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.mixed_precision.bit_width_setter import set_bit_widths
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI, KPITarget
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_aggregation_methods import MpKpiAggregation
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi_functions_mapping import kpi_functions_mapping
@@ -44,15 +44,15 @@
 from model_compression_toolkit.core.common.quantization.set_node_quantization_config import \
     set_quantization_configuration_to_graph
 from model_compression_toolkit.core.common.statistics_correction.statistics_correction import \
     statistics_correction_runner
 from model_compression_toolkit.core.common.substitutions.apply_substitutions import substitute
 from model_compression_toolkit.core.common.substitutions.linear_collapsing_substitution import \
     linear_collapsing_substitute
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 from model_compression_toolkit.core.common.visualization.final_config_visualizer import \
     WeightsFinalBitwidthConfigVisualizer, \
     ActivationFinalBitwidthConfigVisualizer
 from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
 
 
 def core_runner(in_model: Any,
@@ -139,17 +139,17 @@
                    fw_impl=fw_impl)
 
     if target_kpi is not None:
         # Retrieve lists of tuples (node, node's final weights/activation bitwidth)
         weights_conf_nodes_bitwidth = tg.get_final_weights_config()
         activation_conf_nodes_bitwidth = tg.get_final_activation_config()
 
-        common.Logger.info(
+        Logger.info(
             f'Final weights bit-width configuration: {[node_b[1] for node_b in weights_conf_nodes_bitwidth]}')
-        common.Logger.info(
+        Logger.info(
             f'Final activation bit-width configuration: {[node_b[1] for node_b in activation_conf_nodes_bitwidth]}')
 
         if tb_w is not None:
             if len(weights_conf_nodes_bitwidth) > 0:
                 visual = WeightsFinalBitwidthConfigVisualizer(weights_conf_nodes_bitwidth)
                 figure = visual.plot_config_bitwidth()
                 tb_w.add_figure(figure, f'Weights final bit-width config')
@@ -255,17 +255,17 @@
     Args:
         fw_info: FrameworkInfo object.
 
     Returns:
         A TensorBoardWriter object.
     """
     tb_w = None
-    if common.Logger.LOG_PATH is not None:
-        tb_log_dir = os.path.join(os.getcwd(), common.Logger.LOG_PATH, 'tensorboard_logs')
-        common.Logger.info(f'To use Tensorboard, please run: tensorboard --logdir {tb_log_dir}')
+    if Logger.LOG_PATH is not None:
+        tb_log_dir = os.path.join(os.getcwd(), Logger.LOG_PATH, 'tensorboard_logs')
+        Logger.info(f'To use Tensorboard, please run: tensorboard --logdir {tb_log_dir}')
         tb_w = TensorboardWriter(tb_log_dir, fw_info)
     return tb_w
 
 
 def read_model_to_graph(in_model: Any,
                         representative_data_gen: Callable,
                         tpc: TargetPlatformCapabilities,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/latest/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,18 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH
-
-from model_compression_toolkit.core.tpc_models.default_tpc.v4.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
-
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4.tpc_keras import get_keras_tpc as get_keras_tpc_latest
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4.tpc_keras import generate_keras_tpc
-
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tpc_keras import generate_keras_tpc
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4.tpc_pytorch import generate_pytorch_tpc
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tpc_pytorch import generate_pytorch_tpc
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/target_platform_capabilities.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/target_platform_capabilities.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,29 +9,31 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH, LATEST
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH
+from model_compression_toolkit.target_platform_capabilities.constants import LATEST
+
 
 ###############################
 # Build Tensorflow TPC models
 ###############################
 keras_tpc_models_dict = None
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.default_tpc.latest import get_keras_tpc_latest
-    from model_compression_toolkit.core.tpc_models.default_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
-    from model_compression_toolkit.core.tpc_models.default_tpc.v2.tpc_keras import get_keras_tpc as get_keras_tpc_v2
-    from model_compression_toolkit.core.tpc_models.default_tpc.v3.tpc_keras import get_keras_tpc as get_keras_tpc_v3
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4.tpc_keras import get_keras_tpc as get_keras_tpc_v4
-    from model_compression_toolkit.core.tpc_models.default_tpc.v3_lut.tpc_keras import get_keras_tpc as get_keras_tpc_v3_lut
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4_lut.tpc_keras import get_keras_tpc as get_keras_tpc_v4_lut
-    from model_compression_toolkit.core.tpc_models.default_tpc.v5.tpc_keras import get_keras_tpc as get_keras_tpc_v5
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.latest import get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2.tpc_keras import get_keras_tpc as get_keras_tpc_v2
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tpc_keras import get_keras_tpc as get_keras_tpc_v3
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_keras import get_keras_tpc as get_keras_tpc_v4
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3_lut.tpc_keras import get_keras_tpc as get_keras_tpc_v3_lut
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4_lut.tpc_keras import get_keras_tpc as get_keras_tpc_v4_lut
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v5.tpc_keras import get_keras_tpc as get_keras_tpc_v5
 
     # Keras: TPC versioning
     keras_tpc_models_dict = {'v1': get_keras_tpc_v1(),
                              'v2': get_keras_tpc_v2(),
                              'v3': get_keras_tpc_v3(),
                              'v3_lut': get_keras_tpc_v3_lut(),
                              'v4': get_keras_tpc_v4(),
@@ -40,28 +42,28 @@
                              LATEST: get_keras_tpc_latest()}
 
 ###############################
 # Build Pytorch TPC models
 ###############################
 pytorch_tpc_models_dict = None
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.default_tpc.latest import get_pytorch_tpc_latest
-    from model_compression_toolkit.core.tpc_models.default_tpc.v1.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.latest import get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v1
-    from model_compression_toolkit.core.tpc_models.default_tpc.v2.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v2
-    from model_compression_toolkit.core.tpc_models.default_tpc.v3.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v3
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v4
-    from model_compression_toolkit.core.tpc_models.default_tpc.v3_lut.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3_lut.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v3_lut
-    from model_compression_toolkit.core.tpc_models.default_tpc.v4_lut.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4_lut.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v4_lut
-    from model_compression_toolkit.core.tpc_models.default_tpc.v5.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v5.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v5
 
     # Pytorch: TPC versioning
     pytorch_tpc_models_dict = {'v1': get_pytorch_tpc_v1(),
                                'v2': get_pytorch_tpc_v2(),
                                'v3': get_pytorch_tpc_v3(),
                                'v3_lut': get_pytorch_tpc_v3_lut(),
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tp_model.py`

 * *Files 5% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -102,14 +105,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                          tp.get_default_quantization_config_options().clone_and_edit(
                              enable_weights_quantization=False,
                              enable_activation_quantization=False))
 
         # Define operator sets that use mixed_precision_configuration_options:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_keras.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,17 +19,17 @@
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Reshape, ZeroPadding2D, \
         Dropout, \
         MaxPooling2D, Activation, ReLU, Flatten, Cropping2D
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Reshape, ZeroPadding2D, \
         Dropout, MaxPooling2D, Activation, ReLU, Flatten, Cropping2D
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v1/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_pytorch.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 import torch
 from torch import flatten, reshape, split, unsqueeze, dropout, chunk
 from torch.nn import Conv2d, BatchNorm2d
 from torch.nn import Dropout, Flatten
 from torch.nn import ReLU, ReLU6
 from torch.nn.functional import relu, relu6
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tp_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -102,14 +105,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                          tp.get_default_quantization_config_options().clone_and_edit(
                              enable_weights_quantization=False,
                              enable_activation_quantization=False))
 
         conv = tp.OperatorsSet("Conv")
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_keras.py`

 * *Files 2% similar despite different names*

```diff
@@ -20,17 +20,17 @@
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
         Dropout, \
         MaxPooling2D, Activation, ReLU, PReLU, Flatten, Cropping2D
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
         Dropout, MaxPooling2D, Activation, ReLU, PReLU, Flatten, Cropping2D
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v2.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v2 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v2/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_pytorch.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 import torch
 from torch import flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk
 from torch.nn import Conv2d, Linear, BatchNorm2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v2.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v2 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v2 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tp_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -109,14 +112,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                          tp.get_default_quantization_config_options().clone_and_edit(
                              enable_weights_quantization=False,
                              enable_activation_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_keras.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,37 +10,37 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3_lut import __version__ as TPC_VERSION
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
         Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
         Dropout, MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v3.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v3 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Keras TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    default_tp_model = get_tp_model()
-    return generate_keras_tpc(name='default_keras_tpc', tp_model=default_tp_model)
+    lut_mp_tp_model = get_tp_model()
+    return generate_keras_tpc(name='default_keras_lut_mp_tpc', tp_model=lut_mp_tp_model)
 
 
 def generate_keras_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
 
     Args:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_pytorch.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind
 from torch.nn import Conv2d, Linear, BatchNorm2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v3.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v3 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tp_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,16 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel, \
-    QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel, QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -111,14 +113,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                         tp.get_default_quantization_config_options().clone_and_edit(
                             enable_weights_quantization=False,
                             enable_activation_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_keras.py`

 * *Files 5% similar despite different names*

```diff
@@ -10,37 +10,37 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
-from model_compression_toolkit.core.tpc_models.default_tpc.v3_lut import __version__ as TPC_VERSION
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
         Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, \
         Dropout, MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v3_lut.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tp_model import get_tp_model
 import model_compression_toolkit as mct
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Keras TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    lut_mp_tp_model = get_tp_model()
-    return generate_keras_tpc(name='default_keras_lut_mp_tpc', tp_model=lut_mp_tp_model)
+    default_tp_model = get_tp_model()
+    return generate_keras_tpc(name='default_keras_tpc', tp_model=default_tp_model)
 
 
 def generate_keras_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
 
     Args:
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_pytorch.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,17 +18,17 @@
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind
 from torch.nn import Conv2d, Linear, BatchNorm2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v3_lut.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v3_lut import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3_lut import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tp_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -109,14 +112,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                          tp.get_default_quantization_config_options().clone_and_edit(
                              enable_weights_quantization=False,
                              enable_activation_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_keras.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,22 +20,21 @@
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v4.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
-
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Keras TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
     default_tp_model = get_tp_model()
     return generate_keras_tpc(name='default_keras_tpc', tp_model=default_tp_model)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_pytorch.py`

 * *Files 3% similar despite different names*

```diff
@@ -19,17 +19,17 @@
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, \
     permute, transpose, equal, gather, topk
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v3.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tp_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,16 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel, \
-    QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel, QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -111,14 +113,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                         tp.get_default_quantization_config_options().clone_and_edit(
                             enable_weights_quantization=False,
                             enable_activation_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_keras.py`

 * *Files 1% similar despite different names*

```diff
@@ -10,26 +10,26 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 from packaging import version
-from model_compression_toolkit.core.tpc_models.default_tpc.v4_lut import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4_lut import __version__ as TPC_VERSION
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v4_lut.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,34 +13,34 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, permute, transpose
+    gather, equal, transpose, permute
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v4_lut.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v4_lut import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    lut_mp_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='default_pytorch_lut_mp_tpc', tp_model=lut_mp_tp_model)
+    imx500_tpc_tp_model = get_tp_model()
+    return generate_pytorch_tpc(name='imx500_tpc_pytorch_tpc', tp_model=imx500_tpc_tp_model)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tp_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -109,14 +112,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                          tp.get_default_quantization_config_options().clone_and_edit(
                              enable_weights_quantization=False,
                              enable_activation_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_keras.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,17 +20,17 @@
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v4.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/default_tpc/v5/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_pytorch.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,17 +19,17 @@
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
     gather, equal, transpose, permute
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.core.tpc_models.default_tpc.v3.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v3.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/get_target_platform_capabilities.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,63 +1,46 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH
+from model_compression_toolkit.target_platform_capabilities.constants import LATEST
+
+###############################
+# Build Tensorflow TPC models
+###############################
+keras_tpc_models_dict = None
+if FOUND_TF:
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.latest import get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+
+    # Keras: TPC versioning
+    keras_tpc_models_dict = {'v1': get_keras_tpc_v1(),
+                             LATEST: get_keras_tpc_latest()}
+
+###############################
+# Build Pytorch TPC models
+###############################
+pytorch_tpc_models_dict = None
+if FOUND_TORCH:
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.latest import get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_pytorch import \
+        get_pytorch_tpc as get_pytorch_tpc_v1
+
+    # Pytorch: TPC versioning
+    pytorch_tpc_models_dict = {'v1': get_pytorch_tpc_v1(),
+                               LATEST: get_pytorch_tpc_latest()}
+
+tpc_dict = {TENSORFLOW: keras_tpc_models_dict,
+            PYTORCH: pytorch_tpc_models_dict}
 
-from model_compression_toolkit.core.tpc_models.default_tpc.target_platform_capabilities import \
-    tpc_dict as default_tpc_dict
-from model_compression_toolkit.core.tpc_models.imx500_tpc.target_platform_capabilities import \
-    tpc_dict as imx500_tpc_dict
-from model_compression_toolkit.core.tpc_models.tflite_tpc.target_platform_capabilities import \
-    tpc_dict as tflite_tpc_dict
-from model_compression_toolkit.core.tpc_models.qnnpack_tpc.target_platform_capabilities import \
-    tpc_dict as qnnpack_tpc_dict
-from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL, IMX500_TP_MODEL, TFLITE_TP_MODEL, \
-    QNNPACK_TP_MODEL
-from model_compression_toolkit.core.common.constants import LATEST
-
-tpc_dict = {DEFAULT_TP_MODEL: default_tpc_dict,
-            IMX500_TP_MODEL: imx500_tpc_dict,
-            TFLITE_TP_MODEL: tflite_tpc_dict,
-            QNNPACK_TP_MODEL: qnnpack_tpc_dict}
-
-
-def get_target_platform_capabilities(fw_name: str,
-                                     target_platform_name: str,
-                                     target_platform_version: str = None) -> TargetPlatformCapabilities:
-    """
-    Get a TargetPlatformCapabilities by the target platform model name and the framework name.
-    For now, it supports frameworks 'tensorflow' and 'pytorch'. For both of them
-    the target platform model can be 'default', 'imx500', 'tflite', or 'qnnpack'.
-
-    Args:
-        fw_name: Framework name of the TargetPlatformCapabilities.
-        target_platform_name: Target platform model name the model will use for inference.
-        target_platform_version: Target platform capabilities version.
-    Returns:
-        A TargetPlatformCapabilities object that models the hardware and attaches
-        a framework information to it.
-    """
-    assert target_platform_name in tpc_dict, f'Target platform {target_platform_name} is not defined!'
-    fw_tpc = tpc_dict.get(target_platform_name)
-    assert fw_name in fw_tpc, f'Framework {fw_name} is not supported in {target_platform_name}. Please make sure the relevant ' \
-                              f'packages are installed when using MCT for optimizing a {fw_name} model. ' \
-                              f'For Tensorflow, please install tensorflow and tensorflow-model-optimization. ' \
-                              f'For PyTorch, please install torch.'
-    tpc_versions = fw_tpc.get(fw_name)
-    if target_platform_version is None:
-        target_platform_version = LATEST
-    else:
-        assert target_platform_version in tpc_versions, f'TPC version {target_platform_version} is not supported for framework {fw_name}'
-    return tpc_versions.get(target_platform_version)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/latest/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,17 +8,17 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH
-from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tp_model import get_tp_model, generate_tp_model, \
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model, generate_tp_model, \
     get_op_quantization_configs
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_latest
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tpc_keras import generate_keras_tpc
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_keras import generate_keras_tpc
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tpc_pytorch import get_pytorch_tpc as \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_pytorch import get_pytorch_tpc as \
         get_pytorch_tpc_latest
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tpc_pytorch import generate_pytorch_tpc
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tpc_pytorch import generate_pytorch_tpc
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/target_platform_capabilities.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,44 +1,46 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH, LATEST
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH
+from model_compression_toolkit.target_platform_capabilities.constants import LATEST
+
 
 ###############################
 # Build Tensorflow TPC models
 ###############################
 keras_tpc_models_dict = None
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.latest import get_keras_tpc_latest
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.latest import get_keras_tpc_latest
 
     # Keras: TPC versioning
     keras_tpc_models_dict = {'v1': get_keras_tpc_v1(),
                              LATEST: get_keras_tpc_latest()}
 
 ###############################
 # Build Pytorch TPC models
 ###############################
 pytorch_tpc_models_dict = None
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.latest import get_pytorch_tpc_latest
-    from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.latest import get_pytorch_tpc_latest
 
     # Pytorch: TPC versioning
     pytorch_tpc_models_dict = {'v1': get_pytorch_tpc_v1(),
                                LATEST: get_pytorch_tpc_latest()}
 
 tpc_dict = {TENSORFLOW: keras_tpc_models_dict,
             PYTORCH: pytorch_tpc_models_dict}
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -109,14 +112,17 @@
     with generated_tpc:
         # Create an OperatorsSet to represent a set of operations.
         # Each OperatorsSet has a unique label.
         # If a quantization configuration options is passed, these options will
         # be used for operations that will be attached to this set's label.
         # Otherwise, it will be a configure-less set (used in fusing):
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
         # May suit for operations like: Dropout, Reshape, etc.
         tp.OperatorsSet("NoQuantization",
                         tp.get_default_quantization_config_options().clone_and_edit(
                             enable_weights_quantization=False,
                             enable_activation_quantization=False))
 
         # Create Mixed-Precision quantization configuration options from the given list of OpQuantizationConfig objects
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py`

 * *Files 3% similar despite different names*

```diff
@@ -20,17 +20,17 @@
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Dense, Reshape, ZeroPadding2D, Dropout, \
         MaxPooling2D, Activation, ReLU, Add, Subtract, Multiply, PReLU, Flatten, Cropping2D, LeakyReLU, Permute, \
         Conv2DTranspose
 
-from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_pytorch.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,34 +13,34 @@
 # limitations under the License.
 # ==============================================================================
 
 import operator
 
 import torch
 from torch import add, sub, mul, div, flatten, reshape, split, unsqueeze, dropout, sigmoid, tanh, chunk, unbind, topk, \
-    gather, equal, transpose, permute
+    gather, equal, permute, transpose
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d
 from torch.nn import Dropout, Flatten, Hardtanh
 from torch.nn import ReLU, ReLU6, PReLU, SiLU, Sigmoid, Tanh, Hardswish, LeakyReLU
 from torch.nn.functional import relu, relu6, prelu, silu, hardtanh, hardswish, leaky_relu
 
-from model_compression_toolkit.core.tpc_models.imx500_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4_lut.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.imx500_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4_lut import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
     Returns: a Pytorch TargetPlatformCapabilities object for the given TargetPlatformModel.
     """
-    imx500_tpc_tp_model = get_tp_model()
-    return generate_pytorch_tpc(name='imx500_tpc_pytorch_tpc', tp_model=imx500_tpc_tp_model)
+    lut_mp_tp_model = get_tp_model()
+    return generate_pytorch_tpc(name='default_pytorch_lut_mp_tpc', tp_model=lut_mp_tp_model)
 
 
 def generate_pytorch_tpc(name: str, tp_model: tp.TargetPlatformModel):
     """
     Generates a TargetPlatformCapabilities object with default operation sets to layers mapping.
     Args:
         name: Name of the TargetPlatformModel.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/latest/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH
-from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH
+from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_latest
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tpc_keras import generate_keras_tpc
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_keras import generate_keras_tpc
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tpc_pytorch import generate_pytorch_tpc
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tpc_pytorch import generate_pytorch_tpc
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/target_platform_capabilities.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,37 +9,38 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH, LATEST
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH
+from model_compression_toolkit.target_platform_capabilities.constants import LATEST
 
 
 ###############################
 # Build Tensorflow TPC models
 ###############################
 keras_tpc_models_dict = None
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.latest import get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.latest import get_keras_tpc_latest
 
     # Keras: TPC versioning
     keras_tpc_models_dict = {'v1': get_keras_tpc_v1(),
                              LATEST: get_keras_tpc_latest()}
 
 ###############################
 # Build Pytorch TPC models
 ###############################
 pytorch_tpc_models_dict = None
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tpc_pytorch import \
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tpc_pytorch import \
         get_pytorch_tpc as get_pytorch_tpc_v1
-    from model_compression_toolkit.core.tpc_models.qnnpack_tpc.latest import get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.latest import get_pytorch_tpc_latest
 
     # Pytorch: TPC versioning
     pytorch_tpc_models_dict = {'v1': get_pytorch_tpc_v1(),
                                LATEST: get_pytorch_tpc_latest()}
 
 tpc_dict = {TENSORFLOW: keras_tpc_models_dict,
             PYTORCH: pytorch_tpc_models_dict}
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,15 +11,18 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -116,8 +119,11 @@
         # Fusions
         # ------------------- #
         tp.Fusing([conv, batchnorm, relu])
         tp.Fusing([conv, batchnorm])
         tp.Fusing([conv, relu])
         tp.Fusing([linear, relu])
 
+        # Set quantization format to fakely quant
+        generated_tpc.set_quantization_format(QuantizationFormat.FAKELY_QUANT)
+
     return generated_tpc
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,23 +11,23 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import tensorflow as tf
 
 from packaging import version
-from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
 
 if version.parse(tf.__version__) < version.parse("2.6"):
     from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, BatchNormalization, ReLU, \
         Activation
 else:
     from keras.layers import Conv2D, DepthwiseConv2D, Conv2DTranspose, Dense, BatchNormalization, ReLU, Activation
 
-from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,17 +12,17 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 from torch.nn import Conv2d, Linear, BatchNorm2d, ConvTranspose2d, Hardtanh, ReLU, ReLU6
 from torch.nn.functional import relu, relu6, hardtanh
 
-from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.qnnpack_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/target_platform_capabilities.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,39 +8,18 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from model_compression_toolkit.constants import FOUND_TF, FOUND_TORCH
 
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH, TENSORFLOW, PYTORCH, LATEST
+from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tp_model import get_tp_model, generate_tp_model, get_op_quantization_configs
 
-
-###############################
-# Build Tensorflow TPC models
-###############################
-keras_tpc_models_dict = None
 if FOUND_TF:
-    from model_compression_toolkit.core.tpc_models.tflite_tpc.v1.tpc_keras import get_keras_tpc as get_keras_tpc_v1
-    from model_compression_toolkit.core.tpc_models.tflite_tpc.latest import get_keras_tpc_latest
-
-    # Keras: TPC versioning
-    keras_tpc_models_dict = {'v1': get_keras_tpc_v1(),
-                             LATEST: get_keras_tpc_latest()}
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_keras import get_keras_tpc as get_keras_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_keras import generate_keras_tpc
 
-###############################
-# Build Pytorch TPC models
-###############################
-pytorch_tpc_models_dict = None
 if FOUND_TORCH:
-    from model_compression_toolkit.core.tpc_models.tflite_tpc.v1.tpc_pytorch import \
-        get_pytorch_tpc as get_pytorch_tpc_v1
-    from model_compression_toolkit.core.tpc_models.tflite_tpc.latest import get_pytorch_tpc_latest
-
-    # Pytorch: TPC versioning
-    pytorch_tpc_models_dict = {'v1': get_pytorch_tpc_v1(),
-                               LATEST: get_pytorch_tpc_latest()}
-
-tpc_dict = {TENSORFLOW: keras_tpc_models_dict,
-            PYTORCH: pytorch_tpc_models_dict}
-
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_pytorch import get_pytorch_tpc as get_pytorch_tpc_latest
+    from model_compression_toolkit.target_platform_capabilities.tpc_models.default_tpc.v4.tpc_pytorch import generate_pytorch_tpc
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tp_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,16 +11,20 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Tuple
 
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.common.target_platform import OpQuantizationConfig, TargetPlatformModel
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import OpQuantizationConfig, \
+    TargetPlatformModel
+from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import \
+    QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform.quantization_format import \
+    QuantizationFormat
 
 tp = mct.target_platform
 
 
 def get_tp_model() -> TargetPlatformModel:
     """
     A method that generates a default target platform model, with base 8-bit quantization configuration and 8, 4, 2
@@ -61,15 +65,15 @@
         enable_activation_quantization=True,
         quantization_preserving=False,
         fixed_scale=None,
         fixed_zero_point=None,
         weights_multiplier_nbits=None
     )
 
-    mixed_precision_cfg_list = [] # No mixed precision
+    mixed_precision_cfg_list = []  # No mixed precision
 
     return eight_bits, mixed_precision_cfg_list
 
 
 def generate_tp_model(default_config: OpQuantizationConfig,
                       base_config: OpQuantizationConfig,
                       mixed_precision_cfg_list: List[OpQuantizationConfig],
@@ -102,54 +106,58 @@
     # To start defining the model's components (such as operator sets, and fusing patterns),
     # use 'with' the TargetPlatformModel instance, and create them as below:
     with generated_tpc:
         # In TFLite, the quantized operator specifications constraint operators quantization
         # differently. For more details:
         # https://www.tensorflow.org/lite/performance/quantization_spec#int8_quantized_operator_specifications
         tp.OperatorsSet("NoQuantization",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                                  quantization_preserving=True))
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            quantization_preserving=True))
 
         fc = tp.OperatorsSet("FullyConnected",
-                              tp.get_default_quantization_config_options().clone_and_edit(
-                                       weights_per_channel_threshold=False))
+                             tp.get_default_quantization_config_options().clone_and_edit(
+                                 weights_per_channel_threshold=False))
 
         tp.OperatorsSet("L2Normalization",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                                  fixed_zero_point=0, fixed_scale=1 / 128))
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=0, fixed_scale=1 / 128))
         tp.OperatorsSet("LogSoftmax",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                                  fixed_zero_point=127, fixed_scale=16 / 256))
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=127, fixed_scale=16 / 256))
         tp.OperatorsSet("Tanh",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                                  fixed_zero_point=0, fixed_scale=1 / 128))
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=0, fixed_scale=1 / 128))
         tp.OperatorsSet("Softmax",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                                  fixed_zero_point=-128, fixed_scale=1 / 256))
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=-128, fixed_scale=1 / 256))
         tp.OperatorsSet("Logistic",
-                         tp.get_default_quantization_config_options().clone_and_edit(
-                                  fixed_zero_point=-128, fixed_scale=1 / 256))
+                        tp.get_default_quantization_config_options().clone_and_edit(
+                            fixed_zero_point=-128, fixed_scale=1 / 256))
 
         conv2d = tp.OperatorsSet("Conv2d")
         kernel = tp.OperatorSetConcat(conv2d, fc)
 
         relu = tp.OperatorsSet("Relu")
         elu = tp.OperatorsSet("Elu")
         activations_to_fuse = tp.OperatorSetConcat(relu, elu)
 
         batch_norm = tp.OperatorsSet("BatchNorm")
         bias_add = tp.OperatorsSet("BiasAdd")
         add = tp.OperatorsSet("Add")
         squeeze = tp.OperatorsSet("Squeeze",
-                                   qc_options=tp.get_default_quantization_config_options().clone_and_edit(quantization_preserving=True))
+                                  qc_options=tp.get_default_quantization_config_options().clone_and_edit(
+                                      quantization_preserving=True))
         # ------------------- #
         # Fusions
         # ------------------- #
         # Source: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/remapper
         tp.Fusing([kernel, bias_add])
         tp.Fusing([kernel, bias_add, activations_to_fuse])
         tp.Fusing([conv2d, batch_norm, activations_to_fuse])
         tp.Fusing([conv2d, squeeze, activations_to_fuse])
         tp.Fusing([batch_norm, activations_to_fuse])
         tp.Fusing([batch_norm, add, activations_to_fuse])
 
+        # Set quantization format to int8
+        generated_tpc.set_quantization_format(QuantizationFormat.INT8)
+
     return generated_tpc
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tpc_keras.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py`

 * *Files 12% similar despite different names*

```diff
@@ -20,19 +20,19 @@
         DepthwiseConv2D, MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
 else:
     from keras.layers import Conv2D, Dense, Reshape, ZeroPadding2D, AveragePooling2D, Activation, DepthwiseConv2D, \
         MaxPooling2D, ReLU, Add, Softmax, Concatenate, Multiply, Maximum, Minimum, BatchNormalization
 
 from tensorflow.python.keras.layers.core import SlicingOpLambda
 from tensorflow.python.ops.image_ops_impl import ResizeMethod
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.attribute_filter import Eq
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import Eq
 
-from model_compression_toolkit.core.tpc_models.tflite_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.tflite_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_keras_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Keras TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tpc_pytorch.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,19 +11,19 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 from torch.nn import AvgPool2d, MaxPool2d
 from torch.nn.functional import avg_pool2d, max_pool2d, interpolate
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework.attribute_filter import Eq
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework.attribute_filter import Eq
 
-from model_compression_toolkit.core.tpc_models.tflite_tpc.v1.tp_model import get_tp_model
+from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1.tp_model import get_tp_model
 import model_compression_toolkit as mct
-from model_compression_toolkit.core.tpc_models.tflite_tpc.v1 import __version__ as TPC_VERSION
+from model_compression_toolkit.target_platform_capabilities.tpc_models.tflite_tpc.v1 import __version__ as TPC_VERSION
 
 tp = mct.target_platform
 
 
 def get_pytorch_tpc() -> tp.TargetPlatformCapabilities:
     """
     get a Pytorch TargetPlatformCapabilities object with default operation sets to layers mapping.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py`

 * *Files 1% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 # limitations under the License.
 # ==============================================================================
 
 
 from abc import abstractmethod
 from typing import Any, Callable
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 
 
 class Exporter:
     """
     Base class to define API for an Exporter class that exports and saves models.
     At initiation, it gets a model to export. This model must be an exportable model.
     Each exporter needs to implement a method called 'export' which export the model
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,63 +8,64 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from enum import Enum
-from typing import Callable, Dict
-
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import FOUND_TF
-
-
-class KerasExportMode(Enum):
-    FAKELY_QUANT = 0
-
-
-if FOUND_TF:
-    import keras
-    from model_compression_toolkit.exporter.model_wrapper.keras.validate_layer import is_keras_layer_exportable
-    from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_keras_exporter import FakelyQuantKerasExporter
-
-    def keras_export_model(model: keras.models.Model,
-                           save_model_path: str,
-                           is_layer_exportable_fn: Callable = is_keras_layer_exportable,
-                           mode: KerasExportMode = KerasExportMode.FAKELY_QUANT) -> Dict[str, type]:
+import os
+import tempfile
+from typing import Callable
+
+import keras.models
+import tensorflow as tf
+
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_keras_exporter import FakelyQuantKerasExporter
+from model_compression_toolkit.trainable_infrastructure.keras.load_model import keras_load_quantized_model
+
+
+class FakelyQuantTFLiteExporter(FakelyQuantKerasExporter):
+    """
+    Exporter for fakely-quant TFLite models.
+    The exporter expects to receive an exportable model (where each layer's full quantization parameters
+    can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
+    format) and fake-quant layers for the activations.
+    """
+
+    def __init__(self,
+                 model: keras.models.Model,
+                 is_layer_exportable_fn: Callable,
+                 save_model_path: str):
         """
-        Export a Keras quantized model to h5 model.
-        The model will be saved to the path in save_model_path.
-        Mode can be used for different exported files. Currently, keras_export_model
-        supports KerasExportMode.FAKELY_QUANT (where weights and activations are
-        float fakely-quantized values).
 
         Args:
             model: Model to export.
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
-            mode: Mode to export the model according to.
-            save_model_path: Path to save the model.
+            save_model_path: Path to save the exported model.
+        """
+        super().__init__(model,
+                         is_layer_exportable_fn,
+                         save_model_path)
 
-        Returns:
-            Custom objects dictionary needed to load the model.
+        self.exported_model = None
 
+    def export(self):
         """
+        Convert an exportable (fully-quantized) Keras model to a fakely-quant TFLite model
+        (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
 
-        if mode == KerasExportMode.FAKELY_QUANT:
-            exporter = FakelyQuantKerasExporter(model,
-                                                is_layer_exportable_fn,
-                                                save_model_path)
-
-        else:
-            Logger.critical(
-                f'Unsupported mode was used {mode.name} to '
-                f'export Keras model. Please see API for supported modes.')  # pragma: no cover
-
-        exporter.export()
-
-        return exporter.get_custom_objects()
-else:
-    def keras_export_model(*args, **kwargs):
-        Logger.error('Installing tensorflow and tensorflow_model_optimization is mandatory '
-                     'when using keras_export_model. '
-                     'Could not find some or all of TensorFlow packages.')  # pragma: no cover
+        """
+        # Use Keras exporter to quantize model's weights before converting it to TFLite.
+        # Since exporter saves the model, we use a tmp path for saving, and then we delete it.
+        _, tmp_h5_file = tempfile.mkstemp('.h5')
+        custom_objects = FakelyQuantKerasExporter(self.model,
+                                                  self.is_layer_exportable_fn,
+                                                  tmp_h5_file).export()
+
+        model = keras_load_quantized_model(tmp_h5_file)
+        os.remove(tmp_h5_file)
+
+        self.exported_model = tf.lite.TFLiteConverter.from_keras_model(model).convert()
+        Logger.info(f'Exporting FQ tflite model to: {self.save_model_path}')
+        with open(self.save_model_path, 'wb') as f:
+            f.write(self.exported_model)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py`

 * *Files 22% similar despite different names*

```diff
@@ -8,36 +8,68 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+
 from typing import Callable
 
 import torch.nn
 
-from model_compression_toolkit.exporter.model_exporter.fw_agonstic.exporter import Exporter
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
+from model_compression_toolkit.exporter.model_exporter.pytorch.base_pytorch_exporter import BasePyTorchExporter
 
 
-class BasePyTorchExporter(Exporter):
+class FakelyQuantTorchScriptPyTorchExporter(BasePyTorchExporter):
     """
-    Base PyTorch exporter class.
+    Exporter for fakely-quant PyTorch models.
+    The exporter expects to receive an exportable model (where each layer's full quantization parameters
+    can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
+    format) and fake-quant layers for the activations.
     """
 
     def __init__(self,
                  model: torch.nn.Module,
                  is_layer_exportable_fn: Callable,
                  save_model_path: str,
                  repr_dataset: Callable):
         """
+
         Args:
             model: Model to export.
             is_layer_exportable_fn: Callable to check whether a layer can be exported or not.
             save_model_path: Path to save the exported model.
             repr_dataset: Representative dataset (needed for creating torch script).
-
         """
+
         super().__init__(model,
                          is_layer_exportable_fn,
-                         save_model_path)
-        self.repr_dataset = repr_dataset
+                         save_model_path,
+                         repr_dataset)
+
+    def export(self) -> None:
+        """
+        Convert an exportable (fully-quantized) PyTorch model to a fakely-quant model
+        (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
+
+        Returns:
+            Fake-quant PyTorch model.
+        """
+        for layer in self.model.children():
+            self.is_layer_exportable_fn(layer)
+
+        self._substitute_fully_quantized_model()
+
+        torch_traced = torch.jit.trace(self.model,
+                                       to_torch_tensor(next(self.repr_dataset())),
+                                       check_trace=True)
+
+        self.exported_model = torch.jit.script(torch_traced)
+
+        Logger.info(f"Exporting PyTorch torch script Model: {self.save_model_path}")
+
+        torch.jit.save(self.exported_model, self.save_model_path)
+
+
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,25 +12,26 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
 import torch.nn
 
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from model_compression_toolkit.exporter.model_exporter.pytorch.base_pytorch_exporter import BasePyTorchExporter
 from packaging import version
 
 # ONNX opset version 16 is supported from PyTorch 1.12
 if version.parse(torch.__version__) < version.parse("1.12"):
     OPSET_VERSION = 15
 else:
     OPSET_VERSION = 16
 
+
 class FakelyQuantONNXPyTorchExporter(BasePyTorchExporter):
     """
     Exporter for fakely-quant PyTorch models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a fakely-quant model (namely, weights that are in fake-quant
     format) and fake-quant layers for the activations.
     """
@@ -60,20 +61,22 @@
         Convert an exportable (fully-quantized) PyTorch model to a fakely-quant model
         (namely, weights that are in fake-quant format) and fake-quant layers for the activations.
 
         Returns:
             Fake-quant PyTorch model.
         """
         for layer in self.model.children():
-            assert self.is_layer_exportable_fn(layer), f'Layer {layer.name} is not exportable.'
+            self.is_layer_exportable_fn(layer)
 
-        model_input = to_torch_tensor(next(self.repr_dataset())[0])
+        self._substitute_fully_quantized_model()
 
         Logger.info(f"Exporting PyTorch fake quant onnx model: {self.save_model_path}")
 
+        model_input = to_torch_tensor(next(self.repr_dataset())[0])
+
         torch.onnx.export(self.model,
                           model_input,
                           self.save_model_path,
                           opset_version=OPSET_VERSION,
                           verbose=False,
                           input_names=['input'],
                           output_names=['output'],
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_exporter/tflite/int8_tflite_exporter.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py`

 * *Files 7% similar despite different names*

```diff
@@ -18,18 +18,17 @@
 import keras.models
 import numpy as np
 import tensorflow as tf
 from keras import Sequential
 from keras.layers import Dense, Conv2D, Reshape
 from keras.models import clone_model
 
-from model_compression_toolkit import quantizers_infrastructure as qi
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.exporter.model_exporter.keras.fakely_quant_keras_exporter import FakelyQuantKerasExporter
-from model_compression_toolkit.quantizers_infrastructure.keras.inferable_quantizers import constants as keras_inferable_constants
+from mct_quantizers import constants as keras_inferable_constants, KerasQuantizationWrapper
 
 BIAS_INITIALIZER = 'bias_initializer'
 BIAS_REGULARIZER = 'bias_regularizer'
 BIAS_CONSTRAINT = 'bias_constraint'
 ACTIVITY_REGULARIZER = 'activity_regularizer'
 KERNEL_INITIALIZER = 'kernel_initializer'
 KERNEL_REGULARIZER = 'kernel_regularizer'
@@ -45,14 +44,15 @@
 UNITS = 'units'
 PAD_VALID = 'valid'
 KERNEL = 'kernel'
 
 CONV_KERNEL_CHANNEL_AXIS = 3
 CONV_INPUT_CHANNELS_DIM = 4
 
+
 class INT8TFLiteExporter(FakelyQuantKerasExporter):
     """
     Exporter for INT8 TFLite models.
     The exporter expects to receive an exportable model (where each layer's full quantization parameters
     can be retrieved), and convert it into a quantized model where weights and activations are represented
     as integer data type.
     """
@@ -70,15 +70,15 @@
         """
         super().__init__(model,
                          is_layer_exportable_fn,
                          save_model_path)
 
         self.exported_model = None
 
-    def _get_pointwise_layer_to_replace_dense(self, wrapped_layer: qi.KerasQuantizationWrapper) -> keras.layers.Layer:
+    def _get_pointwise_layer_to_replace_dense(self, wrapped_layer: KerasQuantizationWrapper) -> keras.layers.Layer:
         # First we create a pointwise configuration based on the Dense layer's configuration
         dense_cfg = wrapped_layer.layer.get_config()
 
         # List of pw attributes that should be taken from the dense layer as they are.
         pw_attr_list = [LAYER_NAME, ACTIVATION, USE_BIAS, BIAS_CONSTRAINT,
                         BIAS_INITIALIZER, BIAS_REGULARIZER, TRAINABLE, ACTIVITY_REGULARIZER,
                         KERNEL_INITIALIZER, KERNEL_REGULARIZER, KERNEL_CONSTRAINT]
@@ -89,15 +89,15 @@
         pw_cfg.update({KERNEL_SIZE: (1, 1),
                        STRIDES: (1, 1),
                        PADDING: PAD_VALID,
                        FILTERS: dense_cfg[UNITS]})
 
         # Create the point-wise layer
         pw_layer = Conv2D(**pw_cfg)
-        pw_layer.build(wrapped_layer.layer.input_shape)
+        pw_layer.build(wrapped_layer.input_shape)
 
         # Create and set the point-wise weights to assign
         dense_kernel = wrapped_layer.layer.kernel
         pw_weights = []
         pw_kernel = np.reshape(wrapped_layer.get_weights()[0],
                                (1, 1, dense_kernel.get_shape()[0], dense_cfg[UNITS]))
 
@@ -105,68 +105,70 @@
         if wrapped_layer.layer.use_bias:
             pw_bias = wrapped_layer.get_weights()[2]
             pw_weights.append(pw_bias)
 
         pw_layer.set_weights(pw_weights)
 
         # Now that we have the point-wise to replace the dense layer,
-        # we need to wrap it using qi.KerasQuantizationWrapper with a new
+        # we need to wrap it using KerasQuantizationWrapper with a new
         # relevant quantizers.
         # Create new kernel quantizer
         pw_kernel_quantizer_cfg = wrapped_layer.weights_quantizers[KERNEL].get_config()
 
         # In Conv2D channel axis is 3 and not 1 as in Dense
         pw_kernel_quantizer_cfg[keras_inferable_constants.CHANNEL_AXIS] = CONV_KERNEL_CHANNEL_AXIS
 
         # Unquantized weight to conv layer has 4 dimensions (unlike dense which varies)
         pw_kernel_quantizer_cfg[keras_inferable_constants.INPUT_RANK] = CONV_INPUT_CHANNELS_DIM
 
-        assert isinstance(pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD], np.ndarray), f'Expected to find threshold which is a numpy array, but found: {type(pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD])}'
-        pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD] = list(pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD])
+        assert isinstance(pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD],
+                          list), f'Expected to find threshold which is a list, but found: {type(pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD])}'
+        pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD] = list(
+            pw_kernel_quantizer_cfg[keras_inferable_constants.THRESHOLD])
 
         # Now that we have the point-wise quantizer we can instantiate it
         quantizer_class = type(wrapped_layer.weights_quantizers[KERNEL])
         pw_quantizer = quantizer_class(**pw_kernel_quantizer_cfg)
         pw_weights_quantizers = copy.deepcopy(wrapped_layer.weights_quantizers)
         pw_weights_quantizers[KERNEL] = pw_quantizer
 
         # Wrap pw with the new quantizers (the activation is not affected thus we take the Dense quantizers)
-        wrapped_pw = qi.KerasQuantizationWrapper(pw_layer,
-                                                 pw_weights_quantizers,
-                                                 wrapped_layer.activation_quantizers)
+        wrapped_pw = KerasQuantizationWrapper(pw_layer,
+                                              pw_weights_quantizers)
 
         # Compute the shape that the input to the new layer should be reshaped into
         # Example: Dense kernel with the following shape (3, 20) expects to have input with the
         # next dimensions (BATCH_SIZE, x0, x1, ..., xn, 20).
         # Conv layer expects 4-rank input. Thus, the input is reshaped to (BATCH_SIZE, 1, x0*x1*...*xn, 20)
-        dim = wrapped_layer.layer.input_shape[1:-1]
+        dim = wrapped_layer.input_shape[1:-1]
         target_shape = (1, int(np.prod(dim))) + (dense_kernel.get_shape()[0],)
 
         return Sequential([
             Reshape(target_shape=target_shape),
             wrapped_pw,
-            Reshape(wrapped_layer.layer.output_shape[1:])
+            Reshape(wrapped_layer.output_shape[1:])
         ])
 
     def export(self) -> None:
         """
         Export a fully quantized model to its int8 tflite model.
         """
 
-        def _substitute_model(wrapped_layer: qi.KerasQuantizationWrapper) -> keras.layers.Layer:
+        def _substitute_model(layer_to_substitue: keras.layers.Layer) -> keras.layers.Layer:
             assert self.is_layer_exportable_fn(
-                wrapped_layer), f'Layer {wrapped_layer.get_config()} did not pass validation'
+                layer_to_substitue), f'Layer {layer_to_substitue.get_config()} did not pass validation'
 
             # In order to support dense quantization using per-channel quantization (which is
             # unsupported in TFLITE int models) we substitute each dense layer to its equivalent
             # point-wise convolution.
-            if isinstance(wrapped_layer.layer, Dense):
-                return self._get_pointwise_layer_to_replace_dense(wrapped_layer)
+            if isinstance(layer_to_substitue, KerasQuantizationWrapper):
+                if isinstance(layer_to_substitue.layer, Dense):
+                    return self._get_pointwise_layer_to_replace_dense(layer_to_substitue)
 
-            return wrapped_layer
+            return layer_to_substitue
 
         # Transform the model to a new model that can be converted to int8 models.
         # For example: replace dense layers with point-wise layers (to support per-channel quantization)
         self.transformed_model = clone_model(self.model,
                                              clone_function=_substitute_model)
 
         # Convert model to int8 representation
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -9,16 +9,12 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.core.common.constants import FOUND_TF, FOUND_TORCH
+from model_compression_toolkit.exporter.model_wrapper.keras.validate_layer import is_keras_layer_exportable
+from model_compression_toolkit.exporter.model_wrapper.keras.builder.fully_quantized_model_builder import get_exportable_keras_model
 
-if FOUND_TF:
-    from model_compression_toolkit.exporter.model_wrapper.keras.validate_layer import is_keras_layer_exportable
-    from model_compression_toolkit.exporter.model_wrapper.keras.builder.fully_quantized_model_builder import get_exportable_keras_model
-
-if FOUND_TORCH:
-    from model_compression_toolkit.exporter.model_wrapper.pytorch.validate_layer import is_pytorch_layer_exportable
-    from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
+from model_compression_toolkit.exporter.model_wrapper.pytorch.validate_layer import is_pytorch_layer_exportable
+from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/common/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/legacy/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/common/exporter_get_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py`

 * *Files 27% similar despite different names*

```diff
@@ -8,45 +8,56 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.quantizers_infrastructure import QuantizationTarget
-from model_compression_toolkit.quantizers_infrastructure.common.constants import QUANTIZATION_TARGET, \
-    QUANTIZATION_METHOD
-from model_compression_toolkit.quantizers_infrastructure.common.get_all_subclasses import get_all_subclasses
+from typing import Union, Any
 
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from mct_quantizers.common.constants \
+    import QUANTIZATION_TARGET, QUANTIZATION_METHOD, QUANTIZER_TYPE
+from mct_quantizers.common.get_all_subclasses \
+    import get_all_subclasses
 
-def get_quantizer_class(quant_target: QuantizationTarget,
-                        quant_method: QuantizationMethod,
-                        quantizer_base_class: type) -> type:
+
+def get_trainable_quantizer_class(quant_target: QuantizationTarget,
+                                  quantizer_type: Union[Any, Any],
+                                  quant_method: QuantizationMethod,
+                                  quantizer_base_class: type) -> type:
     """
-    Searches for a quantizer class that matches the requested QuantizationTarget and QuantizationMethod.
-    Exactly one class should be found.
+    Searches for a trainable quantizer class that matches the requested QuantizationTarget and QuantizationMethod and
+    a task dedicated quantizer type. Exactly one class should be found.
 
     Args:
-        quant_target: QuantizationTarget value (Weights or Activation) which indicates what is the target for
-            quantization to use the quantizer for.
+        quant_target: QuantizationTarget value which indicates what is the target for quantization to
+            use the quantizer for.
+        quantizer_type: The type of the quantizer (quantization technique).
+            This can differ, depending on the purpose the quantizer is for.
         quant_method: A list of QuantizationMethod values to indicate all type of quantization methods that the
             quantizer supports.
         quantizer_base_class: A type of quantizer that the requested quantizer should inherit from.
 
-    Returns: A class of a quantizer that inherits from BaseKerasInferableQuantizer.
+    Returns: A class of a quantizer that inherits from BaseKerasQATTrainableQuantizer.
 
     """
     qat_quantizer_classes = get_all_subclasses(quantizer_base_class)
-    filtered_quantizers = list(filter(lambda q_class: getattr(q_class, QUANTIZATION_TARGET) == quant_target and
-                                                      getattr(q_class, QUANTIZATION_METHOD) is not None and
-                                                       quant_method in getattr(q_class, QUANTIZATION_METHOD),
+    if len(qat_quantizer_classes) == 0:
+        Logger.error(f"No quantizers were found that inherit from {quantizer_base_class}.")  # pragma: no cover
+
+    filtered_quantizers = list(filter(lambda q_class: getattr(q_class, QUANTIZATION_TARGET, None) is not None and
+                                                      getattr(q_class, QUANTIZATION_TARGET) == quant_target and
+                                                      getattr(q_class, QUANTIZATION_METHOD, None) is not None and
+                                                       quant_method in getattr(q_class, QUANTIZATION_METHOD, []) and
+                                                      getattr(q_class, QUANTIZER_TYPE, None) == quantizer_type,
                                       qat_quantizer_classes))
 
     if len(filtered_quantizers) != 1:
-        Logger.error(f"Found {len(filtered_quantizers)} quantizer for target {quant_target.value} "
-                     f"that matches the requested quantization method {quant_method.name} "
-                     f"but there should be exactly one."
+        Logger.error(f"Found {len(filtered_quantizers)} quantizer for target {quant_target.value} "  # pragma: no cover
+                     f"that matches the requested quantization method {quant_method.name} and "
+                     f"quantizer type {quantizer_type.value} but there should be exactly one."
                      f"The possible quantizers that were found are {filtered_quantizers}.")
 
     return filtered_quantizers[0]
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py`

 * *Files 22% similar despite different names*

```diff
@@ -10,127 +10,136 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Dict, Any
 
-from model_compression_toolkit.core.common import BaseNode, Logger
-from model_compression_toolkit.core.common.constants import THRESHOLD, RANGE_MIN, RANGE_MAX, SIGNED
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.exporter.model_wrapper.common.exporter_get_quantizer import get_quantizer_class
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import QuantizationTarget
-from model_compression_toolkit.quantizers_infrastructure.keras.inferable_quantizers.base_keras_inferable_quantizer \
-    import \
-    BaseKerasInferableQuantizer
-import numpy as np
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.constants import THRESHOLD, RANGE_MIN, RANGE_MAX, SIGNED, CLUSTER_CENTERS, SCALE_PER_CHANNEL
 
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
+from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+from mct_quantizers import constants as qi_keras_consts
 
 def get_inferable_quantizer_kwargs(node: BaseNode,
                                    quantization_target: QuantizationTarget) -> Dict[str, Any]:
     """
     Get the quantization parameters for an inferable quantizer.
     Args:
         node: The node for which the quantizer is being created.
         quantization_target: The target of the quantization (weights or activations).
-
     Returns:
         The quantization parameters as a dictionary.
-
     """
 
     if quantization_target == QuantizationTarget.Weights:
         # Get the weights quantization configuration for the node
         node_w_qc = node.final_weights_quantization_cfg
         quantization_method = node_w_qc.weights_quantization_method
 
         # Return the appropriate quantization parameters based on the quantization method
         if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                    QuantizationMethod.SYMMETRIC]:
-            return {'num_bits': node_w_qc.weights_n_bits,
-                    'threshold': list(node_w_qc.weights_quantization_params[THRESHOLD].flatten()),
-                    'per_channel': node_w_qc.weights_per_channel_threshold,
-                    'channel_axis': node_w_qc.weights_channels_axis,
-                    'input_rank': len(node_w_qc.weights_quantization_params[THRESHOLD].shape)}
+            return {qi_keras_consts.NUM_BITS: node_w_qc.weights_n_bits,
+                    qi_keras_consts.THRESHOLD: list(node_w_qc.weights_quantization_params[THRESHOLD].flatten()),
+                    qi_keras_consts.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
+                    qi_keras_consts.CHANNEL_AXIS: node_w_qc.weights_channels_axis,
+                    qi_keras_consts.INPUT_RANK: len(node_w_qc.weights_quantization_params[THRESHOLD].shape)}
 
         elif quantization_method in [QuantizationMethod.UNIFORM]:
-            return {'num_bits': node_w_qc.weights_n_bits,
-                    'per_channel': node_w_qc.weights_per_channel_threshold,
-                    'min_range': list(node_w_qc.weights_quantization_params[RANGE_MIN].flatten()),
-                    'max_range': list(node_w_qc.weights_quantization_params[RANGE_MAX].flatten()),
-                    'channel_axis': node_w_qc.weights_channels_axis,
-                    'input_rank': len(node_w_qc.weights_quantization_params[THRESHOLD].shape)}
+            return {qi_keras_consts.NUM_BITS: node_w_qc.weights_n_bits,
+                    qi_keras_consts.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
+                    qi_keras_consts.MIN_RANGE: list(node_w_qc.weights_quantization_params[RANGE_MIN].flatten()),
+                    qi_keras_consts.MAX_RANGE: list(node_w_qc.weights_quantization_params[RANGE_MAX].flatten()),
+                    qi_keras_consts.CHANNEL_AXIS: node_w_qc.weights_channels_axis,
+                    qi_keras_consts.INPUT_RANK: len(node_w_qc.weights_quantization_params[RANGE_MIN].shape)}
+
+        elif quantization_method in [QuantizationMethod.LUT_SYM_QUANTIZER, QuantizationMethod.LUT_POT_QUANTIZER]:
+            return {qi_keras_consts.NUM_BITS: node_w_qc.weights_n_bits,
+                    qi_keras_consts.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
+                    qi_keras_consts.CLUSTER_CENTERS: node_w_qc.weights_quantization_params[CLUSTER_CENTERS],
+                    qi_keras_consts.THRESHOLD: list(node_w_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten()),
+                    qi_keras_consts.CHANNEL_AXIS: node_w_qc.weights_channels_axis,
+                    # TODO: how to pass multiplier nbits and eps for a specific node?
+                    qi_keras_consts.INPUT_RANK: len(node_w_qc.weights_quantization_params[SCALE_PER_CHANNEL].shape)}
+
         else:
             Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
 
     elif quantization_target == QuantizationTarget.Activation:
         # Get the activation quantization configuration for the node
         node_qc = node.final_activation_quantization_cfg
         quantization_method = node_qc.activation_quantization_method
 
         # Return the appropriate quantization parameters based on the quantization method
         if quantization_method in [QuantizationMethod.POWER_OF_TWO,
                                    QuantizationMethod.SYMMETRIC]:
-            return {'num_bits': node_qc.activation_n_bits,
+            return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
                     # In activation quantization is per-tensor only - thus we hold the threshold as a list with a len of 1
-                    'threshold': [node_qc.activation_quantization_params[THRESHOLD]],
-                    'signed': node_qc.activation_quantization_params[SIGNED]}
+                    qi_keras_consts.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]],
+                    qi_keras_consts.SIGNED: node_qc.activation_quantization_params[SIGNED]}
 
         elif quantization_method in [QuantizationMethod.UNIFORM]:
-            return {'num_bits': node_qc.activation_n_bits,
+            return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
                     # In activation quantization is per-tensor only - thus we hold the min/max as a list with a len of 1
-                    'min_range': [node_qc.activation_quantization_params[RANGE_MIN]],
-                    'max_range': [node_qc.activation_quantization_params[RANGE_MAX]]}
+                    qi_keras_consts.MIN_RANGE: [node_qc.activation_quantization_params[RANGE_MIN]],
+                    qi_keras_consts.MAX_RANGE: [node_qc.activation_quantization_params[RANGE_MAX]]}
+
+        elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER]:
+            return {qi_keras_consts.NUM_BITS: node_qc.activation_n_bits,
+                    qi_keras_consts.SIGNED: node_qc.activation_quantization_params[SIGNED],
+                    qi_keras_consts.CLUSTER_CENTERS: node_qc.activation_quantization_params[CLUSTER_CENTERS],
+                    qi_keras_consts.THRESHOLD: [node_qc.activation_quantization_params[THRESHOLD]]
+                    # TODO: how to pass multiplier nbits and eps for a specific node?
+                    }
         else:
             Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
     else:
         Logger.critical(f'{quantization_target} is not supported')  # pragma: no cover
 
 
 def get_weights_quantizer_for_node(node: BaseNode) -> BaseKerasInferableQuantizer:
     """
     Get weights quantizer for a node.
-
     Args:
         node: Node to create a weight quantizer for.
-
     Returns:
         Quantizer for the node's weights.
-
     """
     if node.final_weights_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final weights quantization configuration')  # pragma:
         # no cover
     node_w_qc = node.final_weights_quantization_cfg
     weights_quantization_method = node_w_qc.weights_quantization_method
 
-    quantier_for_node = get_quantizer_class(QuantizationTarget.Weights,
-                                            weights_quantization_method,
-                                            BaseKerasInferableQuantizer)
+    quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
+                                                      weights_quantization_method,
+                                                      BaseKerasInferableQuantizer)
     kwargs = get_inferable_quantizer_kwargs(node, QuantizationTarget.Weights)
 
     return quantier_for_node(**kwargs)
 
 
 def get_activations_quantizer_for_node(node: BaseNode) -> BaseKerasInferableQuantizer:
     """
     Get activation quantizer for a node.
-
     Args:
         node: Node to create an activation quantizer for.
-
     Returns:
         Quantizer for the node's activations.
-
     """
     if node.final_activation_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final activation quantization configuration')  #
         # pragma: no cover
     node_act_qc = node.final_activation_quantization_cfg
     activation_quantization_method = node_act_qc.activation_quantization_method
 
-    quantier_for_node = get_quantizer_class(QuantizationTarget.Activation,
-                                            activation_quantization_method,
-                                            BaseKerasInferableQuantizer)
+    quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
+                                                      activation_quantization_method,
+                                                      BaseKerasInferableQuantizer)
     kwargs = get_inferable_quantizer_kwargs(node, QuantizationTarget.Activation)
 
-    return quantier_for_node(**kwargs)
+    return quantier_for_node(**kwargs)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py`

 * *Files 12% similar despite different names*

```diff
@@ -10,65 +10,65 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Any
 
-from keras.engine.input_layer import InputLayer
+from mct_quantizers import BaseInferableQuantizer, KerasActivationQuantizationHolder
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.logger import Logger
+
+if FOUND_TF:
+    from keras.engine.base_layer import Layer
+    from keras.engine.input_layer import InputLayer
+    from mct_quantizers import KerasQuantizationWrapper
 
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.quantizers_infrastructure import KerasQuantizationWrapper
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import BaseInferableQuantizer
-
-
-
-def is_keras_layer_exportable(layer: Any) -> bool:
-    """
-    Check whether a Keras layer is a valid exportable layer or not.
-
-    Args:
-        layer: Keras layer to check if considered to be valid for exporting.
-
-    Returns:
+    def is_keras_layer_exportable(layer: Any) -> bool:
+        """
         Check whether a Keras layer is a valid exportable layer or not.
-    """
-    # Keras Input layers are not wrapped
-    if isinstance(layer, InputLayer):
-        return True
-
-    valid_layer = isinstance(layer, KerasQuantizationWrapper)
-    if not valid_layer:
-        Logger.error(
-            f'Exportable layer must be wrapped using KerasQuantizationWrapper, but layer {layer.name} is of type '
-            f'{type(layer)}') # pragma: no cover
-
-    valid_weights_quantizers = isinstance(layer.weights_quantizers, dict)
-    if not valid_weights_quantizers:
-        Logger.error(
-            f'KerasQuantizationWrapper must have a weights_quantizers but has a '
-            f'{type(layer.weights_quantizers)} object') # pragma: no cover
 
-    for _, weights_quantizer in layer.weights_quantizers.items():
-        if not isinstance(weights_quantizer, BaseInferableQuantizer):
-            Logger.error(
-                f'weights_quantizer must be a BaseInferableQuantizer object but has a '
-                f'{type(weights_quantizer)} object')  # pragma: no cover
+        Args:
+            layer: Keras layer to check if considered to be valid for exporting.
 
-    valid_activation_quantizers = isinstance(layer.activation_quantizers, list)
-    if not valid_activation_quantizers:
-        Logger.error(
-            f'KerasQuantizationWrapper must have a activation_quantizers list but has a '
-            f'{type(layer.activation_quantizers)} object') # pragma: no cover
+        Returns:
+            Check whether a Keras layer is a valid exportable layer or not.
+        """
+        # Keras Input layers are not wrapped
+        if isinstance(layer, InputLayer):
+            return True
 
-    for activation_quantizers in layer.activation_quantizers:
-        if not isinstance(activation_quantizers, BaseInferableQuantizer):
+        valid_layer = isinstance(layer, Layer)
+        if not valid_layer:
             Logger.error(
-                f'activation_quantizers must be a BaseInferableQuantizer object but has a '
-                f'{type(activation_quantizers)} object')  # pragma: no cover
+                f'Exportable layer must be a Keras layer, but layer {layer.name} is of type '
+                f'{type(layer)}') # pragma: no cover
 
-    quantizers = layer.activation_quantizers + list(layer.weights_quantizers.values())
-    is_valid_quantizers = all([isinstance(x, BaseInferableQuantizer) for x in quantizers])
-    if not is_valid_quantizers:
-        Logger.error(f'Found a quantizer that is not of type BaseInferableQuantizer') # pragma: no cover
+        if isinstance(layer, KerasQuantizationWrapper):
+            valid_weights_quantizers = isinstance(layer.weights_quantizers, dict)
+            if not valid_weights_quantizers:
+                Logger.error(
+                    f'KerasQuantizationWrapper must have a weights_quantizers but has a '
+                    f'{type(layer.weights_quantizers)} object') # pragma: no cover
+
+            if len(layer.weights_quantizers) == 0:
+                Logger.error(f'KerasQuantizationWrapper must have at least one weight quantizer, but found {len(layer.weights_quantizers)} quantizers. If layer is not quantized it should be a Keras layer.')
+
+            for _, weights_quantizer in layer.weights_quantizers.items():
+                if not isinstance(weights_quantizer, BaseInferableQuantizer):
+                    Logger.error(
+                        f'weights_quantizer must be a BaseInferableQuantizer object but has a '
+                        f'{type(weights_quantizer)} object')  # pragma: no cover
+
+        if isinstance(layer, KerasActivationQuantizationHolder):
+            if not isinstance(layer.activation_holder_quantizer, BaseInferableQuantizer):
+                Logger.error(
+                    f'activation quantizer in KerasActivationQuantizationHolder'
+                    f' must be a BaseInferableQuantizer object but has a '
+                    f'{type(layer.activation_holder_quantizer)} object')  # pragma: no cover
 
-    return True
+        return True
+else:
+    def is_keras_layer_exportable(*args, **kwargs):  # pragma: no cover
+        Logger.error('Installing tensorflow and tensorflow_model_optimization is mandatory '
+                     'when using is_keras_layer_exportable. '
+                     'Could not find Tensorflow package.')
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py`

 * *Files 13% similar despite different names*

```diff
@@ -11,21 +11,24 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Dict, Any
 
-from model_compression_toolkit.core.common import BaseNode, Logger
-from model_compression_toolkit.core.common.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.exporter.model_wrapper.common.exporter_get_quantizer import get_quantizer_class
-from model_compression_toolkit.quantizers_infrastructure import pytorch_inferable_quantizers, QuantizationTarget, \
-    BasePyTorchInferableQuantizer
-from model_compression_toolkit.quantizers_infrastructure.pytorch.inferable_quantizers import constants as qi_inferable_quantizers_constants
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.constants import THRESHOLD, SIGNED, RANGE_MIN, RANGE_MAX, \
+    SCALE_PER_CHANNEL, CLUSTER_CENTERS
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
+from mct_quantizers import \
+    constants as qi_inferable_quantizers_constants
+from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
 import numpy as np
 
 
 def get_weights_inferable_quantizer_kwargs(node: BaseNode) -> Dict[str, Any]:
     # Get the weights quantization configuration for the node
     node_w_qc = node.final_weights_quantization_cfg
     quantization_method = node_w_qc.weights_quantization_method
@@ -40,14 +43,23 @@
 
     elif quantization_method in [QuantizationMethod.UNIFORM]:
         return {qi_inferable_quantizers_constants.NUM_BITS: node_w_qc.weights_n_bits,
                 qi_inferable_quantizers_constants.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
                 qi_inferable_quantizers_constants.MIN_RANGE: node_w_qc.weights_quantization_params[RANGE_MIN].flatten(),
                 qi_inferable_quantizers_constants.MAX_RANGE: node_w_qc.weights_quantization_params[RANGE_MAX].flatten(),
                 qi_inferable_quantizers_constants.CHANNEL_AXIS: node_w_qc.weights_channels_axis}
+
+    elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER, QuantizationMethod.LUT_SYM_QUANTIZER]:
+        return {qi_inferable_quantizers_constants.NUM_BITS: node_w_qc.weights_n_bits,
+                qi_inferable_quantizers_constants.CLUSTER_CENTERS: node_w_qc.weights_quantization_params[CLUSTER_CENTERS].flatten(),
+                qi_inferable_quantizers_constants.THRESHOLD: node_w_qc.weights_quantization_params[SCALE_PER_CHANNEL].flatten(),
+                qi_inferable_quantizers_constants.PER_CHANNEL: node_w_qc.weights_per_channel_threshold,
+                qi_inferable_quantizers_constants.CHANNEL_AXIS: node_w_qc.weights_channels_axis}
+                # TODO: Add MULTIPLIER_N_BITS & EPS to node quantization config
+
     else:
         Logger.critical(f'Not supported quantization method for weights inferable quantizers.')  # pragma: no cover
 
 
 def get_activation_inferable_quantizer_kwargs(node: BaseNode) -> Dict[str, Any]:
     # Get the activation quantization configuration for the node
     node_qc = node.final_activation_quantization_cfg
@@ -60,19 +72,28 @@
                 qi_inferable_quantizers_constants.THRESHOLD: np.asarray([node_qc.activation_quantization_params[THRESHOLD]]),
                 qi_inferable_quantizers_constants.SIGNED: node_qc.activation_quantization_params.get(SIGNED)}
 
     elif quantization_method in [QuantizationMethod.UNIFORM]:
         return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.activation_n_bits,
                 qi_inferable_quantizers_constants.MIN_RANGE: np.asarray([node_qc.activation_quantization_params[RANGE_MIN]]),
                 qi_inferable_quantizers_constants.MAX_RANGE: np.asarray([node_qc.activation_quantization_params[RANGE_MAX]])}
+
+    elif quantization_method in [QuantizationMethod.LUT_POT_QUANTIZER]:
+        return {qi_inferable_quantizers_constants.NUM_BITS: node_qc.activation_n_bits,
+                qi_inferable_quantizers_constants.CLUSTER_CENTERS: np.asarray(
+                    [node_qc.activation_quantization_params[CLUSTER_CENTERS]]),
+                qi_inferable_quantizers_constants.THRESHOLD: np.asarray(
+                    [node_qc.activation_quantization_params[THRESHOLD]]),
+                qi_inferable_quantizers_constants.SIGNED: node_qc.activation_quantization_params.get(SIGNED)}
+        # TODO: Add MULTIPLIER_N_BITS & EPS to node quantization config
     else:
         Logger.critical(f'Not supported quantization method for inferable quantizers.')  # pragma: no cover
 
 
-def get_weights_quantizer_for_node(node: BaseNode) -> pytorch_inferable_quantizers.BasePyTorchInferableQuantizer:
+def get_weights_quantizer_for_node(node: BaseNode) -> BasePyTorchInferableQuantizer:
     """
     Get weights quantizer for a node.
 
     Args:
         node: Node to create a weight quantizer for.
 
     Returns:
@@ -81,23 +102,23 @@
     """
     if node.final_weights_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final weights quantization configuration')  # pragma:
         # no cover
     node_w_qc = node.final_weights_quantization_cfg
     weights_quantization_method = node_w_qc.weights_quantization_method
 
-    quantier_for_node = get_quantizer_class(QuantizationTarget.Weights,
-                                            weights_quantization_method,
-                                            BasePyTorchInferableQuantizer)
+    quantier_for_node = get_inferable_quantizer_class(QuantizationTarget.Weights,
+                                                      weights_quantization_method,
+                                                      BasePyTorchInferableQuantizer)
     kwargs = get_weights_inferable_quantizer_kwargs(node)
 
     return quantier_for_node(**kwargs)
 
 
-def get_activations_quantizer_for_node(node: BaseNode) -> pytorch_inferable_quantizers.BasePyTorchInferableQuantizer:
+def get_activations_quantizer_for_node(node: BaseNode) -> BasePyTorchInferableQuantizer:
     """
     Get activation quantizer for a node.
 
     Args:
         node: Node to create an activation quantizer for.
 
     Returns:
@@ -106,14 +127,14 @@
     """
     if node.final_activation_quantization_cfg is None:
         Logger.critical(f'Can not set quantizer for a node with no final activation quantization configuration')  #
         # pragma: no cover
     node_act_qc = node.final_activation_quantization_cfg
     activation_quantization_method = node_act_qc.activation_quantization_method
 
-    quantier_for_node = get_quantizer_class(QuantizationTarget.Activation,
-                                            activation_quantization_method,
-                                            BasePyTorchInferableQuantizer)
+    quantizer_for_node = get_inferable_quantizer_class(QuantizationTarget.Activation,
+                                                       activation_quantization_method,
+                                                       BasePyTorchInferableQuantizer)
     kwargs = get_activation_inferable_quantizer_kwargs(node)
 
-    return quantier_for_node(**kwargs)
+    return quantizer_for_node(**kwargs)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_config.py`

 * *Files 20% similar despite different names*

```diff
@@ -12,193 +12,166 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from enum import Enum
 from typing import Callable, Any, Dict
 from model_compression_toolkit.core.common.defaultdict import DefaultDict
 from model_compression_toolkit.core import common
-from model_compression_toolkit.gptq.common.gptq_quantizer_config import GPTQQuantizerConfig, SoftQuantizerConfig
+from model_compression_toolkit.gptq.common.gptq_constants import QUANT_PARAM_LEARNING_STR, MAX_LSB_STR, REG_DEFAULT
 
 
 class RoundingType(Enum):
     """
     An enum for choosing the GPTQ rounding methods
     0. STRAIGHT-THROUGH ESTIMATOR
     1. SoftQuantizer
     """
     STE = 0
     SoftQuantizer = 1
 
 
+class GPTQHessianWeightsConfig:
+    """
+    Configuration to use for computing the Hessian-based weights for GPTQ loss metric.
+    """
+
+    def __init__(self,
+                 hessians_num_samples: int = 16,
+                 norm_weights: bool = True,
+                 log_norm: bool = True,
+                 scale_log_norm: bool = False,
+                 hessians_n_iter: int = 50):
+
+        """
+        Initialize a GPTQHessianWeightsConfig.
+
+        Args:
+            hessians_num_samples (int): Number of samples to use for computing the Hessian-based weights.
+            norm_weights (bool): Whether to normalize the returned weights (to get values between 0 and 1).
+            log_norm (bool): Whether to use log normalization to the GPTQ Hessian-based weights.
+            scale_log_norm (bool): Whether to scale the final vector of the Hessian weights.
+            hessians_n_iter (int): Number of random iterations to run Hessian approximation for GPTQ weights.
+        """
+
+        self.hessians_num_samples = hessians_num_samples
+        self.norm_weights = norm_weights
+        self.log_norm = log_norm
+        self.scale_log_norm = scale_log_norm
+        self.hessians_n_iter = hessians_n_iter
+
+
 class GradientPTQConfig:
     """
     Configuration to use for quantization with GradientPTQ (experimental).
     """
 
-    def __init__(self,
-                 n_iter: int,
+    def __init__(self, n_iter: int,
                  optimizer: Any,
                  optimizer_rest: Any = None,
                  loss: Callable = None,
                  log_function: Callable = None,
                  train_bias: bool = True,
-                 quantization_parameters_learning: bool = False,
                  rounding_type: RoundingType = RoundingType.SoftQuantizer,
-                 lsb_change_per_bit_width: dict = DefaultDict({}, lambda: 1),
-                 eps: float = 1e-6,
-                 use_jac_based_weights: bool = True,
-                 num_samples_for_loss: int = 16,
-                 norm_weights: bool = False,
+                 use_hessian_based_weights: bool = True,
                  optimizer_quantization_parameter: Any = None,
                  optimizer_bias: Any = None,
-                 log_norm: bool = True,
-                 weights_n_iter: int = 50,
-                 quantizer_config: GPTQQuantizerConfig = SoftQuantizerConfig()):
+                 regularization_factor: float = REG_DEFAULT,
+                 hessian_weights_config: GPTQHessianWeightsConfig = GPTQHessianWeightsConfig(),
+                 gptq_quantizer_params_override: Dict[str, Any] = None):
         """
         Initialize a GradientPTQConfig.
 
         Args:
             n_iter (int): Number of iterations to train.
             optimizer (Any): Optimizer to use.
             optimizer_rest (Any): Optimizer to use for bias and quantizer parameters.
             loss (Callable): The loss to use. should accept 6 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors,
              the 3rd is a list of quantized weights, the 4th is a list of float weights, the 5th and 6th lists are the mean and std of the tensors
              accordingly. see example in multiple_tensors_mse_loss
             log_function (Callable): Function to log information about the GPTQ process.
             train_bias (bool): Whether to update the bias during the training or not.
-            quantization_parameters_learning (bool): Whether to update the quantization param during the training or not.
             rounding_type (RoundingType): An enum that defines the rounding type.
-            lsb_change_per_bit_width (dict): Whether to update the bias during the training or not.
-            eps (float): A floating point value for numeric stability.
-            use_jac_based_weights (bool): Whether to use jacobian-based weights for weighted average loss.
-            num_samples_for_loss (int): Number of samples to use for computing the jacobian-based weights.
-            norm_weights (bool): Whether to normalize the returned weights (to get values between 0 and 1).
+            use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
             optimizer_quantization_parameter (Any): Optimizer to override the rest optimizer  for quantizer parameters.
-            optimizer_bias (Any): Optimizer to override the rest optimizerfor bias.
-            log_norm (bool): Whether to use log normalization to the GPTQ Jacobian-based weights.
-            weights_n_iter (int): Number of random iterations to run Jacobian approximation for GPTQ weights.
-            quantizer_config (GPTQQuantizerConfig): A class that contains the quantizer specific config.
+            optimizer_bias (Any): Optimizer to override the rest optimizer for bias.
+            regularization_factor (float): A floating point number that defines the regularization factor.
+            hessian_weights_config (GPTQHessianWeightsConfig): A configuration that include all necessary arguments to run a computation of Hessian weights for the GPTQ loss.
+            gptq_quantizer_params_override (dict): A dictionary of parameters to override in GPTQ quantizer instantiation. Defaults to None (no parameters).
 
         """
         self.n_iter = n_iter
         self.optimizer = optimizer
         self.optimizer_rest = optimizer_rest
         self.loss = loss
         self.log_function = log_function
         self.train_bias = train_bias
 
-        if quantization_parameters_learning and rounding_type == RoundingType.STE:
-            common.Logger.error("Quantization parameters learning is not supported with STE rounding.")
-
-        self.quantization_parameters_learning = quantization_parameters_learning
         self.rounding_type = rounding_type
-        self.lsb_change_per_bit_width = lsb_change_per_bit_width
-        self.eps = eps
-        self.use_jac_based_weights = use_jac_based_weights
-        self.num_samples_for_loss = num_samples_for_loss
-        self.norm_weights = norm_weights
+        self.use_hessian_based_weights = use_hessian_based_weights
         self.optimizer_quantization_parameter = optimizer_quantization_parameter
         self.optimizer_bias = optimizer_bias
-        self.log_norm = log_norm
-        self.weights_n_iter = weights_n_iter
-
-        if self._verify_quantizer_config(quantizer_config, rounding_type):
-            self.quantizer_config = quantizer_config
-        else:
-            common.Logger.error(f"Quantizer config of type {type(quantizer_config)} "
-                                f"is not suitable for rounding type {rounding_type}")
-
-
-    def _verify_quantizer_config(self, quantizer_config, rounding_type) -> bool:
-        """
-        Verifies that the given quantizer config matches the given rounding type.
-
-        Args:
-            quantizer_config: A quantizer config.
-            rounding_type: A RoundingType.
-
-        Returns: True if the quantizer config matches the rounding type, False otherwise.
-
-        """
-        if rounding_type == RoundingType.SoftQuantizer:
-            return type(quantizer_config) == SoftQuantizerConfig
-
-        # Here, we compare type() and not isinstance to exclude instance equality because of inheritance
-        return type(quantizer_config) == GPTQQuantizerConfig
+        self.regularization_factor = regularization_factor
+        self.hessian_weights_config = hessian_weights_config
 
+        self.gptq_quantizer_params_override = {} if gptq_quantizer_params_override is None \
+            else gptq_quantizer_params_override
 
 
 class GradientPTQConfigV2(GradientPTQConfig):
     """
     Configuration to use for quantization with GradientPTQV2 (experimental).
     """
-    def __init__(self,
-                 n_epochs: int,
+    def __init__(self, n_epochs: int,
                  optimizer: Any,
                  optimizer_rest: Any = None,
                  loss: Callable = None,
                  log_function: Callable = None,
                  train_bias: bool = True,
-                 quantization_parameters_learning: bool = False,
                  rounding_type: RoundingType = RoundingType.SoftQuantizer,
-                 lsb_change_per_bit_width: dict = DefaultDict({}, lambda: 1),
-                 eps: float = 1e-6,
-                 use_jac_based_weights: bool = True,
-                 num_samples_for_loss: int = 16,
-                 norm_weights: bool = False,
+                 use_hessian_based_weights: bool = True,
                  optimizer_quantization_parameter: Any = None,
                  optimizer_bias: Any = None,
-                 log_norm: bool = True,
-                 weights_n_iter: int = 50,
-                 quantizer_config: GPTQQuantizerConfig = SoftQuantizerConfig()):
+                 regularization_factor: float = REG_DEFAULT,
+                 hessian_weights_config: GPTQHessianWeightsConfig = GPTQHessianWeightsConfig(),
+                 gptq_quantizer_params_override: Dict[str, Any] = None):
         """
         Initialize a GradientPTQConfigV2.
 
         Args:
             n_epochs (int): Number of representative dataset epochs to train.
             optimizer (Any): Optimizer to use.
             optimizer_rest (Any): Optimizer to use for bias and quantizer parameters.
             loss (Callable): The loss to use. should accept 6 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors,
              the 3rd is a list of quantized weights, the 4th is a list of float weights, the 5th and 6th lists are the mean and std of the tensors
              accordingly. see example in multiple_tensors_mse_loss
             log_function (Callable): Function to log information about the GPTQ process.
             train_bias (bool): Whether to update the bias during the training or not.
-            quantization_parameters_learning (bool): Whether to update the quantization param during the training or not.
             rounding_type (RoundingType): An enum that defines the rounding type.
-            lsb_change_per_bit_width (dict): Whether to update the bias during the training or not.
-            eps (float): A floating point value for numeric stability.
-            use_jac_based_weights (bool): Whether to use jacobian-based weights for weighted average loss.
-            num_samples_for_loss (int): Number of samples to use for computing the jacobian-based weights.
-            norm_weights (bool): Whether to normalize the returned weights (to get values between 0 and 1).
+            use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
             optimizer_quantization_parameter (Any): Optimizer to override the rest optimizer  for quantizer parameters.
             optimizer_bias (Any): Optimizer to override the rest optimizerfor bias.
-            log_norm (bool): Whether to use log normalization to the GPTQ Jacobian-based weights.
-            weights_n_iter (int): Number of random iterations to run Jacobian approximation for GPTQ weights.
-            quantizer_config (Any): A class that contains the quantizer specific config.
+            regularization_factor (float): A floating point number that defines the regularization factor.
+            hessian_weights_config (GPTQHessianWeightsConfig): A configuration that include all necessary arguments to run a computation of Hessian weights for the GPTQ loss.
+            gptq_quantizer_params_override (dict): A dictionary of parameters to override in GPTQ quantizer instantiation. Defaults to None (no parameters).
 
         """
 
         super().__init__(n_iter=None,
                          optimizer=optimizer,
                          optimizer_rest=optimizer_rest,
                          loss=loss,
                          log_function=log_function,
                          train_bias=train_bias,
-                         quantization_parameters_learning=quantization_parameters_learning,
                          rounding_type=rounding_type,
-                         lsb_change_per_bit_width=lsb_change_per_bit_width,
-                         eps=eps,
-                         use_jac_based_weights=use_jac_based_weights,
-                         num_samples_for_loss=num_samples_for_loss,
-                         norm_weights=norm_weights,
+                         use_hessian_based_weights=use_hessian_based_weights,
                          optimizer_quantization_parameter=optimizer_quantization_parameter,
                          optimizer_bias=optimizer_bias,
-                         log_norm=log_norm,
-                         weights_n_iter=weights_n_iter,
-                         quantizer_config=quantizer_config)
+                         regularization_factor=regularization_factor,
+                         hessian_weights_config=hessian_weights_config,
+                         gptq_quantizer_params_override=gptq_quantizer_params_override)
         self.n_epochs = n_epochs
 
     @classmethod
     def from_v1(cls, n_ptq_iter: int, config_v1: GradientPTQConfig):
         """
         Initialize a GradientPTQConfigV2 from GradientPTQConfig instance.
 
@@ -207,10 +180,7 @@
             config_v1 (GradientPTQConfig): A GPTQ config to convert to V2.
 
         """
         n_epochs = int(round(config_v1.n_iter) / n_ptq_iter)
         v1_params = config_v1.__dict__
         v1_params = {k: v for k, v in v1_params.items() if k != 'n_iter'}
         return cls(n_epochs, **v1_params)
-
-
-
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_graph.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_graph.py`

 * *Files 26% similar despite different names*

```diff
@@ -9,14 +9,17 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Tuple, List
+
+from model_compression_toolkit.core import FrameworkInfo
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.core.common.graph.base_node import BaseNode
 
 
 def get_compare_points(input_graph: Graph) -> Tuple[List[BaseNode], List[str], List, List]:
     """
     Create a list of nodes with weights in a graph and a corresponding list
@@ -38,7 +41,26 @@
     for n in input_graph.get_topo_sorted_nodes():
         if len(n.weights) > 0 and n.is_weights_quantization_enabled() and not n.reuse:
             compare_points.append(n)
             compare_points_name.append(n.name)
             compare_points_std.append(n.prior_info.std_output)
             compare_points_mean.append(n.prior_info.mean_output)
     return compare_points, compare_points_name, compare_points_mean, compare_points_std
+
+
+def get_kernel_attribute_name_for_gptq(layer_type: type, fw_info: FrameworkInfo) -> str:
+    """
+    Returns a layer's kernel attribute name for GPTQ training purposes.
+
+    Args:
+        layer_type: A type of model's layer.
+        fw_info: A FrameworkInfo object.
+
+    Returns: The name of the kernel attribute.
+
+    """
+    kernel_attribute = fw_info.get_kernel_op_attributes(layer_type)
+    if len(kernel_attribute) != 1:
+        Logger.error(  # pragma: no cover
+            f"In GPTQ training only the kernel weights attribute should be trained, but number of kernel "
+            f"attributes is {len(kernel_attribute)}.")
+    return kernel_attribute[0]
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/common/gptq_training.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/common/gptq_training.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,47 +12,47 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 from abc import ABC, abstractmethod
 import numpy as np
 from typing import Callable, List, Any
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType
-from model_compression_toolkit.core.common import Graph, Logger, BaseNode
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
+from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
+from model_compression_toolkit.gptq.common.gptq_constants import QUANT_PARAM_LEARNING_STR
+from model_compression_toolkit.gptq.common.gptq_framework_implementation import GPTQFrameworkImplemantation
 from model_compression_toolkit.gptq.common.gptq_graph import get_compare_points
 from model_compression_toolkit.core.common.model_builder_mode import ModelBuilderMode
+from model_compression_toolkit.logger import Logger
 
 
 class GPTQTrainer(ABC):
     """
     Abstract GPTQ training class for fine-tuning a quantized model
     """
 
     def __init__(self,
                  graph_float: Graph,
                  graph_quant: Graph,
                  gptq_config: GradientPTQConfig,
-                 fw_impl: FrameworkImplementation,
-                 fw_info: FrameworkInfo,
-                 representative_data_gen: Callable):
+                 fw_impl: GPTQFrameworkImplemantation,
+                 fw_info: FrameworkInfo):
         """
         Build two models from a graph: A teacher network (float model) and a student network (quantized model).
         Use the dataset generator to pass images through the teacher and student networks to get intermediate
         layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
         in the student network, to minimize it in the next similar steps.
         All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
         Args:
             graph_float: Graph to build a float networks from.
             graph_quant: Graph to build a quantized networks from.
             gptq_config: GradientPTQConfig with parameters about the tuning process.
             fw_impl: Framework implementation
             fw_info: Framework information
-            representative_data_gen: Dataset to use for inputs of the models.
         """
         self.graph_float = copy.deepcopy(graph_float)
         self.graph_quant = copy.deepcopy(graph_quant)
         self.gptq_config = gptq_config
         self.fw_impl = fw_impl
         self.fw_info = fw_info
 
@@ -62,18 +62,14 @@
         self.compare_points, _, self.compare_points_mean, self.compare_points_std = get_compare_points(self.graph_float)
 
         self.float_model, self.float_user_info = fw_impl.model_builder(self.graph_float,
                                                                        mode=ModelBuilderMode.FLOAT,
                                                                        append2output=self.compare_points,
                                                                        fw_info=self.fw_info)
 
-        if self.gptq_config.rounding_type == RoundingType.SoftQuantizer:
-            # dry run on the representative dataset to count number of batches
-            self.count_num_batches_for_training(representative_data_gen)
-
         self.fxp_model, self.gptq_user_info = self.build_gptq_model()
 
     def get_optimizer_with_param(self,
                                  flattened_trainable_weights: List[Any],
                                  flattened_bias_weights: List[Any],
                                  trainable_quantization_parameters: List[Any]) -> List[Any]:
         """
@@ -84,52 +80,61 @@
             trainable_quantization_parameters: list of trainable quantization parameters
         Returns:
             List of Optimizer objects with parameters
         """
 
         w2train = [*flattened_trainable_weights]
 
+        quant_params_learning = self.gptq_config.gptq_quantizer_params_override.get(QUANT_PARAM_LEARNING_STR, False)
+
         optimizer_with_param = [(self.gptq_config.optimizer, w2train)]
-        if self.gptq_config.train_bias or self.gptq_config.quantization_parameters_learning:
+        if self.gptq_config.train_bias or quant_params_learning:
             w2train_res = []
             if self.gptq_config.train_bias:
                 if self.gptq_config.optimizer_bias is not None:
                     optimizer_with_param.append((self.gptq_config.optimizer_bias, flattened_bias_weights))
                 else:
                     w2train_res.extend(flattened_bias_weights)
                     if self.gptq_config.optimizer_rest is None:
-                        Logger.error(
+                        Logger.error(  # pragma: no cover
                             "To enable bias micro training an additional optimizer is required, please define the optimizer_rest")
-            if self.gptq_config.quantization_parameters_learning:
+            if quant_params_learning:
                 if self.gptq_config.optimizer_quantization_parameter is not None:  # Ability to override optimizer
                     optimizer_with_param.append((self.gptq_config.optimizer_quantization_parameter,
                                                  trainable_quantization_parameters))
                 else:
                     w2train_res.extend(trainable_quantization_parameters)
                 if self.gptq_config.optimizer_rest is None:
-                    Logger.error(
-                        "To enable bias micro training an additional optimizer is required, please define the optimizer_rest")
-            optimizer_with_param.append((self.gptq_config.optimizer_rest, w2train_res))
+                    Logger.error(  # pragma: no cover
+                        "To enable quantization parameters micro training an additional optimizer is required, please define the optimizer_rest")
+            if len(w2train_res) > 0:
+                # Either bias or quantization parameters are trainable but did not provide a specific optimizer,
+                # so we should use optimizer_rest to train them
+                if self.gptq_config.optimizer_rest is None:
+                    Logger.error(  # pragma: no cover
+                        "To enable bias or quantization parameters micro training an additional optimizer is required, please define the optimizer_rest")
+                optimizer_with_param.append((self.gptq_config.optimizer_rest, w2train_res))
 
         return optimizer_with_param
 
 
-    def compute_jacobian_based_weights(self,
-                                       representative_data_gen: Callable) -> np.ndarray:
+    def compute_hessian_based_weights(self,
+                                      representative_data_gen: Callable) -> np.ndarray:
         """
-        Computes the jacobian-based weights using the framework's model_grad method per batch of images.
+        Computes the Hessian-based weights using the framework's model_grad method per batch of images.
 
         Args:
-            representative_data_gen: Dataset used for inference to compute the jacobian-based weights.
+            representative_data_gen: Dataset used for inference to compute the Hessian-based weights.
 
         Returns: A vector of weights, one for each compare point,
         to be used for the loss metric weighted average computation when running GPTQ training.
         """
-        if self.gptq_config.use_jac_based_weights:
-            images = self._generate_images_batch(representative_data_gen, self.gptq_config.num_samples_for_loss)
+        if self.gptq_config.use_hessian_based_weights:
+            images = self._generate_images_batch(representative_data_gen,
+                                                 self.gptq_config.hessian_weights_config.hessians_num_samples)
 
             model_output_replacement = self._get_model_output_replacement()
 
             points_apprx_jacobians_weights = []
             for i in range(1, images.shape[0] + 1):
                 Logger.info(f"Computing Jacobian-based weights approximation for image sample {i} out of {images.shape[0]}...")
                 # Note that in GPTQ loss weights computation we assume that there aren't replacement output nodes,
@@ -139,25 +144,26 @@
                                                              {inode: self.fw_impl.to_tensor(images[i - 1:i]) for inode
                                                               in
                                                               self.graph_float.get_inputs()},
                                                              self.compare_points,
                                                              output_list=model_output_replacement,
                                                              all_outputs_indices=[],
                                                              alpha=0,
-                                                             norm_weights=self.gptq_config.norm_weights,
-                                                             n_iter=self.gptq_config.weights_n_iter)
+                                                             norm_weights=self.gptq_config.hessian_weights_config.norm_weights,
+                                                             n_iter=self.gptq_config.hessian_weights_config.hessians_n_iter)
                 points_apprx_jacobians_weights.append(image_ip_gradients)
-            if self.gptq_config.log_norm:
+            if self.gptq_config.hessian_weights_config.log_norm:
                 mean_jacobian_weights = np.mean(points_apprx_jacobians_weights, axis=0)
                 mean_jacobian_weights = np.where(mean_jacobian_weights != 0, mean_jacobian_weights,
                                                  np.partition(mean_jacobian_weights, 1)[1])
                 log_weights = np.log10(mean_jacobian_weights)
 
-                # To add scaling to the normalized weights replace return statement with the following line:
-                # return log_weights - np.min(log_weights) / (np.max(log_weights) - np.min(log_weights))
+                if self.gptq_config.hessian_weights_config.scale_log_norm:
+                    return (log_weights - np.min(log_weights)) / (np.max(log_weights) - np.min(log_weights))
+
                 return log_weights - np.min(log_weights)
             else:
                 return np.mean(points_apprx_jacobians_weights, axis=0)
         else:
             num_nodes = len(self.compare_points)
             return np.asarray([1 / num_nodes for _ in range(num_nodes)])
 
@@ -245,35 +251,20 @@
                 prev_node = self.graph_float.get_prev_nodes(n.node)
                 assert len(prev_node) == 1, "A none compatible output node has multiple inputs, " \
                                             "which is incompatible for metric computation."
                 prev_node = prev_node[0]
             replacement_outputs.append(prev_node)
         return replacement_outputs
 
-    def count_num_batches_for_training(self, representative_data_gen):
-        """
-        Runs a "dry-run" of the representative dataset to count the number of batches for each training epoch.
-
-        Args:
-            representative_data_gen: A callable method to retrieve images from Dataset.
-
-        Returns: The number of batches for each training epoch.
-
-        """
-        num_batches = 0
-        for _ in representative_data_gen():
-            num_batches += 1
-        self.gptq_config.quantizer_config.set_num_batches(num_batches)
-
 
 def gptq_training(graph_float: Graph,
                   graph_quant: Graph,
                   gptq_config: GradientPTQConfig,
                   representative_data_gen: Callable,
-                  fw_impl: FrameworkImplementation,
+                  fw_impl: GPTQFrameworkImplemantation,
                   fw_info: FrameworkInfo) -> Graph:
     """
     GPTQ training process using knowledge distillation with a teacher network (float model) and a student network (quantized model).
     Args:
         graph_float: Graph to build a float networks from.
         graph_quant: Graph to build a quantized networks from.
         gptq_config: GradientPTQConfig with parameters about the tuning process.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/gptq_loss.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/gptq_loss.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/gptq_training.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/gptq_training.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,47 +8,41 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Callable, List, Tuple
+from typing import Callable, List, Tuple, Union
 
-import tensorflow as tf
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
+import numpy as np
+from torch.nn import Module
 from tqdm import tqdm
-
-# As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
-from model_compression_toolkit.gptq.common.gptq_constants import REGULARIZATION_VALUES
-from model_compression_toolkit.gptq.keras.gptq_model_builder import GPTQKerasModelBuilder
-from packaging import version
-
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.engine.base_layer import TensorFlowOpLayer
-else:
-    from keras.engine.base_layer import TensorFlowOpLayer
-
-from model_compression_toolkit.core import common
+import copy
+import torch
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
 from model_compression_toolkit.gptq.common.gptq_training import GPTQTrainer
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2, RoundingType
-from model_compression_toolkit.core.common import Graph
-from model_compression_toolkit.gptq.keras.graph_info import get_trainable_parameters, get_weights_for_loss, \
-    get_soft_rounding_reg
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.core.common import Graph, BaseNode
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
-import numpy as np
-import copy
-from model_compression_toolkit.core.keras.constants import BIAS, USE_BIAS
-from model_compression_toolkit.gptq.keras.quantizer import WeightQuantizeConfig
+from model_compression_toolkit.core.pytorch.constants import BIAS
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, set_model, torch_tensor_to_numpy
+from model_compression_toolkit.gptq.pytorch.graph_info import get_gptq_trainable_parameters, \
+    get_weights_for_loss
+from model_compression_toolkit.gptq.pytorch.quantizer.quantization_builder import quantization_builder
+from model_compression_toolkit.gptq.pytorch.quantizer.regularization_factory import get_regularization
+from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
 
 
-class KerasGPTQTrainer(GPTQTrainer):
+class PytorchGPTQTrainer(GPTQTrainer):
     """
-    Keras GPTQ training class for fine-tuning a quantized model
+    Pytorch GPTQ training class for fine-tuning a quantized model
     """
 
     def __init__(self,
                  graph_float: Graph,
                  graph_quant: Graph,
                  gptq_config: GradientPTQConfigV2,
                  fw_impl: FrameworkImplementation,
@@ -59,230 +53,242 @@
         Use the dataset generator to pass images through the teacher and student networks to get intermediate
         layers outputs. Use the outputs to compute the observed loss and to back-propagate the error
         in the student network, to minimize it in the next similar steps.
         All parameters (such as number of iterations, optimizer, etc.) are in GradientPTQConfig.
         Args:
             graph_float: Graph to build a float networks from.
             graph_quant: Graph to build a quantized networks from.
-            gptq_config: GradientPTQConfig with parameters about the tuning process.
+            gptq_config: GradientPTQConfigV2 with parameters about the tuning process.
             fw_impl: FrameworkImplementation object with a specific framework methods implementation.
-            fw_info: Framework information.
+            fw_info: Framework information
             representative_data_gen: Dataset to use for inputs of the models.
         """
-        super().__init__(graph_float,
-                         graph_quant,
-                         gptq_config,
-                         fw_impl,
-                         fw_info,
-                         representative_data_gen)
-
+        super().__init__(graph_float, graph_quant, gptq_config, fw_impl, fw_info)
         self.loss_list = []
         self.input_scale = 1
+        if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
+            Logger.error("Input scale mismatch between float and GPTQ networks")  # pragma: no cover
+        else:
+            self.input_scale = self.gptq_user_info.input_scale
 
-        trainable_weights, bias_weights, trainable_threshold, temperature_weights = get_trainable_parameters(
+        trainable_weights, trainable_bias, trainable_threshold = get_gptq_trainable_parameters(
             self.fxp_model,
-            fw_info,
-            add_bias=gptq_config.train_bias)
+            add_bias=self.gptq_config.train_bias)
 
         self.flp_weights_list, self.fxp_weights_list = get_weights_for_loss(self.fxp_model)
-
         if not (len(self.compare_points) == len(trainable_weights) == len(self.flp_weights_list) == len(
                 self.fxp_weights_list)):
-            raise Exception(
+            Logger.error(
                 "GPTQ: Mismatch between number of compare points, number of layers with trainable weights " +
                 "and number of float and quantized weights for loss")
 
-        flattened_trainable_weights = [w for layer_weights in trainable_weights for w in layer_weights]
-        flattened_bias_weights = [w for layer_weights in bias_weights for w in layer_weights]
-        trainable_quantization_parameters = trainable_threshold
-        self.optimizer_with_param = self.get_optimizer_with_param(flattened_trainable_weights,
-                                                                  flattened_bias_weights,
-                                                                  trainable_quantization_parameters)
-        self.has_params_to_train = np.sum([len(optimizer_params_tuple[1]) for optimizer_params_tuple in self.optimizer_with_param])>0
+        self.optimizer_with_param = self.get_optimizer_with_param(trainable_weights,
+                                                                  trainable_bias,
+                                                                  trainable_threshold)
 
-        if self.float_user_info.input_scale != self.gptq_user_info.input_scale:
-            common.Logger.error("Input scale mismatch between float and GPTQ networks")  # pragma: no cover
-        else:
-            self.input_scale = self.gptq_user_info.input_scale
+        self.weights_for_average_loss = to_torch_tensor(self.compute_hessian_based_weights(representative_data_gen))
 
-        self.weights_for_average_loss = self.compute_jacobian_based_weights(representative_data_gen)
+        self.reg_func = get_regularization(self.gptq_config, representative_data_gen)
 
-    def build_gptq_model(self):
+    def _is_gptq_weights_trainable(self,
+                                   node: BaseNode) -> bool:
         """
-        Build the GPTQ model with QuantizationWrappers
+        A function for deciding if a layer should be fine-tuned during GPTQ.
+        Args:
+            node (BaseNode): Node for quantization decision
         Returns:
-            Quantized graph for GPTQ fine-tuning, GPTQ graph user info
+            A boolean whether the layer is to be wrapped with a Quantization Wrapper.
         """
 
-        return GPTQKerasModelBuilder(graph=self.graph_quant,
-                                     gptq_config=self.gptq_config,
-                                     append2output=self.compare_points,
-                                     fw_info=self.fw_info,
-                                     return_float_outputs=True).build_model()
-
-    def compute_gradients(self, in_y_float: List[tf.Tensor], input_data: List[np.ndarray],
-                          in_optimizer_with_param: List,
-                          training=True) -> Tuple[tf.Tensor, List[tf.Tensor]]:
+        if node.is_weights_quantization_enabled() and not self.fw_info.is_kernel_op(node.type):
+            Logger.error(f"GPTQ Error: Quantizing node {node.name} of type {node.type} "
+                         f"without a kernel isn't supported.")
+        return node.is_weights_quantization_enabled()
+
+    def gptq_wrapper(self,
+                     n: BaseNode,
+                     layer: Module) -> Union[PytorchQuantizationWrapper, Module]:
         """
-        Get outputs from both teacher and student networks. Compute the observed error,
-        and use it to compute the gradients and applying them to the student weights.
+        A function which takes a computational graph node and a pytorch layer and perform the quantization wrapping.
+
         Args:
-            in_y_float: A list of reference tensor from the floating point network.
-            input_data: A list of Input tensors to pass through the networks.
-            in_optimizer_with_param: A list of optimizer classes to update with the corresponding parameters.
-            training: A boolean flag stating if the network is running in training mode.
+            n: A node of mct graph.
+            layer: A pytorch layer
 
-        Returns:
-            Loss and gradients.
+        Returns: Wrapped layer if the layer should be wrap, otherwise returns the layer as is.
         """
-        param2grad = []
-        for _, p in in_optimizer_with_param:
-            param2grad.extend(p)
-
-        with tf.GradientTape(persistent=True) as tape:
-            y_fxp = self.fxp_model(input_data, training=training)  # running fxp model
-            loss_value = self.gptq_config.loss(y_fxp,
-                                               in_y_float,
-                                               self.fxp_weights_list,
-                                               self.flp_weights_list,
-                                               self.compare_points_mean,
-                                               self.compare_points_std,
-                                               self.weights_for_average_loss)
-
-            reg_value = self.gptq_config.quantizer_config.get_regularization_value(
-                self.fxp_model,
-                **{REGULARIZATION_VALUES: self._get_quantizer_regularization_values(self.gptq_config.rounding_type)})
-
-            loss_value += reg_value
-
-        # Use the gradient tape to automatically retrieve
-        # the gradients of the trainable variables with respect to the loss.
-        grads = tape.gradient(loss_value, param2grad)
-        res = []
-        i = 0
-        for _, p in in_optimizer_with_param:
-            res.append(grads[i:(i + len(p))])
-            i += len(p)
-        return loss_value, res
 
-    def train(self, representative_data_gen: Callable):
+        if self._is_gptq_weights_trainable(n):
+            weights_quantizers, activation_quantizers = quantization_builder(n, self.gptq_config)
+            return PytorchQuantizationWrapper(layer,
+                                              weights_quantizers=weights_quantizers)
+        else:
+            return layer
+
+    def get_activation_quantizer_holder(self, n: BaseNode) -> Callable:
         """
-        Train the quantized model using GPTQ training process in Keras framework
+        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
+        If the layer is not supposed to be wrapped with an activation quantizer - return None.
         Args:
-            representative_data_gen: Dataset to use for inputs of the models.
+            n: Node to attach a PytorchActivationQuantizationHolder to its output.
+        Returns:
+            A PytorchActivationQuantizationHolder module for the node's activation quantization.
         """
-        compute_gradients = self.compute_gradients
+        _, activation_quantizers = quantization_builder(n, self.gptq_config)
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node we no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return PytorchActivationQuantizationHolder(activation_quantizers[0])
+        Logger.error(
+            f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
+            f'were found for node {n}')
+
+    def build_gptq_model(self):
+        """
+        Build the GPTQ model with QuantizationWrappers
+        Returns:
+            Quantized graph for GPTQ fine-tuning, GPTQ graph user info
+        """
+        gptq_model, gptq_user_info = PyTorchModelBuilder(graph=self.graph_quant,
+                                                         append2output=self.compare_points,
+                                                         fw_info=self.fw_info,
+                                                         wrapper=self.gptq_wrapper,
+                                                         return_float_outputs=True,
+                                                         get_activation_quantizer_holder_fn=self.get_activation_quantizer_holder).build_model()
+
+        return gptq_model, gptq_user_info
+
+    def train(self, representative_data_gen: Callable):
+        """
+          GPTQ Training using pytorch framework
+          Args:
+              representative_data_gen: Dataset generator to get images.
+          Returns:
+              Graph after GPTQ training
+          """
+        # Set Optimizers
+        for (optimizer, params) in self.optimizer_with_param:
+            optimizer.param_groups.clear()
+            optimizer.add_param_group({'params': params})
+
+        # Set models mode
+        set_model(self.float_model, False)
+        set_model(self.fxp_model, True)
+        self._set_requires_grad()
 
         # ----------------------------------------------
         # Training loop
         # ----------------------------------------------
-        if self.has_params_to_train:
-            self.micro_training_loop(representative_data_gen,
-                                     compute_gradients,
-                                     self.optimizer_with_param,
-                                     self.gptq_config.n_epochs,
-                                     True)
+        self.micro_training_loop(representative_data_gen, self.gptq_config.n_epochs)
 
-    @tf.function
-    def nano_training_step(self, input_data, in_compute_gradients, in_optimizer_with_param, is_training):
+    def compute_gradients(self,
+                          y_float: List[torch.Tensor],
+                          input_tensors: List[torch.Tensor]) -> Tuple[torch.Tensor, List[np.ndarray]]:
         """
-        This function run part of the training step, wrapped by a tf.function for acceleration.
+        Get outputs from both teacher and student networks. Compute the observed error,
+        and use it to compute the gradients and applying them to the student weights.
         Args:
-            input_data: input data for the step.
-            in_compute_gradients: A callable function that compute the gradients.
-            in_optimizer_with_param: A list of optimizer classes to update with the corresponding parameters.
-            is_training: A boolean flag stating if the network is running in training mode.
-
+            y_float: A list of reference tensor from the floating point network.
+            input_tensors: A list of Input tensors to pass through the networks.
         Returns:
-            loss value and gradients
-
+            Loss and gradients.
         """
 
-        # run float model
-        y_float = self.float_model(input_data)
-        # rung quantized model and calculate loss & gradients
-        loss_value_step, grads = in_compute_gradients(y_float, input_data, in_optimizer_with_param,
-                                                      training=is_training)
-        return loss_value_step, grads
+        # Forward-pass
+        y_fxp = self.fxp_model(input_tensors)
+
+        # Loss
+        loss_value = self.gptq_config.loss(y_fxp,
+                                           y_float,
+                                           self.fxp_weights_list,
+                                           self.flp_weights_list,
+                                           self.compare_points_mean,
+                                           self.compare_points_std,
+                                           self.weights_for_average_loss)
+
+        reg_value = self.reg_func(self.fxp_model, self.gptq_config.regularization_factor)
+
+        loss_value += reg_value
+
+        # Back-pass
+        loss_value.backward()
+
+        # Get gradients
+        grads = []
+        for param in self.fxp_model.parameters():
+            if param.requires_grad and param.grad is not None:
+                grads.append(torch_tensor_to_numpy(param.grad))
+
+        return loss_value, grads
 
     def micro_training_loop(self,
                             data_function: Callable,
-                            in_compute_gradients: Callable,
-                            in_optimizer_with_param: List[Tuple[tf.keras.optimizers.Optimizer, List[tf.Tensor]]],
-                            n_epochs: int,
-                            is_training: bool):
+                            n_epochs: int):
         """
         This function run a micro training loop on given set of parameters.
         Args:
             data_function: A callable function that give a batch of samples.
-            in_compute_gradients: A callable function that compute the gradients.
-            in_optimizer_with_param: A list of optimizer classes to update with the corresponding parameters.
             n_epochs: Number of update iterations of representative dataset.
-            is_training: A boolean flag stating if the network is running in training mode.
-
-        Returns: None
-
         """
         for _ in tqdm(range(n_epochs)):
             for data in tqdm(data_function()):
                 input_data = [d * self.input_scale for d in data]
-
-                loss_value_step, grads = self.nano_training_step(input_data, in_compute_gradients, in_optimizer_with_param, is_training)
-                # Run one step of gradient descent by updating
-                # the value of the variables to minimize the loss.
-                for i, (o, p) in enumerate(in_optimizer_with_param):
-                    o.apply_gradients(zip(grads[i], p))
+                input_tensor = to_torch_tensor(input_data)
+                y_float = self.float_model(input_tensor)  # running float model
+                loss_value, grads = self.compute_gradients(y_float, input_tensor)
+                # Run one step of gradient descent by updating the value of the variables to minimize the loss.
+                for (optimizer, _) in self.optimizer_with_param:
+                    optimizer.step()
+                    optimizer.zero_grad()
                 if self.gptq_config.log_function is not None:
-                    self.gptq_config.log_function(loss_value_step, grads[0], in_optimizer_with_param[0][-1],
-                                                  self.compare_points)
-                self.loss_list.append(loss_value_step.numpy())
-                common.Logger.debug(f'last loss value: {self.loss_list[-1]}')
+                    self.gptq_config.log_function(loss_value.item(),
+                                                  torch_tensor_to_numpy(grads),
+                                                  torch_tensor_to_numpy(self.optimizer_with_param[0][-1]))
+                self.loss_list.append(loss_value.item())
+                Logger.debug(f'last loss value: {self.loss_list[-1]}')
 
-    def update_graph(self):
+    def update_graph(self) -> Graph:
         """
         Update a graph using GPTQ after minimizing the loss between the float model's output
         and the quantized model's outputs.
         Returns:
             Updated graph after GPTQ.
         """
-        graph = copy.copy(self.graph_quant)
+        graph_quant = copy.copy(self.graph_quant)
 
-        for layer in self.fxp_model.layers:
-            if isinstance(layer, QuantizeWrapper) and isinstance(
-                    layer.quantize_config, WeightQuantizeConfig):
-                node = graph.find_node_by_name(layer.layer.name)
-                if len(node) == 0 and isinstance(layer.layer, TensorFlowOpLayer):
-                    node = graph.find_node_by_name('_'.join(layer.layer.name.split('_')[3:]))
+        # Update graph after training
+        for name, layer in self.fxp_model.named_modules():
+            if isinstance(layer, PytorchQuantizationWrapper):
+                node = self.graph_quant.find_node_by_name(name)
                 if len(node) != 1:
-                    common.Logger.error(f"Can't update GPTQ graph due to missing layer named: {layer.layer.name}")
+                    Logger.error(f"Can't update GPTQ graph due to missing layer named: {name}")
                 node = node[0]
+                kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=node.type,
+                                                                      fw_info=self.fw_info)
                 weights, weight_quant_config, activation_quant_config = \
-                    layer.quantize_config.update_layer_quantization_params(layer)
+                    layer.weights_quantizers[kernel_attribute].update_layer_quantization_params(layer)
                 for weight_attr, weight in weights.items():
-                    node.set_weights_by_keys(weight_attr, weight.numpy())
+                    node.set_weights_by_keys(weight_attr, self.fw_impl.to_numpy(weight))
                 for config_attr, config_value in weight_quant_config.items():
                     node.final_weights_quantization_cfg.set_quant_config_attr(config_attr, config_value)
                 for config_attr, config_value in activation_quant_config.items():
                     node.final_activation_quantization_cfg.set_quant_config_attr(config_attr, config_value)
-                if self.gptq_config.train_bias:
-                    use_bias = layer.layer.get_config().get(USE_BIAS)
-                    if use_bias is not None and use_bias:
-                        new_bias = layer.layer.bias.numpy()
-                        node.set_weights_by_keys(BIAS, new_bias)
+                if self.gptq_config.train_bias and hasattr(layer.layer, BIAS):
+                    node.set_weights_by_keys(BIAS, self.fw_impl.to_numpy(getattr(layer.layer, BIAS)))
 
-        return graph
+        return graph_quant
 
-
-    def _get_quantizer_regularization_values(self, rounding_type: RoundingType) -> List[tf.Tensor]:
+    def _set_requires_grad(self):
         """
-        Mapping between a rounding type to its matching regularization method.
-
-        Args:
-            rounding_type: GPTQ rounding type.
-
-        Returns: A regularization computation method.
-
+        Set require_grad flag for trainable parameters for GPTQ training
         """
-        if rounding_type == RoundingType.SoftQuantizer:
-            return get_soft_rounding_reg(self.fxp_model)
-        else:
-            return []
+        # Float model: freeze all the parameters in the network
+        for param in self.float_model.parameters():
+            param.requires_grad = False
+
+        # Fxp model: unfreeze bias trainable parameters
+        for layer in self.fxp_model.modules():
+            if isinstance(layer, PytorchQuantizationWrapper):
+                if hasattr(layer.layer, BIAS):
+                    bias = getattr(layer.layer, BIAS)
+                    bias.requires_grad = self.gptq_config.train_bias
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/graph_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/graph_info.py`

 * *Files 18% similar despite different names*

```diff
@@ -9,29 +9,30 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-
 import tensorflow as tf
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
 from typing import Tuple, List
-
 from model_compression_toolkit.core.keras.constants import USE_BIAS
-from model_compression_toolkit.gptq.keras.quantizer import WeightQuantizeConfig
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from tensorflow.keras.models import Model
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
+from model_compression_toolkit.logger import Logger
+from mct_quantizers import KerasQuantizationWrapper
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-def get_trainable_parameters(fxp_model: Model,
-                             fw_info: FrameworkInfo,
-                             add_bias: bool = False) -> (
-        List[tf.Variable], List[tf.Variable], List[tf.Variable], List[tf.Variable], List[tf.Variable]):
+def get_gptq_trainable_parameters(fxp_model: Model,
+                                  fw_info: FrameworkInfo,
+                                  add_bias: bool = False) -> (
+        List[tf.Variable], List[tf.Variable], List[tf.Variable]):
     """
     Get trainable parameters from all layers in a model
 
     Args:
         fxp_model: Model to get its trainable parameters.
         fw_info: Framework information needed for keras kernel ops list.
         add_bias: Whether to include biases of the model (if there are) or not.
@@ -39,32 +40,37 @@
     Returns:
         A list of trainable variables in a model. Each item is a list of a layers weights.
     """
 
     trainable_weights: List[tf.Tensor] = []
     trainable_threshold: List[tf.Tensor] = []
     bias_weights: List[List[tf.Tensor]] = []
-    temperature_weights: List[tf.Tensor] = []
+
     for layer in fxp_model.layers:
-        if isinstance(layer, QuantizeWrapper) and isinstance(
-                layer.quantize_config, WeightQuantizeConfig):
-            # collect trainable weights per layer
-            layer_trainable_weights = layer.quantize_config.get_aux_variable()
-            layer_trainable_threshold = layer.quantize_config.get_quantization_variable()
+        if isinstance(layer, KerasQuantizationWrapper):
+            kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
+                                                                  fw_info=DEFAULT_KERAS_INFO)
+
+            # collect trainable weights per quantizer
+            if kernel_attribute not in layer.weights_quantizers:
+                Logger.error(f'{kernel_attribute} was not found in weight quantizers of layer {layer.layer}')
+
+            quantizer_trainable_weights = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.WEIGHTS)
+            quantizer_trainable_threshold = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.QPARAMS)
+            trainable_weights.append(quantizer_trainable_weights)
+            trainable_threshold.extend(quantizer_trainable_threshold)
 
             if add_bias:
                 kernel_ops_attrs = fw_info.kernel_ops_attributes_mapping.get(type(layer.layer))
                 use_bias = kernel_ops_attrs is not None and kernel_ops_attrs[0] is not None \
                            and layer.layer.get_config().get(USE_BIAS)
                 if use_bias is not None and use_bias:
                     bias_weights.append([layer.layer.bias])
-            trainable_weights.append(layer_trainable_weights)
-            trainable_threshold.extend(layer_trainable_threshold)
 
-    return trainable_weights, bias_weights, trainable_threshold, temperature_weights
+    return trainable_weights, bias_weights, trainable_threshold
 
 
 def get_weights_for_loss(fxp_model: Model) -> Tuple[List[list], List[list]]:
     """
     Get all float and quantized kernels for the GPTQ loss
 
     Args:
@@ -74,37 +80,19 @@
         A list of float kernels, each item is the float kernel of the layer
         A list of quantized kernels, each item is the quantized kernel of the layer
     """
 
     flp_weights_list = []
     fxp_weights_list = []
     for layer in fxp_model.layers:
-        if isinstance(layer, QuantizeWrapper) and isinstance(
-                layer.quantize_config, WeightQuantizeConfig):
+        if isinstance(layer, KerasQuantizationWrapper):
 
             # collect pairs of float and quantized weights per layer
             _layer_flp_weights, _layer_fxp_weights = [], []
-            for weight, quantizer, quantizer_vars in layer._weight_vars:
-                _layer_flp_weights.append(weight)
-                _layer_fxp_weights.append(quantizer(weight, training=False, weights=quantizer_vars))
+            for weight, quantizer_vars, quantizer in layer.get_weights_vars():
+                _layer_flp_weights.append(quantizer_vars)
+                _layer_fxp_weights.append(quantizer(training=False, inputs=quantizer_vars))
+
             flp_weights_list.append(_layer_flp_weights)
             fxp_weights_list.append(_layer_fxp_weights)
 
     return flp_weights_list, fxp_weights_list
-
-
-def get_soft_rounding_reg(fxp_model: Model) -> List[tf.Tensor]:
-    """
-    This function returns the soft quantizer regularization values for SoftRounding.
-
-    Args:
-        fxp_model: A model to be quantized with SoftRounding.
-
-    Returns: A list of tensors.
-    """
-
-    soft_reg_aux: List[tf.Tensor] = []
-    for layer in fxp_model.layers:
-        if isinstance(layer, QuantizeWrapper) and isinstance(
-                layer.quantize_config, WeightQuantizeConfig):
-            soft_reg_aux.append(layer.quantize_config.weight_quantizer.get_regularization())
-    return soft_reg_aux
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantization_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,44 +12,42 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable, Tuple
 from packaging import version
 
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import TENSORFLOW
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
 from model_compression_toolkit.core.common.user_info import UserInformation
 from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
-    MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import MixedPrecisionQuantizationConfigV2
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 
 LR_DEFAULT = 0.15
 LR_REST_DEFAULT = 1e-4
 LR_BIAS_DEFAULT = 1e-4
 LR_QUANTIZATION_PARAM_DEFAULT = 1e-3
 GPTQ_MOMENTUM = 0.9
 
-if common.constants.FOUND_TF:
+if FOUND_TF:
     import tensorflow as tf
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-    from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
+    from model_compression_toolkit.gptq.keras.gptq_keras_implementation import GPTQKerasImplemantation
     from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
     from tensorflow.keras.models import Model
     from model_compression_toolkit.gptq.keras.gptq_loss import GPTQMultipleTensorsLoss
-    from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
     from model_compression_toolkit import get_target_platform_capabilities
 
     # As from TF2.9 optimizers package is changed
     if version.parse(tf.__version__) < version.parse("2.9"):
         from keras.optimizer_v2.optimizer_v2 import OptimizerV2
     else:
@@ -58,67 +56,69 @@
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
     def get_keras_gptq_config(n_epochs: int,
                               optimizer: OptimizerV2 = tf.keras.optimizers.Adam(learning_rate=LR_DEFAULT),
                               optimizer_rest: OptimizerV2 = tf.keras.optimizers.Adam(learning_rate=LR_REST_DEFAULT),
                               loss: Callable = GPTQMultipleTensorsLoss(),
-                              log_function: Callable = None) -> GradientPTQConfigV2:
+                              log_function: Callable = None,
+                              use_hessian_based_weights: bool = True) -> GradientPTQConfigV2:
         """
         Create a GradientPTQConfigV2 instance for Keras models.
 
         args:
             n_epochs (int): Number of epochs for running the representative dataset for fine-tuning.
             optimizer (OptimizerV2): Keras optimizer to use for fine-tuning for auxiliry variable with a default learning rate set to 0.2.
             optimizer_rest (OptimizerV2): Keras optimizer to use for fine-tuning of the bias variable.
             loss (Callable): loss to use during fine-tuning. should accept 4 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors, the 3rd is a list of quantized weights and the 4th is a list of float weights.
             log_function (Callable): Function to log information about the gptq process.
+            use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
 
         returns:
             a GradientPTQConfigV2 object to use when fine-tuning the quantized model using gptq.
 
         Examples:
 
             Import MCT and TensorFlow:
 
             >>> import model_compression_toolkit as mct
             >>> import tensorflow as tf
 
             Create a GradientPTQConfigV2 to run for 5 epochs:
 
-            >>> gptq_conf = mct.get_keras_gptq_config(n_epochs=5)
+            >>> gptq_conf = mct.gptq.get_keras_gptq_config(n_epochs=5)
 
             Other Tensorflow optimizers can be passed:
 
-            >>> gptq_conf = mct.get_keras_gptq_config(n_epochs=3, optimizer=tf.keras.optimizers.Nadam())
+            >>> gptq_conf = mct.gptq.get_keras_gptq_config(n_epochs=3, optimizer=tf.keras.optimizers.Nadam())
 
             The configuration can be passed to :func:`~model_compression_toolkit.keras_post_training_quantization` in order to quantize a keras model using gptq.
 
         """
-        bias_optimizer = tf.keras.optimizers.SGD(learning_rate=LR_BIAS_DEFAULT, momentum=GPTQ_MOMENTUM)
+        bias_optimizer = tf.keras.optimizers.SGD(learning_rate=LR_BIAS_DEFAULT,
+                                                 momentum=GPTQ_MOMENTUM)
         return GradientPTQConfigV2(n_epochs,
                                    optimizer,
                                    optimizer_rest=optimizer_rest,
                                    loss=loss,
                                    log_function=log_function,
                                    train_bias=True,
-                                   quantization_parameters_learning=True,
-                                   optimizer_bias=bias_optimizer)
+                                   optimizer_bias=bias_optimizer,
+                                   use_hessian_based_weights=use_hessian_based_weights)
 
 
     def keras_gradient_post_training_quantization_experimental(in_model: Model,
                                                                representative_data_gen: Callable,
                                                                gptq_config: GradientPTQConfigV2,
                                                                gptq_representative_data_gen: Callable = None,
                                                                target_kpi: KPI = None,
                                                                core_config: CoreConfig = CoreConfig(),
                                                                fw_info: FrameworkInfo = DEFAULT_KERAS_INFO,
                                                                target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC,
-                                                               new_experimental_exporter: bool = False) -> \
-    Tuple[Model, UserInformation]:
+                                                               new_experimental_exporter: bool = True) -> Tuple[Model, UserInformation]:
         """
         Quantize a trained Keras model using post-training quantization. The model is quantized using a
         symmetric constraint quantization thresholds (power of two).
         The model is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
         For each possible bit width (per layer) a threshold is then being calculated using the collected
@@ -136,15 +136,15 @@
             representative_data_gen (Callable): Dataset used for calibration.
             gptq_config (GradientPTQConfigV2): Configuration for using gptq (e.g. optimizer).
             gptq_representative_data_gen (Callable): Dataset used for GPTQ training. If None defaults to representative_data_gen
             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
             fw_info (FrameworkInfo): Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.). `Default Keras info <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py>`_
             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-            new_experimental_exporter (bool): Whether exporting the quantized model using new exporter or not (in progress. Avoiding it for now is recommended).
+            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
 
             A quantized model and information the user may need to handle the quantized model.
 
         Examples:
 
@@ -160,52 +160,53 @@
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 224, 224, 3))]
 
             Create an MCT core config, containing the quantization configuration:
 
-            >>> config = mct.CoreConfig()
+            >>> config = mct.core.CoreConfig()
 
             If mixed precision is desired, create an MCT core config with a mixed-precision configuration, to quantize a model
             with different bitwidths for different layers.
             The candidates bitwidth for quantization should be defined in the target platform model:
 
-            >>> config = mct.CoreConfig(mixed_precision_config=mct.MixedPrecisionQuantizationConfigV2(num_of_images=1))
+            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfigV2(num_of_images=1))
 
             For mixed-precision set a target KPI object:
             Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
             while the bias will not):
 
-            >>> kpi = mct.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+            >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
             Create GPTQ config:
 
-            >>> gptq_config = mct.get_keras_gptq_config(n_epochs=1)
+            >>> gptq_config = mct.gptq.get_keras_gptq_config(n_epochs=1)
 
             Pass the model with the representative dataset generator to get a quantized model:
 
-            >>> quantized_model, quantization_info = mct.keras_gradient_post_training_quantization_experimental(model, repr_datagen, gptq_config, target_kpi=kpi, core_config=config)
+            >>> quantized_model, quantization_info = mct.gptq.keras_gradient_post_training_quantization_experimental(model, repr_datagen, gptq_config, target_kpi=kpi, core_config=config)
 
         """
         KerasModelValidation(model=in_model,
                              fw_info=fw_info).validate()
 
         if core_config.mixed_precision_enable:
             if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+                Logger.error("Given quantization config to mixed-precision facade is not of type "
                                     "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
                                     "API, or pass a valid mixed precision configuration.")  # pragma: no cover
 
-            common.Logger.info("Using experimental mixed-precision quantization. "
+            Logger.info("Using experimental mixed-precision quantization. "
                                "If you encounter an issue please file a bug.")
+
         tb_w = _init_tensorboard_writer(fw_info)
 
-        fw_impl = KerasImplementation()
+        fw_impl = GPTQKerasImplemantation()
 
         tg, bit_widths_config = core_runner(in_model=in_model,
                                             representative_data_gen=representative_data_gen,
                                             core_config=core_config,
                                             fw_info=fw_info,
                                             fw_impl=fw_impl,
                                             tpc=target_platform_capabilities,
@@ -221,16 +222,19 @@
                               fw_impl,
                               tb_w)
 
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen, tb_w, tg_gptq, fw_impl, fw_info)
 
         if new_experimental_exporter:
-            Logger.warning('Using new experimental exported models. '
-                           'Please do not use unless you are familiar with what you are doing')
+            Logger.warning('Using new experimental wrapped and ready for export models. To '
+                           'disable it, please set new_experimental_exporter to False when '
+                           'calling keras_gradient_post_training_quantization_experimental. '
+                           'If you encounter an issue please file a bug.')
+
             return get_exportable_keras_model(tg_gptq)
 
         return export_model(tg_gptq,
                             fw_info,
                             fw_impl,
                             tb_w,
                             bit_widths_config)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,8 +9,7 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from model_compression_toolkit.gptq.keras.quantizer.configs.weight_quantizer_gptq_config import WeightQuantizeConfig
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/base_quantizer_gptq_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,65 +1,49 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from typing import Union
 
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_config import QuantizeConfig
-from typing import Tuple, List, Any, Dict
-from tensorflow import Tensor
-import six, abc
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TORCH
 
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
+    TrainableQuantizerActivationConfig
+from model_compression_toolkit.trainable_infrastructure.pytorch.base_pytorch_quantizer import \
+    BasePytorchTrainableQuantizer
 
-@six.add_metaclass(abc.ABCMeta)
-class BaseQuantizeConfig(QuantizeConfig):
-    """
-    Base QuantizeConfig to define extra API methods needed by the GPTQ post-processing.
-    """
+if FOUND_TORCH:
 
-    @abc.abstractmethod
-    def get_quantization_variable(self):
+    class BasePytorchQATTrainableQuantizer(BasePytorchTrainableQuantizer):
         """
-        A Functions that get the quantization parameters such as threshold, min, max ,etc.
-
-        Returns: A list of trainable variable
-
-        """
-
-    @abc.abstractmethod
-    def update_layer_quantization_params(self, layer) -> Tuple[Dict[str, Any],
-                                                               Dict[str, Any],
-                                                               Dict[str, Any]]:
-        """
-        A Function to calculate the needed change in attributes in NodeQuantizationConfig after retraining.
-        Usually a function of the config quantizers.
-
-        Args:
-            layer: layer being quantized.
-
-        Returns:
-            3 dictionaries of attributes the quantize_config retraining has changed during GPTQ retraining.
-            Keys must match NodeQuantizationConfig attributes:
-            1. layer weights
-            2. weight quantization config attributes
-            3. activation quantization config attributes
-
+        A base class for trainable Keras quantizer for QAT.
         """
 
-    @abc.abstractmethod
-    def get_trainable_quantizer_parameters(self) -> List[Tensor]:
-        """
-        A function to get a list trainable of trainable parameters for GPTQ retraining from config quantizers
+        def __init__(self,
+                     quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
+            """
+            Initializes BasePytorchQATTrainableQuantizer object.
 
-        Returns:
-            A list of trainable Tensors
+            Args:
+                quantization_config: quantizer config class contains all the information about a quantizer configuration.
+            """
+            super().__init__(quantization_config)
 
-        """
+else:
+    class BasePytorchQATTrainableQuantizer(BasePytorchTrainableQuantizer):
+        def __init__(self,
+                     quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
+            super().__init__(quantization_config)
+            Logger.critical('Installing Pytorch is mandatory '
+                            'when using BasePytorchQATTrainableQuantizer. '
+                            'Could not find torch package.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/configs/weight_quantizer_gptq_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,219 +1,224 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import List, Tuple, Any, Dict
-
-from tensorflow import Tensor
 import tensorflow as tf
-from packaging import version
-
-# As from Tensorflow 2.6, keras is a separate package and some classes should be imported differently.
-from model_compression_toolkit.gptq.keras.quantizer.soft_rounding.symmetric_soft_quantizer import SymmetricSoftRounding
-
-if version.parse(tf.__version__) < version.parse("2.6"):
-    from tensorflow.python.keras.layers import Layer
-else:
-    from keras.engine.base_layer import Layer
+import numpy as np
 
-from tensorflow.python.training.tracking.data_structures import ListWrapper
-from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer
+from model_compression_toolkit.gptq import RoundingType
+from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from model_compression_toolkit.gptq.common.gptq_constants import \
+    SOFT_ROUNDING_GAMMA, SOFT_ROUNDING_ZETA, AUXVAR
+from model_compression_toolkit.gptq.keras.quantizer import quant_utils as qutils
+from typing import Dict, Any
+from model_compression_toolkit.constants import RANGE_MIN, RANGE_MAX
+from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig
+from mct_quantizers import mark_quantizer
+from model_compression_toolkit.trainable_infrastructure.common.quant_utils import \
+    get_threshold_reshape_shape
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+
+
+def soft_rounding_uniform_quantizer(input_tensor: tf.Tensor,
+                                    auxvar_tensor: tf.Variable,
+                                    min_tensor: tf.Tensor,
+                                    max_tensor: tf.Tensor,
+                                    num_bits: int) -> tf.Tensor:
+    """
+    Quantize a tensor uniformly for GPTQ quantizers.
 
-from model_compression_toolkit.gptq.keras.quantizer.configs.base_quantizer_gptq_config import BaseQuantizeConfig
-from model_compression_toolkit.core.keras.constants import KERNEL
+    Args:
+        input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
+        auxvar_tensor: Tensor that manifests the bit shift of the quantized weights due to gptq training.
+        min_tensor: Tensor with values to compute the min threshold.
+        max_tensor: Tensor with values to compute the max threshold.
+        num_bits: Num of bits to use.
 
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2, RoundingType
-from model_compression_toolkit.gptq.keras.quantizer.ste_rounding.symmetric_ste import STEWeightQuantizer
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import QuantizationMethod
-from model_compression_toolkit.core.common.constants import THRESHOLD, RANGE_MAX, RANGE_MIN
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeWeightsQuantizationConfig
-from model_compression_toolkit.gptq.common import gptq_constants
+    Returns:
+        A quantized tensor.
+    """
+    # adjusts the quantization range so the quantization grid includes zero.
+    min_range, max_range = qutils.fix_range_to_include_zero(min_tensor, max_tensor, num_bits)
+    delta = qutils.calculate_delta_uniform(min_range, max_range, num_bits)
+    input_tensor_int = qutils.ste_floor((input_tensor - min_range) / delta)
+    tensor_q = input_tensor_int + auxvar_tensor
+    return delta * qutils.ste_clip(tensor_q,
+                                   min_val=0,
+                                   max_val=2 ** num_bits - 1) + min_range
 
 
-class WeightQuantizeConfig(BaseQuantizeConfig):
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.UNIFORM],
+                quantizer_type=RoundingType.SoftQuantizer)
+class UniformSoftRoundingGPTQ(BaseKerasGPTQTrainableQuantizer):
     """
-    QuantizeConfig to quantize the weights of a layer using a TrainableQuantizer.
+    Trainable uniform quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
-    def __init__(self, weight_attrs: List[str],
-                 final_weights_quantization_cfg: NodeWeightsQuantizationConfig,
-                 gptq_config: GradientPTQConfigV2):
-        """
-        Initialize a TrainableQuantizer and set as the weights quantizer.
-        Args:
-            weight_attrs: Attributes of the layer's weights to quantize.
-            final_weights_quantization_cfg: quantization config of the current layer.
-            gptq_config: A GPTQ configuration calls.
-        """
-
-        num_bits = final_weights_quantization_cfg.weights_n_bits
-        weight_channel_axis = final_weights_quantization_cfg.weights_channels_axis
-        max_lsbs_change_map = gptq_config.lsb_change_per_bit_width
-        self.weight_attrs = weight_attrs
-        self.final_weights_quantization_cfg = final_weights_quantization_cfg
-        self.gptq_config = gptq_config
-
-        if final_weights_quantization_cfg.weights_quantization_method in [QuantizationMethod.SYMMETRIC,
-                                                                          QuantizationMethod.POWER_OF_TWO]:
-            is_power_of_two = QuantizationMethod.POWER_OF_TWO == final_weights_quantization_cfg.weights_quantization_method
-            threshold_values = final_weights_quantization_cfg.weights_quantization_params.get(THRESHOLD)
-            if gptq_config.rounding_type == RoundingType.STE:
-                self.weight_quantizer = STEWeightQuantizer(num_bits=num_bits,
-                                                           per_channel=len(
-                                                               threshold_values.flatten()) > 1,
-                                                           threshold_values=threshold_values,
-                                                           signed=True,
-                                                           power_of_two=is_power_of_two,
-                                                           quantization_axis=weight_channel_axis,
-                                                           max_lsbs_change_map=max_lsbs_change_map)
-            elif gptq_config.rounding_type == RoundingType.SoftQuantizer:
-                self.weight_quantizer = SymmetricSoftRounding(num_bits=num_bits,
-                                                              per_channel=len(
-                                                                  threshold_values.flatten()) > 1,
-                                                              threshold_values=threshold_values,
-                                                              signed=True,
-                                                              n_batches=gptq_config.quantizer_config.n_batches,
-                                                              quantization_parameter_learning=gptq_config.quantization_parameters_learning,
-                                                              quantization_axis=weight_channel_axis,
-                                                              n_epochs=gptq_config.n_epochs,
-                                                              power_of_two=is_power_of_two)
-            else:
-                common.Logger.error(
-                    f"For quantization method {final_weights_quantization_cfg.weights_quantization_method}, "
-                    f"GPTQ Rounding type {gptq_config.rounding_type} is not supported")
-
-    def get_weights_and_quantizers(self, layer: Layer) -> List[Tuple[Tensor, Quantizer]]:
-        """
-        Get a list of tuples with weights and the weight quantizer.
-        The layer's attributes are used to get the weights.
-        Args:
-            layer: The layer the WeightQuantizeConfig wraps.
-
-        Returns:
-            List of tuples of the layer's weights and the weight quantizer.
+    def __init__(self,
+                 quantization_config: TrainableQuantizerWeightsConfig,
+                 quantization_parameter_learning: bool = False):
         """
-        return [(getattr(layer, weight_attr), self.weight_quantizer)
-                for weight_attr in self.weight_attrs]
+        Initialize a UniformSoftRoundingGPTQ object with parameters to use
+        for the quantization.
 
-    def get_activations_and_quantizers(self, layer: Layer) -> list:
-        return []
-
-    def set_quantize_weights(self, layer: Layer, quantize_weights: List[Tensor]):
-        """
-        Set the layer weights with new passed weights.
         Args:
-            layer: Layer to set its attributes.
-            quantize_weights: Quantized weights to set as new weights.
-
+            quantization_config: Trainable weight quantizer config.
+            quantization_parameter_learning: Whether to train the quantization threshold.
         """
-        if len(self.weight_attrs) != len(quantize_weights):
-            raise ValueError(
-                '`set_quantize_weights` called on layer {} with {} '
-                'weight parameters, but layer expects {} values.'.format(
-                    layer.name, len(quantize_weights), len(self.weight_attrs)))  # pragma: no cover
-
-        for weight_attr, weight in zip(self.weight_attrs, quantize_weights):
-            current_weight = getattr(layer, weight_attr)
-            if current_weight.shape != weight.shape:
-                raise ValueError('Existing layer weight shape {} is incompatible with'
-                                 'provided weight shape {}'.format(
-                    current_weight.shape, weight.shape))  # pragma: no cover
-
-            setattr(layer, weight_attr, weight)
-
-    def set_quantize_activations(self, layer, quantize_activations: ListWrapper):
-        pass
-
-    def get_output_quantizers(self, layer: Layer) -> list:
-        return []
-
-    @classmethod
-    def from_config(cls, config: dict):
+        super().__init__(quantization_config)
+        self.num_bits = quantization_config.weights_n_bits
+        self.per_channel = quantization_config.weights_per_channel_threshold
+
+        self.min_values = quantization_config.weights_quantization_params[RANGE_MIN]
+        self.max_values = quantization_config.weights_quantization_params[RANGE_MAX]
+
+        self.quantization_axis = quantization_config.weights_channels_axis
+        assert quantization_parameter_learning is False, \
+            "Quantization parameters learning in UniformSoftRoundingGPTQ not implemented yet"
+        self.quantization_parameter_learning = quantization_parameter_learning
+        self.num_channels = self.min_values.shape[self.quantization_axis] if self.per_channel else 1
+
+        # gamma and zeta are stretch parameters for computing the rectified sigmoid function.
+        # See: https://arxiv.org/pdf/2004.10568.pdf
+        self.gamma = SOFT_ROUNDING_GAMMA
+        self.zeta = SOFT_ROUNDING_ZETA
+
+    def initialize_quantization(self,
+                                tensor_shape: Any,
+                                name: str,
+                                layer: Any):
         """
-        Instantiates a `WeightQuantizeConfig` from its config.
+        Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
-            config: Output of `get_config()`.
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
+        """
+
+        if self.per_channel:
+            reshape_shape = get_threshold_reshape_shape(tensor_shape,
+                                                        quant_axis=self.quantization_axis,
+                                                        quant_axis_dim=self.num_channels)
+        else:
+            reshape_shape = [self.num_channels]
+
+        min_tensor = layer.add_weight(
+            f"{name}_{FQ_MIN}",
+            shape=reshape_shape,
+            initializer=tf.keras.initializers.Constant(1.0),
+            trainable=False)
+        min_tensor.assign(self.min_values.reshape(reshape_shape))
+
+        max_tensor = layer.add_weight(
+            f"{name}_{FQ_MAX}",
+            shape=reshape_shape,
+            initializer=tf.keras.initializers.Constant(1.0),
+            trainable=False)
+        max_tensor.assign(self.max_values.reshape(reshape_shape))
+
+        w = getattr(layer.layer, name)
+        auxvar_tensor = layer.add_weight(
+            f"{name}_{AUXVAR}",
+            shape=list(w.shape),
+            initializer=tf.keras.initializers.Constant(0.0),
+            trainable=True)
+
+        w = layer.layer.depthwise_kernel if isinstance(layer.layer, (tf.keras.layers.DepthwiseConv2D,
+                                                                     tf.keras.layers.DepthwiseConv1D)) \
+            else layer.layer.kernel
+        delta = qutils.calculate_delta_uniform(min_tensor, max_tensor, self.num_bits)
+        w_clipped_normed = qutils.clip((w - min_tensor)/ delta, 0, 2 ** self.num_bits - 1)
+        rest = w_clipped_normed - tf.floor(w_clipped_normed)  # rest of rounding [0, 1)
+        alpha = -qutils.safe_log((self.zeta - self.gamma) / (rest - self.gamma) - 1, 1e-16)  # => sigmoid(alpha) = rest
+        auxvar_tensor.assign(alpha)
+
+        # Add quantization variables
+        self.add_quantizer_variable(AUXVAR, auxvar_tensor, VariableGroup.WEIGHTS)
+        self.add_quantizer_variable(RANGE_MIN, min_tensor, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(RANGE_MAX, max_tensor, VariableGroup.QPARAMS)
 
-        Returns:
-            A `WeightQuantizeConfig` instance.
+    def get_soft_targets(self) -> tf.Tensor:
         """
+        Computes the rectified sigmoid function for the quantization target parameters.
 
-        return cls(**config)
+        Returns:
+            A tensor with the soft rounding targets values.
 
-    def get_config(self) -> Dict[str, Any]:
-        """
-        Returns: The WeightQuantizeConfig configuration.
         """
-        return {
-            'weight_attrs': self.weight_attrs,
-            'final_weights_quantization_cfg': self.final_weights_quantization_cfg,
-            'gptq_config': self.gptq_config,
-        }
+        return qutils.clip(
+            tf.sigmoid(self.get_quantizer_variable(AUXVAR)) * (self.zeta - self.gamma) + self.gamma, 1, 0)
 
-    def update_layer_quantization_params(self, layer: Layer) -> (Dict[str, tf.Tensor], Dict[str, Dict], Dict):
+    def __call__(self,
+                 inputs: tf.Tensor,
+                 training: bool):
         """
-        A Function to calculate the needed change in attributes in NodeQuantizationConfig after retraining.
-        Usually a function of the config quantizers.
+        Quantize a tensor.
 
         Args:
-            layer: layer being quantized.
+            inputs: Input tensor to quantize.
+            training: Whether the graph is in training mode.
 
         Returns:
-            3 dictionaries describing the change in layer's weights, weights config, activation config
-            that changed during GPTQ retraining.
-            Keys must match NodeQuantizationConfig attributes
-
+            The quantized tensor.
         """
-        weights = {}
-        for weight, quantizer, quantizer_vars in layer._weight_vars:
-            weights.update({KERNEL: quantizer(weight, training=False, weights=quantizer_vars)})
 
-        quant_config = {gptq_constants.WEIGHTS_QUANTIZATION_PARAMS: self.weight_quantizer.get_quant_config(layer)}
+        min_tensor = self.get_quantizer_variable(RANGE_MIN)
+        max_tensor = self.get_quantizer_variable(RANGE_MAX)
 
-        return weights, quant_config, {}
-
-    def get_trainable_quantizer_parameters(self) -> List[tf.Tensor]:
-        """
-        A function to get a list trainable of trainable parameters for GPTQ retraining from config quantizers
+        #####################################################
+        # Soft Rounding
+        #####################################################
+        aux_var = self.get_soft_targets()
+        if not training:
+            aux_var = tf.cast(tf.math.greater_equal(aux_var, 0.5), tf.float32)
+
+        if self.per_channel:
+            reshape_shape = get_threshold_reshape_shape(inputs.shape,
+                                                        quant_axis=self.quantization_axis,
+                                                        quant_axis_dim=-1)
+
+            #####################################################
+            # Quantized Input
+            #####################################################
+            q_tensor = soft_rounding_uniform_quantizer(input_tensor=inputs,
+                                                       auxvar_tensor=aux_var,
+                                                       min_tensor=tf.reshape(min_tensor, reshape_shape),
+                                                       max_tensor=tf.reshape(max_tensor, reshape_shape),
+                                                       num_bits=self.num_bits)
+
+        else:
+            q_tensor = soft_rounding_uniform_quantizer(input_tensor=inputs,
+                                                       auxvar_tensor=aux_var,
+                                                       min_tensor=min_tensor,
+                                                       max_tensor=max_tensor,
+                                                       num_bits=self.num_bits)
 
-        Returns:
-            A list of trainable Tensors
+        return q_tensor
 
+    def get_quant_config(self) -> Dict[str, np.ndarray]:
         """
-        return self.weight_quantizer.get_trainable_parameters()
-
-    def get_aux_variable(self) -> List[tf.Tensor]:
-        return [self.weight_quantizer.get_aux_variable()]
-
-    def get_quantization_variable(self) -> List[tf.Tensor]:
-        return self.weight_quantizer.get_quantization_variable()
+        Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
 
-    def __eq__(self, other: Any) -> bool:
-        """
-        Check whether it equals to another object or not.
+        Returns:
+            A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
+            Keys must match NodeQuantizationConfig attributes
         """
-        if not isinstance(other, WeightQuantizeConfig):
-            return False
-
-        return (self.weight_attrs == other.weight_attrs and
-                self.weight_quantizer == other.weight_quantizer and
-                self.gptq_config == other.gptq_config)
 
-    def __ne__(self, other: Any) -> bool:
-        """
-        Check whether it differs from another object or not.
-        """
-        return not self.__eq__(other)
+        return {RANGE_MIN: self.get_quantizer_variable(RANGE_MIN).numpy(),
+                RANGE_MAX: self.get_quantizer_variable(RANGE_MAX).numpy()}
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/kernel_functions.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/quant_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,50 +1,36 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import tensorflow as tf
-from model_compression_toolkit.core.keras.constants import KERNEL
+from typing import Tuple, List
 
 
-def get_kernel(weights_list: list) -> tf.Tensor:
+def get_threshold_reshape_shape(tensor_shape: Tuple, quant_axis: int, quant_axis_dim: int) -> List[int]:
     """
-    This function a list of weights and return the kernel
-    Args:
-        weights_list:  A list of Tensors
+    Gets a shape that contains 1 in all axis except the quantization axis, to adjust the threshold tensor for
+    per-channel quantization.
 
-    Returns: The kernel tensor.
+    Args:
+        tensor_shape: The shape of th
 
-    """
-    for w in weights_list:
-        if KERNEL in w.name:
-            return w
-    raise Exception("Can't find kernel variable")
+        e tensor to be quantized.
+        quant_axis: The axis along which the quantization happens (usually the tensor's channel axis).
+        quant_axis_dim: The dimension of the quantization axis.
 
+    Returns: A shape to reshape the threshold tensor according to.
 
-def threshold_reshape(threshold_tensor: tf.Tensor, input_w: tf.Tensor, in_quantization_axis: int) -> tf.Tensor:
     """
-    This function take a threshold tensor and re-aline it to the weight tensor channel axis.
-    Args:
-        threshold_tensor: A tensor of threshold
-        input_w: A weight tensor
-        in_quantization_axis: A int value that represent the channel axis.
-
-    Returns: A reshape tensor of threshold.
+    n_axis = len(tensor_shape)
+    quantization_axis = n_axis + quant_axis if quant_axis < 0 else quant_axis
 
-    """
-    input_shape = input_w.shape
-    n_axis = len(input_shape)
-    quantization_axis = n_axis + in_quantization_axis if in_quantization_axis < 0 else in_quantization_axis
-    reshape_shape = [-1 if i == quantization_axis else 1 for i in range(n_axis)]
-    ptq_threshold_tensor = tf.reshape(threshold_tensor, reshape_shape)
-    return ptq_threshold_tensor
+    return [quant_axis_dim if i == quantization_axis else 1 for i in range(n_axis)]
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/quant_utils.py`

 * *Files 20% similar despite different names*

```diff
@@ -10,58 +10,34 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import tensorflow as tf
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD
+from model_compression_toolkit.constants import MIN_THRESHOLD
 from typing import Tuple
 
 
-def symmetric_rounding_quantizer(input_tensor: tf.Tensor,
-                                 auxvar_tensor: tf.Variable,
-                                 threshold_tensor: tf.Tensor,
-                                 num_bits: int,
-                                 signed: bool,
-                                 power_of_two: bool) -> tf.Tensor:
-    """
-    Quantize a tensor symmetrically for GPTQ quantizers.
-
-    Args:
-        input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
-        auxvar_tensor: Tensor that manifests the bit shift the weight due to gptq.
-        threshold_tensor: Tensor with values to compute the threshold.
-        num_bits: Num of bits to use.
-        signed: Signedness of the quantization range.
-        power_of_two: Whether the threshold should be constrained or not.
-
-    Returns:
-        A quantized tensor.
-    """
-
-    if power_of_two:
-        threshold_tensor = power_of_two_max(threshold_tensor)
-    delta = calculate_delta(threshold_tensor, num_bits, signed)
-    input_tensor = tf.stop_gradient(input_tensor)
-    input_tensor_int = tf.floor(input_tensor / delta)
-    tensor_q = input_tensor_int + auxvar_tensor
-    min_int = -int(signed) * (2 ** (num_bits - int(signed)))
-    max_int = (2 ** (num_bits - int(signed))) - 1
-    return delta * clip(tensor_q, max_val=max_int, min_val=min_int)
-
-
 def ste_ceil(x: tf.Tensor) -> tf.Tensor:
     """
     Return the ceil values of a tensor.
     """
     error = tf.stop_gradient(tf.math.ceil(x) - x)
     return error + x
 
 
+def ste_floor(x: tf.Tensor) -> tf.Tensor:
+    """
+    Return the floor values of a tensor.
+    """
+    error = tf.stop_gradient(tf.math.floor(x) - x)
+    return error + x
+
+
 def safe_log(x: tf.Tensor, eps: float) -> tf.Tensor:
     """
     Computes log function of x unless x is smaller than some small value, so the log function would not fail.
 
     Args:
         x: input variable.
         eps: limit value.
@@ -100,26 +76,21 @@
                     signed: bool) -> tf.Tensor:
     """
     Compute the step size for the quantization.
     """
     return max_tensor / (2 ** (num_bits - int(signed)))
 
 
-def adjustable_steps(x: tf.Variable, t: float) -> tf.Tensor:
+def calculate_delta_uniform(min_tensor: tf.Tensor,
+                            max_tensor: tf.Tensor,
+                            num_bits: int) -> tf.Tensor:
     """
-    A function to gradually quantize a float variable to an integer of values [-1, 0 ,1]
-    Args:
-        x: input float variable
-        t: temperature to control quantization
-
-    Returns:
-        semi-quantized variable
-
+    Compute the step size for the uniform quantization.
     """
-    return tf.sigmoid(tf.add(x, 1) / t) + tf.sigmoid(tf.add(x, -1) / t) - 1
+    return (max_tensor-min_tensor) / (2 ** num_bits - 1)
 
 
 def ste_clip(x: [tf.Tensor, tf.Variable], max_val=1, min_val=None) -> tf.Tensor:
     """
     clip a variable between fixed values such that min_val<=output<=max_val
     Args:
         x: input variable
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py`

 * *Files 15% similar despite different names*

```diff
@@ -12,349 +12,250 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 import tensorflow as tf
 import numpy as np
 
+from model_compression_toolkit.gptq import RoundingType
 from model_compression_toolkit.core.common import max_power_of_two
-from model_compression_toolkit.gptq.common.gptq_constants import PTQ_THRESHOLD, SCALE_PTQ, N_EPOCHS, \
-    MAX_ITERATIONS_DEFAULT, SOFT_ROUNDING_GAMMA, SOFT_ROUNDING_ZETA, SOFT_ROUNDING_BETA, GPTQ_ITER, AUXVAR
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget
+from model_compression_toolkit.gptq.common.gptq_constants import PTQ_THRESHOLD, SCALE_PTQ, \
+    SOFT_ROUNDING_GAMMA, SOFT_ROUNDING_ZETA, AUXVAR
 from model_compression_toolkit.gptq.keras.quantizer import quant_utils as qutils
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
-from tensorflow.python.framework.tensor_shape import TensorShape
-from typing import Dict, Any, List
-from model_compression_toolkit.core.common.constants import THRESHOLD, MIN_THRESHOLD
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.keras.quantizer.base_quantizer import BaseTrainableQuantizer
-
-
-
-class LinearTempDecay:
+from typing import Dict, Any
+from model_compression_toolkit.constants import THRESHOLD, MIN_THRESHOLD
+from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
+from model_compression_toolkit.gptq.keras.quantizer.quant_utils import power_of_two_max, clip, calculate_delta
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig
+from mct_quantizers import mark_quantizer
+from model_compression_toolkit.trainable_infrastructure.common.quant_utils import \
+    get_threshold_reshape_shape
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+
+
+def soft_rounding_symmetric_quantizer(input_tensor: tf.Tensor,
+                                      auxvar_tensor: tf.Variable,
+                                      threshold_tensor: tf.Tensor,
+                                      num_bits: int,
+                                      signed: bool,
+                                      power_of_two: bool) -> tf.Tensor:
     """
-    Annealing process for the soft quantizer regularization temperature term.
-    """
-
-    def __init__(self, t_max: int, rel_start_decay: float = 0.2, start_b: int = 20, end_b: int = 2):
-        """
-        Initializes a LinearTempDecay object.
-
-        Args:
-            t_max: maximal time step.
-            rel_start_decay: Decay step size at the beginning of the process.
-            start_b: Starting value of the regularization term.
-            end_b: Target value of the regularization term.
-        """
+    Quantize a tensor symmetrically for GPTQ quantizers.
 
-        self.t_max = t_max
-        self.start_decay = rel_start_decay * t_max
-        self.start_b = start_b
-        self.end_b = end_b
+    Args:
+        input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
+        auxvar_tensor: Tensor that manifests the bit shift of the quantized weights due to gptq training.
+        threshold_tensor: Tensor with values to compute the threshold.
+        num_bits: Num of bits to use.
+        signed: Signedness of the quantization range.
+        power_of_two: Whether the threshold should be constrained or not.
 
-    def __call__(self, t: int) -> float:
-        """
-        Cosine annealing scheduler for soft quantizer regularization temperature term.
-
-        Args:
-            t: The current time step.
-
-        Returns: Scheduled temperature.
-        """
-
-        is_before_start_decay = tf.cast(t < self.start_decay, tf.float32)
-
-        rel_t = (t - self.start_decay) / (self.t_max - self.start_decay)
+    Returns:
+        A quantized tensor.
+    """
 
-        return self.start_b * is_before_start_decay + \
-               (1 - is_before_start_decay) * \
-               (self.end_b + (self.start_b - self.end_b) * tf.math.maximum(0.0, (1 - rel_t)))
+    if power_of_two:
+        threshold_tensor = power_of_two_max(threshold_tensor)
+    delta = calculate_delta(threshold_tensor, num_bits, signed)
+    input_tensor = tf.stop_gradient(input_tensor)
+    input_tensor_int = tf.floor(input_tensor / delta)
+    tensor_q = input_tensor_int + auxvar_tensor
+    min_int = -int(signed) * (2 ** (num_bits - int(signed)))
+    max_int = (2 ** (num_bits - int(signed))) - 1
+    return delta * clip(tensor_q, max_val=max_int, min_val=min_int)
 
 
-class SymmetricSoftRounding(BaseTrainableQuantizer):
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
+                quantizer_type=RoundingType.SoftQuantizer)
+class SymmetricSoftRoundingGPTQ(BaseKerasGPTQTrainableQuantizer):
     """
     Trainable symmetric quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
-    def __init__(self, num_bits: int,
-                 per_channel: bool,
-                 signed: bool,
-                 power_of_two: bool,
-                 n_batches: int,
-                 quantization_parameter_learning: bool,
-                 threshold_values: np.ndarray,
-                 quantization_axis: int = -1,
-                 n_epochs: int = N_EPOCHS):
+    def __init__(self,
+                 quantization_config: TrainableQuantizerWeightsConfig,
+                 quantization_parameter_learning: bool = False):
         """
-        Initialize a SymmetricSoftRounding object with parameters to use
+        Initialize a SymmetricSoftRoundingGPTQ object with parameters to use
         for the quantization.
+
         Args:
-            num_bits: Number of bits to use for the quantization.
-            per_channel: Whether to quantize per-channel or per-tensor.
-            signed: Signedness to use for the quantization range.
-            power_of_two: Whether the threshold should be constrained or not.
-            n_batches: The expected number of batches for each trainig epoch.
+            quantization_config: Trainable weights quantizer config.
             quantization_parameter_learning: Whether to train the quantization threshold.
-            threshold_values: Threshold to use for the quantization.
-            quantization_axis: Axis of tensor to use for the quantization.
-            n_epochs: Number of epochs to run training for.
         """
+        super().__init__(quantization_config)
+        self.num_bits = quantization_config.weights_n_bits
+        self.per_channel = quantization_config.weights_per_channel_threshold
 
-        super().__init__()
-        self.num_bits = num_bits
-        self.per_channel = per_channel
-        self.signed = signed
-        self.power_of_two = power_of_two
-        self.quantization_parameter_learning = quantization_parameter_learning
-        self.quantization_axis = quantization_axis
+        threshold_values = quantization_config.weights_quantization_params[THRESHOLD]
         self.threshold_shape = np.asarray(threshold_values).shape
         self.threshold_values = np.reshape(np.asarray(threshold_values), [-1]) if self.per_channel else np.asarray(
             threshold_values)
+
+        self.quantization_axis = quantization_config.weights_channels_axis
+        self.power_of_two = quantization_config.weights_quantization_method == QuantizationMethod.POWER_OF_TWO
+        self.quantization_parameter_learning = quantization_parameter_learning
         self.num_channels = len(self.threshold_values) if self.per_channel else 1
 
         # gamma and zeta are stretch parameters for computing the rectified sigmoind function.
-        # beta is used to set the regularization term.
         # See: https://arxiv.org/pdf/2004.10568.pdf
         self.gamma = SOFT_ROUNDING_GAMMA
         self.zeta = SOFT_ROUNDING_ZETA
-        self.beta = SOFT_ROUNDING_BETA
 
         self.quantizer_parameters = {}
 
-        # Initializing the temperature decay according to the number of expected gradient steps
-        if n_batches is None:
-            Logger.warning(f"Number of batches is not set correctly for the Soft Quantizer. A default value of "  # pragma: no cover
-                           f"{MAX_ITERATIONS_DEFAULT} is used to set the temperature decay which may affect the results.")
-
-        init_decay = MAX_ITERATIONS_DEFAULT if n_batches is None else n_epochs * n_batches
-        self.linear_decay = LinearTempDecay(init_decay)
-
-    def build(self,
-              tensor_shape: TensorShape,
-              name: str,
-              layer: QuantizeWrapper) -> Dict[str, tf.Variable]:
+    def initialize_quantization(self,
+                                tensor_shape: Any,
+                                name: str,
+                                layer: Any):
         """
-        Add variables to the quantizer.
+        Add quantizer parameters to the quantizer parameters dictionary
 
         Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
-
-        Returns:
-            Dictionary of new variables.
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
 
-        super().build(tensor_shape, name, layer)
-
         if self.per_channel:
-            reshape_shape = self._get_threshold_reshape_shape(tensor_shape, quant_axis_dim=self.num_channels)
+            reshape_shape = get_threshold_reshape_shape(tensor_shape,
+                                                        quant_axis=self.quantization_axis,
+                                                        quant_axis_dim=self.num_channels)
         else:
             reshape_shape = [self.num_channels]
 
-        ar_iter = layer.add_weight(
-            f"{name}_{GPTQ_ITER}",
-            shape=(),
-            initializer=tf.keras.initializers.Constant(0.0),
-            trainable=False)
-
         ptq_threshold_tensor = layer.add_weight(
             f"{name}_{PTQ_THRESHOLD}",
             shape=reshape_shape,
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         ptq_threshold_tensor.assign(self.threshold_values.reshape(reshape_shape))
 
         w = getattr(layer.layer, name)
         auxvar_tensor = layer.add_weight(
             f"{name}_{AUXVAR}",
-            shape=[*w.shape],
+            shape=list(w.shape),
             initializer=tf.keras.initializers.Constant(0.0),
             trainable=True)
 
-        delta = qutils.calculate_delta(ptq_threshold_tensor, self.num_bits, self.signed)
+        delta = qutils.calculate_delta(ptq_threshold_tensor, self.num_bits, signed=True)
         w_floor = tf.floor(w / delta)
         rest = (w / delta) - w_floor  # rest of rounding [0, 1)
         # Note that (rest - self.gamma) can't be zero since rest is positive and gamma is negative, so the division
         # is safe
         alpha = -qutils.safe_log((self.zeta - self.gamma) / (rest - self.gamma) - 1, 1e-16)  # => sigmoid(alpha) = rest
 
         auxvar_tensor.assign(alpha)
 
-        self.quantizer_parameters.update({AUXVAR: auxvar_tensor,
-                                          PTQ_THRESHOLD: ptq_threshold_tensor,
-                                          GPTQ_ITER: ar_iter})
+        # Add quantization variables
+        self.add_quantizer_variable(AUXVAR, auxvar_tensor, VariableGroup.WEIGHTS)
+        self.add_quantizer_variable(PTQ_THRESHOLD, ptq_threshold_tensor, VariableGroup.QPARAMS)
 
-        if self.quantization_parameter_learning:
+        if self.quantization_parameter_learning and not self.power_of_two:
             scale = layer.add_weight(
                 f"{name}_{SCALE_PTQ}",
                 shape=self.num_channels,
                 initializer=tf.keras.initializers.Constant(1.0),
                 trainable=True)
-            self.quantizer_parameters.update({SCALE_PTQ: scale})
-
-        return self.quantizer_parameters
-
-    def get_quantization_variable(self) -> List[tf.Tensor]:
-        """
-        Returns:
-            A list of the quantization parameters (if there are defined parameters for the quantizer).
-        """
-
-        if self.quantization_parameter_learning and not self.power_of_two:
-            return [self.quantizer_parameters[SCALE_PTQ]]
-        else:
-            return []
-
-    def get_regularization(self) -> tf.Tensor:
-        """
-        Computes the regularization term for the soft rounding loss.
-
-        Returns:
-            regularization term.
-        """
-
-        st = self.get_soft_targets()
-        b = self.linear_decay(self.ar_iter.value())
-        return tf.reduce_sum(1 - tf.pow(tf.math.abs(st - .5) * 2, b))
-
-    def get_trainable_parameters(self) -> List[tf.Tensor]:
-        """
-        A function to get a list trainable of trainable parameters of the quantizer for GPTQ retraining
-
-        Returns:
-            A list of trainable Tensors
-        """
-        return [t for t in self.quantizer_parameters.values() if t.trainable]
-
-    def get_config(self) -> Dict[str, Any]:
-        """
-        Returns:
-            Configuration of SymmetricSoftRounding.
-        """
-
-        return {
-            'num_bits': self.num_bits,
-            'per_channel': self.per_channel,
-        }
+            self.add_quantizer_variable(SCALE_PTQ, scale, VariableGroup.QPARAMS)
 
     def get_soft_targets(self) -> tf.Tensor:
         """
         Computes the rectified sigmoid function for the quantization target parameters.
 
         Returns:
             A tensor with the soft rounding targets values.
 
         """
         return qutils.clip(
-            tf.sigmoid(self.quantizer_parameters[AUXVAR]) * (self.zeta - self.gamma) + self.gamma, 1, 0)
+            tf.sigmoid(self.get_quantizer_variable(AUXVAR)) * (self.zeta - self.gamma) + self.gamma, 1, 0)
 
-    def get_aux_variable(self) -> tf.Tensor:
-        """
-        Returns:
-            The auxiliary variable of the rounding learning.
-        """
-        return self.quantizer_parameters[AUXVAR]
-
-    def __call__(self, inputs: tf.Tensor,
-                 training: bool,
-                 weights: Dict[str, tf.Variable],
-                 **kwargs: Dict[str, Any]) -> tf.Tensor:
+    def __call__(self,
+                 inputs: tf.Tensor,
+                 training: bool):
         """
         Quantize a tensor.
 
         Args:
             inputs: Input tensor to quantize.
             training: Whether the graph is in training mode.
-            weights: Dictionary of weights the quantizer can use to quantize the tensor.
-            **kwargs: Additional variables the quantizer may receive.
 
         Returns:
             The quantized tensor.
         """
 
-        self.ar_iter = weights[GPTQ_ITER]
-        ptq_threshold_tensor = weights[PTQ_THRESHOLD]
+        ptq_threshold_tensor = self.get_quantizer_variable(PTQ_THRESHOLD)
+
+        #####################################################
+        # Soft Rounding
+        #####################################################
+        aux_var = self.get_soft_targets()
+        if not training:
+            aux_var = tf.cast(tf.math.greater_equal(aux_var, 0.5), tf.float32)
 
         if self.per_channel:
-            reshape_shape = self._get_threshold_reshape_shape(inputs.shape, quant_axis_dim=-1)
+            reshape_shape = get_threshold_reshape_shape(inputs.shape,
+                                                        quant_axis=self.quantization_axis,
+                                                        quant_axis_dim=-1)
 
             ##########################################################
             # Calculate soft rounding targets and optimized threshold
             ##########################################################
             ptq_threshold_tensor_hat = tf.reshape(ptq_threshold_tensor, reshape_shape)
-            aux_var = self.get_soft_targets()
-
-            #####################################################
-            # Soft Rounding
-            #####################################################
-            if training:
-                self.ar_iter.assign_add(1.0)
-            else:
-                aux_var = tf.cast(weights[AUXVAR] >= 0, tf.float32)
 
             #####################################################
             # Quantized Input
             #####################################################
-            q_tensor = qutils.symmetric_rounding_quantizer(input_tensor=inputs,
-                                                           auxvar_tensor=aux_var,
-                                                           threshold_tensor=ptq_threshold_tensor_hat,
-                                                           num_bits=self.num_bits,
-                                                           signed=self.signed,
-                                                           power_of_two=self.power_of_two)
+            q_tensor = soft_rounding_symmetric_quantizer(input_tensor=inputs,
+                                                         auxvar_tensor=aux_var,
+                                                         threshold_tensor=ptq_threshold_tensor_hat,
+                                                         num_bits=self.num_bits,
+                                                         signed=True,
+                                                         power_of_two=self.power_of_two)
 
             if self.quantization_parameter_learning and not self.power_of_two:
-                scale = tf.reshape(self.quantizer_parameters[SCALE_PTQ], reshape_shape)
+                scale = tf.reshape(self.get_quantizer_variable(SCALE_PTQ), reshape_shape)
+                scale = tf.where(scale <= 0, MIN_THRESHOLD, scale)
                 q_tensor *= scale
 
-            return q_tensor
         else:
-            return qutils.symmetric_rounding_quantizer(input_tensor=inputs,
-                                                       auxvar_tensor=weights[AUXVAR],
-                                                       threshold_tensor=ptq_threshold_tensor.value(),
-                                                       num_bits=self.num_bits,
-                                                       signed=self.signed,
-                                                       power_of_two=self.power_of_two)
-
-    # TODO: Extract this method to a parent class of all GPTQ quantizer and use it in other quantizers (such as STE)
-    def _get_threshold_reshape_shape(self, tensor_shape, quant_axis_dim):
-        """
-        Gets a shape that contains 1 in all axis except the quantization axis, to adjust the threshold tensor for
-        per-channel quantization.
+            q_tensor = soft_rounding_symmetric_quantizer(input_tensor=inputs,
+                                                         auxvar_tensor=aux_var,
+                                                         threshold_tensor=ptq_threshold_tensor.value(),
+                                                         num_bits=self.num_bits,
+                                                         signed=True,
+                                                         power_of_two=self.power_of_two)
 
-        Args:
-            tensor_shape: The shape of the tensor to be quantize.
-            quant_axis_dim: The dimension of the quantization axis.
-
-        Returns: A shape to reshape the threshold tensor according to.
-
-        """
-        n_axis = len(tensor_shape)
-        quantization_axis = n_axis + self.quantization_axis if self.quantization_axis < 0 else \
-            self.quantization_axis
+            if self.quantization_parameter_learning and not self.power_of_two:
+                scale = self.get_quantizer_variable(SCALE_PTQ)
+                scale = tf.where(scale <= 0, MIN_THRESHOLD, scale)
+                q_tensor *= scale
 
-        return [quant_axis_dim if i == quantization_axis else 1 for i in range(n_axis)]
+        return q_tensor
 
-    def get_quant_config(self, layer) -> Dict[str, np.ndarray]:
+    def get_quant_config(self) -> Dict[str, np.ndarray]:
         """
         Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
 
-        Args:
-            layer: quantized layer
-
         Returns:
             A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
             Keys must match NodeQuantizationConfig attributes
         """
 
         if self.power_of_two:
-            old_threshold = self.quantizer_parameters[PTQ_THRESHOLD]
+            old_threshold = self.get_quantizer_variable(PTQ_THRESHOLD)
             old_threshold = max_power_of_two(old_threshold, MIN_THRESHOLD)
 
         else:
-            old_threshold = self.quantizer_parameters[PTQ_THRESHOLD]
+            old_threshold = self.get_quantizer_variable(PTQ_THRESHOLD)
             if self.quantization_parameter_learning:
-                scale = tf.reshape(self.quantizer_parameters[SCALE_PTQ], self.threshold_shape)
+                scale = self.get_quantizer_variable(SCALE_PTQ)
+                if self.per_channel:
+                    scale = tf.reshape(scale, self.threshold_shape)
+                scale = tf.where(scale <= 0, MIN_THRESHOLD, scale)
                 old_threshold = old_threshold * scale
             old_threshold = old_threshold.numpy()
         old_threshold = old_threshold.reshape(self.threshold_shape)
         return {THRESHOLD: old_threshold}
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,281 +1,247 @@
-# Copyright 2021 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-from typing import Dict, Any, List
-
+import torch
+import torch.nn as nn
+from typing import Dict
 import numpy as np
-import tensorflow as tf
-from tensorflow_model_optimization.python.core.quantization.keras.quantize_wrapper import QuantizeWrapper
-from tensorflow.python.framework.tensor_shape import TensorShape
-from model_compression_toolkit.core.keras.quantizer.base_quantizer import BaseTrainableQuantizer
-from model_compression_toolkit.gptq.common.gptq_constants import GPTQ_ITER, THRESHOLD_TENSOR, AUXVAR
-from model_compression_toolkit.gptq.keras.quantizer import quant_utils as  qutils
-from model_compression_toolkit.core.common.constants import THRESHOLD
-from model_compression_toolkit.core.common.defaultdict import DefaultDict
-from model_compression_toolkit.gptq.keras.quantizer.kernel_functions import get_kernel
-
-
-def symmetric_quantizer(input_tensor: tf.Tensor,
-                        max_tensor: tf.Tensor,
-                        num_bits: int,
-                        signed: bool,
-                        power_of_two: bool = False) -> tf.Tensor:
-    """
-    Quantize a tensor symmetrically.
-    Args:
-        input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
-        max_tensor: Tensor with max values to compute the threshold.
-        num_bits: Num of bits to use.
-        signed: Signedness of the quantization range.
-        power_of_two: Whether the threshold should be constrained or not.
 
-    Returns:
-        A quantized tensor.
+from model_compression_toolkit.core.common import max_power_of_two
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget, PytorchQuantizationWrapper
+from model_compression_toolkit.gptq.common.gptq_config import RoundingType
+from model_compression_toolkit.gptq.pytorch.quantizer.base_pytorch_gptq_quantizer import \
+    BasePytorchGPTQTrainableQuantizer
+from model_compression_toolkit.core.pytorch.utils import to_torch_tensor, torch_tensor_to_numpy
+from model_compression_toolkit.gptq.pytorch.quantizer import quant_utils as qutils
+from model_compression_toolkit.gptq.common.gptq_constants import PTQ_THRESHOLD, SCALE_PTQ, \
+    SOFT_ROUNDING_GAMMA, SOFT_ROUNDING_ZETA, AUXVAR
+from model_compression_toolkit.constants import THRESHOLD, MIN_THRESHOLD
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig
+from mct_quantizers import mark_quantizer
+from model_compression_toolkit.trainable_infrastructure.common.quant_utils import \
+    get_threshold_reshape_shape
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+
+
+def soft_rounding_symmetric_quantizer(input_tensor: torch.Tensor,
+                                      auxvar_tensor: torch.Tensor,
+                                      threshold_tensor: torch.Tensor,
+                                      num_bits: int,
+                                      signed: bool,
+                                      power_of_two: bool) -> torch.Tensor:
     """
+    Quantize a tensor symmetrically for GPTQ quantizers.
 
-    if power_of_two:
-        max_tensor = qutils.power_of_two_max(max_tensor)
-    delta = qutils.calculate_delta(max_tensor, num_bits, signed)
-    tensor_q = qutils.ste_round(input_tensor / delta)
-    min_int = -int(signed) * (2 ** (num_bits - int(signed)))
-    max_int = (2 ** (num_bits - int(signed))) - 1
-    return delta * qutils.ste_clip(tensor_q, max_val=max_int, min_val=min_int)
-
-
-def pertubation_symmetric_quantizer(input_tensor: tf.Tensor,
-                                    auxvar_tensor: tf.Variable,
-                                    max_tensor: tf.Tensor,
-                                    num_bits: int,
-                                    signed: bool,
-                                    power_of_two: bool,
-                                    max_lsbs_change: int = 1) -> tf.Tensor:
-    """
-    Quantize a tensor symmetrically with maximum LSBs shift.
     Args:
         input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
-        auxvar_tensor: Tensor that manifests the bit shift the weight due to gptq
-        max_tensor: Tensor with max values to compute the threshold.
+        auxvar_tensor: Tensor that manifests the bit shift of the quantized weights due to gptq training.
+        threshold_tensor: Tensor with values to compute the threshold.
         num_bits: Num of bits to use.
         signed: Signedness of the quantization range.
         power_of_two: Whether the threshold should be constrained or not.
-        max_lsbs_change: maximum number of LSBs that the auxvar is allowed to change
 
     Returns:
         A quantized tensor.
     """
 
     if power_of_two:
-        max_tensor = qutils.power_of_two_max(max_tensor)
-    delta = qutils.calculate_delta(max_tensor, num_bits, signed)
-    input_tensor_int = tf.stop_gradient(tf.round(input_tensor / delta))
-    tensor_q = qutils.ste_round(
-        input_tensor_int + qutils.ste_clip(auxvar_tensor, max_val=max_lsbs_change * delta) / delta)
-    min_int = -int(signed) * (2 ** (num_bits - int(signed)))
-    max_int = (2 ** (num_bits - int(signed))) - 1
-    return delta * qutils.ste_clip(tensor_q, max_val=max_int, min_val=min_int)
-
-
-class STEWeightQuantizer(BaseTrainableQuantizer):
+        threshold_tensor = qutils.power_of_two_max(threshold_tensor)
+    delta = qutils.calculate_delta(threshold_tensor, num_bits, signed)
+    with torch.no_grad():
+        input_tensor_int = torch.floor(input_tensor / delta)
+    tensor_q = input_tensor_int + auxvar_tensor
+    int_threshold = 2 ** (num_bits - int(signed))
+    return delta * qutils.ste_clip(tensor_q,
+                                   min_val=-int(signed) * int_threshold,
+                                   max_val=int_threshold - 1)
+
+
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
+                quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
+                quantizer_type=RoundingType.SoftQuantizer)
+class SymmetricSoftRoundingGPTQ(BasePytorchGPTQTrainableQuantizer):
     """
-    Trainable constrained quantizer to quantize a layer inputs.
+    Trainable symmetric quantizer to optimize the rounding of the quantized values using a soft quantization method.
     """
 
     def __init__(self,
-                 num_bits: int,
-                 per_channel: bool,
-                 signed: bool,
-                 threshold_values: np.ndarray,
-                 quantization_axis: int = -1,
-                 power_of_two: bool = True,
-                 max_lsbs_change_map: dict = DefaultDict({}, lambda: 1)):
+                 quantization_config: TrainableQuantizerWeightsConfig,
+                 quantization_parameter_learning: bool = False):
         """
-        Initialize a TrainableWeightQuantizer object with parameters to use
-        for the quantization.
+        Construct a Pytorch model that utilize a fake weight quantizer of soft-quantizer for symmetric quantizer.
 
         Args:
-            num_bits: Number of bits to use for the quantization.
-            per_channel: Whether to quantize per-channel or per-tensor.
-            signed: Signedness to use for the quantization range.
-            threshold_values: Threshold to use for the quantization.
-            quantization_axis: Axis of tensor to use for the quantization.
-            power_of_two: Whether the threshold should be constrained or not.
-            max_lsbs_change_map: a mapping between number of bits to max lsb change.
+            quantization_config: Trainable weights quantizer config.
+            quantization_parameter_learning (Bool): Whether to learn the threshold or not
         """
-        self.num_bits = num_bits
-        self.per_channel = per_channel
-        self.signed = signed
+
+        super().__init__(quantization_config)
+        self.num_bits = quantization_config.weights_n_bits
+        self.per_channel = quantization_config.weights_per_channel_threshold
+
+        threshold_values = quantization_config.weights_quantization_params[THRESHOLD]
         self.threshold_shape = np.asarray(threshold_values).shape
         self.threshold_values = np.reshape(np.asarray(threshold_values), [-1]) if self.per_channel else float(
             threshold_values)
-        self.quantization_axis = quantization_axis
-        self.power_of_two = power_of_two
-        self.max_lsbs_change = max_lsbs_change_map.get(num_bits)
-        self.quantizer_parameters = {}
 
-    def build(self,
-              tensor_shape: TensorShape,
-              name: str,
-              layer: QuantizeWrapper) -> Dict[str, tf.Variable]:
-        """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        self.quantization_axis = quantization_config.weights_channels_axis
+        self.power_of_two = quantization_config.weights_quantization_method == QuantizationMethod.POWER_OF_TWO
+        self.quantization_parameter_learning = quantization_parameter_learning
+
+        # gamma and zeta are stretch parameters for computing the rectified sigmoind function.
+        # See: https://arxiv.org/pdf/2004.10568.pdf
+        self.gamma = SOFT_ROUNDING_GAMMA
+        self.zeta = SOFT_ROUNDING_ZETA
 
-        Returns:
-            Dictionary of new variables.
-        """
-        w_shape = get_kernel(layer.weights).shape
-        ar_iter = layer.add_weight(
-            f"{name}_{GPTQ_ITER}",
-            shape=(),
-            initializer=tf.keras.initializers.Constant(0.0),
-            trainable=False)
-
-        ptq_threshold_tensor = layer.add_weight(
-            name + THRESHOLD_TENSOR,
-            shape=len(self.threshold_values) if self.per_channel else (),
-            initializer=tf.keras.initializers.Constant(1.0),
-            trainable=False)
-        ptq_threshold_tensor.assign(self.threshold_values)
-
-        auxvar_tensor = layer.add_weight(
-            f"{name}_{AUXVAR}",
-            shape=w_shape,
-            initializer=tf.keras.initializers.Constant(0.0),
-            trainable=True)
+        self.quantizer_parameters = {}
 
-        # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {THRESHOLD_TENSOR: ptq_threshold_tensor,
-                                     AUXVAR: auxvar_tensor,
-                                     GPTQ_ITER: ar_iter}
-        return self.quantizer_parameters
-
-    def __call__(self, inputs: tf.Tensor,
-                 training: bool,
-                 weights: Dict[str, tf.Variable],
-                 **kwargs: Dict[str, Any]):
+    def initialize_quantization(self,
+                                tensor_shape: torch.Size,
+                                name: str,
+                                layer: PytorchQuantizationWrapper):
         """
-        Quantize a tensor.
-        Args:
-            inputs: Input tensor to quantize.
-            training: Whether the graph is in training mode.
-            weights: Dictionary of weights the quantizer can use to quantize the tensor.
-            **kwargs: Additional variables the quantizer may receive.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            The quantized tensor.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
 
-        auxvar = weights[AUXVAR]
-        ptq_threshold_tensor = weights[THRESHOLD_TENSOR]
-
         if self.per_channel:
-            input_shape = inputs.shape
-            n_axis = len(input_shape)
-            quantization_axis = n_axis + self.quantization_axis if self.quantization_axis < 0 else \
-                self.quantization_axis
-            reshape_shape = [-1 if i == quantization_axis else 1 for i in range(n_axis)]
-            ptq_threshold_tensor = tf.reshape(ptq_threshold_tensor, reshape_shape)
-            q_tensor = pertubation_symmetric_quantizer(inputs, auxvar,
-                                                       ptq_threshold_tensor,
-                                                       self.num_bits,
-                                                       self.signed,
-                                                       self.power_of_two,
-                                                       max_lsbs_change=self.max_lsbs_change)
-            return q_tensor
+            threshold_tensor = to_torch_tensor(self.threshold_values)
         else:
-            return pertubation_symmetric_quantizer(inputs, auxvar,
-                                                   ptq_threshold_tensor,
-                                                   self.num_bits,
-                                                   self.signed,
-                                                   self.power_of_two)
+            threshold_tensor = torch.tensor(self.threshold_values)
+        layer.register_parameter(f"{name}_{PTQ_THRESHOLD}",
+                                 nn.Parameter(threshold_tensor, requires_grad=False))
+
+        w = layer.layer.weight
+        delta = qutils.calculate_delta(threshold_tensor.reshape(self.threshold_shape), self.num_bits, signed=True)
+        w_clipped_normed = torch.clip(w / delta, -2**(self.num_bits-1), 2**(self.num_bits-1)-1)
+        rest = w_clipped_normed - torch.floor(w_clipped_normed)  # rest of rounding [0, 1)
+        # Note that (rest - self.gamma) can't be zero since rest is positive and gamma is negative, so the division
+        # is safe
+        alpha = -torch.log((self.zeta - self.gamma) / (rest - self.gamma) - 1)  # => sigmoid(alpha) = rest
 
-    def get_aux_variable(self) -> tf.Tensor:
-        return self.quantizer_parameters[AUXVAR]
+        layer.register_parameter(f"{name}_{AUXVAR}", nn.Parameter(alpha, requires_grad=True))
 
-    def get_config(self) -> Dict[str, Any]:
-        """
-        Returns: Configuration of TrainableQuantizer.
-        """
+        # save the quantizer added parameters for later calculations
+        self.add_quantizer_variable(PTQ_THRESHOLD, layer.get_parameter(f"{name}_{PTQ_THRESHOLD}"), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(AUXVAR, layer.get_parameter(f"{name}_{AUXVAR}"), VariableGroup.WEIGHTS)
 
-        return {
-            'num_bits': self.num_bits,
-            'per_channel': self.per_channel,
-            'symmetric': self.symmetric,
-            'power_of_two': self.power_of_two
-        }
+        if self.quantization_parameter_learning:
+            if self.per_channel:
+                layer.register_parameter(f"{name}_{SCALE_PTQ}",
+                                         nn.Parameter(to_torch_tensor(torch.ones_like(torch.Tensor(self.threshold_values))),
+                                                      requires_grad=True))
+            else:
+                layer.register_parameter(f"{name}_{SCALE_PTQ}",
+                                         nn.Parameter(to_torch_tensor((torch.tensor([1.0], requires_grad=True)))))
+            self.add_quantizer_variable(SCALE_PTQ, layer.get_parameter(f"{name}_{SCALE_PTQ}"), VariableGroup.QPARAMS)
 
-    def get_quant_config(self, layer) -> Dict[str, np.ndarray]:
+    def get_soft_targets(self) -> torch.Tensor:
         """
-        Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
-
-        Args:
-            layer: quantized layer
+        Computes the rectified sigmoid function for the quantization target parameters.
 
         Returns:
-            A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
-            Keys must match NodeQuantizationConfig attributes
+            A tensor with the soft rounding targets values.
 
         """
-        old_threshold = self.quantizer_parameters[THRESHOLD_TENSOR]
-        return {THRESHOLD: old_threshold.numpy().reshape(self.threshold_shape)}
+        scaled_sigmoid = torch.sigmoid(self.get_quantizer_variable(AUXVAR)) * (self.zeta - self.gamma) + self.gamma
+        return torch.clip(scaled_sigmoid, min=0, max=1)
 
-    def get_trainable_parameters(self):
+    def get_quant_config(self) -> Dict[str, np.ndarray]:
         """
-        A function to get a list trainable of trainable parameters of the quantizer for GPTQ retraining
+        Returns the config used to edit NodeQuantizationConfig after GPTQ retraining
 
         Returns:
-            A list of trainable Tensors
+            A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
+            Keys must match NodeQuantizationConfig attributes
 
         """
-        return [t for t in self.quantizer_parameters.values() if t.trainable]
-
-    def get_quantization_variable(self) -> List[tf.Tensor]:
+        old_threshold = torch_tensor_to_numpy(self.get_quantizer_variable(PTQ_THRESHOLD))
+        old_threshold = np.resize(old_threshold, self.threshold_shape)
+        if self.power_of_two:
+            old_threshold = max_power_of_two(old_threshold, MIN_THRESHOLD)
+        else:
+            if self.quantization_parameter_learning:
+                scale = torch.reshape(self.get_quantizer_variable(SCALE_PTQ), self.threshold_shape)
+                scale = torch.where(scale <= 0, torch.tensor(MIN_THRESHOLD, device=scale.device), scale)
+                old_threshold = old_threshold * torch_tensor_to_numpy(scale)
+        old_threshold = old_threshold.reshape(self.threshold_shape)
+        return {THRESHOLD: old_threshold}
+
+    def __call__(self,
+                 inputs: nn.Parameter,
+                 training: bool) -> torch.Tensor:
         """
-         This function return a list of quantizer parameters.
-         Returns: A list of the quantizer parameters
-
-         """
-        return [self.quantizer_parameters[THRESHOLD_TENSOR]]
+        Quantize a tensor.
 
-    def __eq__(self, other: Any) -> bool:
-        """
-        Check if equals to another object.
         Args:
-            other: Other object to compare.
+            inputs: Input tensor to quantize.
+            training: whether in training mode or not
 
         Returns:
-            Whether they are equal or not.
+            quantized tensor
         """
-        if not isinstance(other, STEWeightQuantizer):
-            return False
+        auxvar = self.get_quantizer_variable(AUXVAR)
+        ptq_threshold_tensor = self.get_quantizer_variable(PTQ_THRESHOLD)
 
-        return (self.num_bits == other.num_bits and
-                self.per_channel == other.per_channel and
-                self.symmetric == other.symmetric)
+        #####################################################
+        # Soft Rounding
+        #####################################################
+        aux_var = self.get_soft_targets()
+        if not training:
+            aux_var = (aux_var >= 0.5).to(auxvar.dtype)
 
-    def __ne__(self, other: Any) -> bool:
-        """
-        Check if not equals to another object.
-        Args:
-            other: Other object to compare.
+        if self.per_channel:
+            reshape_shape = get_threshold_reshape_shape(inputs.shape,
+                                                        quant_axis=self.quantization_axis,
+                                                        quant_axis_dim=-1)
+
+            ##########################################################
+            # Calculate soft rounding targets and optimized threshold
+            ##########################################################
+            ptq_threshold_tensor_hat = torch.reshape(ptq_threshold_tensor, reshape_shape)
+
+            #####################################################
+            # Quantized Input
+            #####################################################
+            q_tensor = soft_rounding_symmetric_quantizer(input_tensor=inputs,
+                                                         auxvar_tensor=aux_var,
+                                                         threshold_tensor=ptq_threshold_tensor_hat,
+                                                         num_bits=self.num_bits,
+                                                         signed=True,
+                                                         power_of_two=self.power_of_two)
+
+            if self.quantization_parameter_learning and not self.power_of_two:
+                scale = torch.reshape(self.get_quantizer_variable(SCALE_PTQ), reshape_shape)
+                scale = torch.where(scale <= 0, torch.tensor(MIN_THRESHOLD, device=scale.device), scale)
+                q_tensor *= scale
 
-        Returns:
-            Whether they are differ or not.
-        """
-        return not self.__eq__(other)
+        else:
+            q_tensor = soft_rounding_symmetric_quantizer(input_tensor=inputs,
+                                                         auxvar_tensor=aux_var,
+                                                         threshold_tensor=ptq_threshold_tensor,
+                                                         num_bits=self.num_bits,
+                                                         signed=True,
+                                                         power_of_two=self.power_of_two)
+
+            if self.quantization_parameter_learning and not self.power_of_two:
+                scale = self.get_quantizer_variable(SCALE_PTQ)
+                scale = torch.where(scale <= 0, torch.tensor(MIN_THRESHOLD, device=scale.device), scale)
+                q_tensor *= scale
+
+        return q_tensor
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/gptq_graph_info.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/graph_info.py`

 * *Files 18% similar despite different names*

```diff
@@ -11,64 +11,74 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import torch
 import torch.nn as nn
 from typing import List
-from model_compression_toolkit.gptq.pytorch.quantizer.quantizer_wrapper import WeightQuantizerWrapper
 from model_compression_toolkit.core.pytorch.constants import BIAS
+from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
+from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
+from model_compression_toolkit.logger import Logger
+from mct_quantizers import PytorchQuantizationWrapper
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-def get_trainable_parameters(fxp_model: nn.Module,
-                             add_bias: bool = False,
-                             quantization_parameters_learning: bool = False
-                             ) -> (List[nn.Parameter], List[nn.Parameter], List[nn.Parameter]):
+def get_gptq_trainable_parameters(fxp_model: nn.Module,
+                                  add_bias: bool = False,
+                                  ) -> (List[nn.Parameter], List[nn.Parameter], List[nn.Parameter]):
     """
     Get trainable parameters from all layers in a model
 
     Args:
         fxp_model: Model to get its trainable parameters.
         add_bias: Whether to include biases of the model (if there are) or not.
-        quantization_parameters_learning: Whether to include quantization parameters of the model or not.
+
     Returns:
         A list of trainable variables in a model. Each item is a list of a layers weights.
     """
 
     trainable_aux_weights = nn.ParameterList()
     trainable_threshold = nn.ParameterList()
     trainable_bias = nn.ParameterList()
-    trainable_temperature = nn.ParameterList()
 
     for layer in fxp_model.modules():
-        if isinstance(layer, WeightQuantizerWrapper):
-            trainable_aux_weights.append(layer.weight_quantizer.get_aux_variable())
-            if quantization_parameters_learning:
-                trainable_threshold.extend(layer.weight_quantizer.get_quantization_variable())
-            if add_bias and hasattr(layer.op, BIAS):
-                bias = getattr(layer.op, BIAS)
+        if isinstance(layer, PytorchQuantizationWrapper):
+            kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=type(layer.layer),
+                                                                  fw_info=DEFAULT_PYTORCH_INFO)
+
+            # collect trainable weights per quantizer
+            if kernel_attribute not in layer.weights_quantizers:
+                Logger.error(f'{kernel_attribute} was not found in weight quantizers of layer {layer.layer}')
+            quantizer_trainable_weights = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.WEIGHTS)
+            quantizer_trainable_threshold = layer.weights_quantizers[kernel_attribute].get_trainable_variables(VariableGroup.QPARAMS)
+            trainable_aux_weights.extend(quantizer_trainable_weights)
+            trainable_threshold.extend(quantizer_trainable_threshold)
+
+            if add_bias and hasattr(layer.layer, BIAS):
+                bias = getattr(layer.layer, BIAS)
                 trainable_bias.append(bias)
 
-    return trainable_aux_weights, trainable_bias, trainable_threshold, trainable_temperature
+    return trainable_aux_weights, trainable_bias, trainable_threshold
 
 
-def get_weights_for_loss(fxp_model: nn.Module) -> [List, List]:
+def get_weights_for_loss(fxp_model: nn.Module) -> [List[nn.Parameter], List[torch.Tensor]]:
     """
     Get all float and quantized kernels for the GPTQ loss
 
     Args:
         fxp_model: Model to get its float and quantized weights.
 
     Returns:
         A list of float kernels, each item is the float kernel of the layer
         A list of quantized kernels, each item is the quantized kernel of the layer
     """
 
     flp_weights_list, fxp_weights_list = [], []
     for layer in fxp_model.modules():
-        if isinstance(layer, WeightQuantizerWrapper):
+        if isinstance(layer, PytorchQuantizationWrapper):
             # Collect pairs of float and quantized weights per layer
-            weights = layer.op.weight
-            flp_weights_list.append(weights)
-            fxp_weights_list.append(layer.weight_quantizer(weights, training=False))
+            for weight, quantizer_vars, quantizer in layer.get_weights_vars():
+                flp_weights_list.append(quantizer_vars)
+                fxp_weights_list.append(quantizer(training=False, inputs=quantizer_vars))
 
     return flp_weights_list, fxp_weights_list
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/gptq_loss.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/gptq_loss.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantization_facade.py`

 * *Files 9% similar despite different names*

```diff
@@ -10,37 +10,38 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common.constants import FOUND_TORCH
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import PYTORCH
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2, RoundingType
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import PYTORCH
+from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
+from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
+from model_compression_toolkit.gptq.keras.quantization_facade import GPTQ_MOMENTUM
 from model_compression_toolkit.gptq.runner import gptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-from model_compression_toolkit import CoreConfig, GPTQQuantizerConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfigV2
 
 LR_DEFAULT = 1e-4
 LR_REST_DEFAULT = 1e-4
 LR_BIAS_DEFAULT = 1e-4
 LR_QUANTIZATION_PARAM_DEFAULT = 1e-4
 
 if FOUND_TORCH:
     from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-    from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.core.pytorch.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.gptq.pytorch.gptq_pytorch_implementation import GPTQPytorchImplemantation
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.gptq.pytorch.gptq_loss import multiple_tensors_mse_loss
     from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
     import torch
     from torch.nn import Module
     from torch.optim import Adam, Optimizer
     from model_compression_toolkit import get_target_platform_capabilities
 
@@ -48,70 +49,58 @@
     DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
 
     def get_pytorch_gptq_config(n_epochs: int,
                                 optimizer: Optimizer = Adam([torch.Tensor([])], lr=LR_DEFAULT),
                                 optimizer_rest: Optimizer = Adam([torch.Tensor([])], lr=LR_REST_DEFAULT),
                                 loss: Callable = multiple_tensors_mse_loss,
-                                log_function: Callable = None) -> GradientPTQConfigV2:
+                                log_function: Callable = None,
+                                use_hessian_based_weights: bool = True) -> GradientPTQConfigV2:
         """
         Create a GradientPTQConfigV2 instance for Pytorch models.
 
         args:
             n_epochs (int): Number of epochs for running the representative dataset for fine-tuning.
             optimizer (Optimizer): Pytorch optimizer to use for fine-tuning for auxiliry variable.
             optimizer_rest (Optimizer): Pytorch optimizer to use for fine-tuning of the bias variable.
             loss (Callable): loss to use during fine-tuning. should accept 4 lists of tensors. 1st list of quantized tensors, the 2nd list is the float tensors, the 3rd is a list of quantized weights and the 4th is a list of float weights.
             log_function (Callable): Function to log information about the gptq process.
+            use_hessian_based_weights (bool): Whether to use Hessian-based weights for weighted average loss.
 
         returns:
             a GradientPTQConfigV2 object to use when fine-tuning the quantized model using gptq.
 
         Examples:
 
             Import MCT and Create a GradientPTQConfigV2 to run for 5 epochs:
 
             >>> import model_compression_toolkit as mct
-            >>> gptq_conf = mct.get_pytorch_gptq_config(n_epochs=5)
+            >>> gptq_conf = mct.gptq.get_pytorch_gptq_config(n_epochs=5)
 
             Other PyTorch optimizers can be passed with dummy params:
 
             >>> import torch
-            >>> gptq_conf = mct.get_pytorch_gptq_config(n_epochs=3, optimizer=torch.optim.Adam([torch.Tensor(1)]))
+            >>> gptq_conf = mct.gptq.get_pytorch_gptq_config(n_epochs=3, optimizer=torch.optim.Adam([torch.Tensor(1)]))
 
             The configuration can be passed to :func:`~model_compression_toolkit.pytorch_post_training_quantization` in order to quantize a pytorch model using gptq.
 
         """
-        bias_optimizer = Adam([torch.Tensor([])], lr=LR_BIAS_DEFAULT)
-        optimizer_quantization_parameter = Adam([torch.Tensor([])], lr=LR_QUANTIZATION_PARAM_DEFAULT)
-        # TODO: Once implementing Soft Quantizer for GPTQ in Pytorch:
-        #  - change default quantization_parameters_learning to True.
-        #  - remove explicit rounding_type and quantizer_config (and let it use the default GradientPTQConfig).
-        return GradientPTQConfigV2(n_epochs,
-                                   optimizer,
-                                   optimizer_rest=optimizer_rest,
-                                   loss=loss,
-                                   log_function=log_function,
-                                   train_bias=True,
-                                   optimizer_quantization_parameter=optimizer_quantization_parameter,
-                                   optimizer_bias=bias_optimizer,
-                                   rounding_type=RoundingType.STE,
-                                   quantizer_config=GPTQQuantizerConfig(),
-                                   quantization_parameters_learning=False,
-                                   )
+        bias_optimizer = torch.optim.SGD([torch.Tensor([])], lr=LR_BIAS_DEFAULT, momentum=GPTQ_MOMENTUM)
+        return GradientPTQConfigV2(n_epochs, optimizer, optimizer_rest=optimizer_rest, loss=loss,
+                                   log_function=log_function, train_bias=True, optimizer_bias=bias_optimizer, use_hessian_based_weights=use_hessian_based_weights)
 
 
     def pytorch_gradient_post_training_quantization_experimental(model: Module,
                                                                  representative_data_gen: Callable,
                                                                  target_kpi: KPI = None,
                                                                  core_config: CoreConfig = CoreConfig(),
                                                                  gptq_config: GradientPTQConfigV2 = None,
                                                                  gptq_representative_data_gen: Callable = None,
                                                                  target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC,
-                                                                 new_experimental_exporter: bool = False):
+                                                                 new_experimental_exporter: bool = True):
         """
         Quantize a trained Pytorch module using post-training quantization.
         By default, the module is quantized using a symmetric constraint quantization thresholds
         (power of two) as defined in the default TargetPlatformCapabilities.
         The module is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
@@ -127,16 +116,16 @@
         Args:
             model (Module): Pytorch model to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
             gptq_config (GradientPTQConfigV2): Configuration for using gptq (e.g. optimizer).
             gptq_representative_data_gen (Callable): Dataset used for GPTQ training. If None defaults to representative_data_gen
-            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to. `Default PyTorch TPC <https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/tpc_models/pytorch_tp_models/pytorch_default.py>`_
-            new_experimental_exporter (bool): Whether exporting the quantized model using new exporter or not (in progress. Avoiding it for now is recommended).
+            target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
+            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
             A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
             Import a Pytorch module:
@@ -151,34 +140,34 @@
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 3, 224, 224))]
 
             Create MCT core configurations with number of calibration iterations set to 1:
 
-            >>> config = mct.CoreConfig()
+            >>> config = mct.core.CoreConfig()
 
             Pass the module, the representative dataset generator and the configuration (optional) to get a quantized module
 
-            >>> quantized_module, quantization_info = mct.pytorch_gradient_post_training_quantization_experimental(module, repr_datagen, core_config=config, gptq_config=gptq_conf)
+            >>> quantized_module, quantization_info = mct.gptq.pytorch_gradient_post_training_quantization_experimental(module, repr_datagen, core_config=config, gptq_config=gptq_conf)
 
         """
 
         if core_config.mixed_precision_enable:
             if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+                Logger.error("Given quantization config to mixed-precision facade is not of type "
                                     "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
                                     "API, or pass a valid mixed precision configuration.")  # pragma: no cover
 
-            common.Logger.info("Using experimental mixed-precision quantization. "
+            Logger.info("Using experimental mixed-precision quantization. "
                                "If you encounter an issue please file a bug.")
 
         tb_w = _init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
 
-        fw_impl = PytorchImplementation()
+        fw_impl = GPTQPytorchImplemantation()
 
         # ---------------------- #
         # Core Runner
         # ---------------------- #
         graph, bit_widths_config = core_runner(in_model=model,
                                                representative_data_gen=representative_data_gen,
                                                core_config=core_config,
@@ -198,18 +187,20 @@
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen, tb_w, graph_gptq, fw_impl, DEFAULT_PYTORCH_INFO)
 
         # ---------------------- #
         # Export
         # ---------------------- #
         if new_experimental_exporter:
-            Logger.warning('Using new experimental exported models. '
-                           'Please do not use unless you are familiar with what you are doing')
+            Logger.warning('Using new experimental wrapped and ready for export models. To '
+                           'disable it, please set new_experimental_exporter to False when '
+                           'calling pytorch_gradient_post_training_quantization_experimental. '
+                           'If you encounter an issue please file a bug.')
 
-            return get_fully_quantized_pytorch_model(graph_gptq)
+            return get_exportable_pytorch_model(graph_gptq)
 
         return export_model(graph_gptq,
                             DEFAULT_PYTORCH_INFO,
                             fw_impl,
                             tb_w,
                             bit_widths_config)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py`

 * *Files 16% similar despite different names*

```diff
@@ -8,42 +8,16 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Union, Tuple
+from typing import Tuple
 import torch
-from torch.nn.functional import softmax, log_softmax, one_hot
-from model_compression_toolkit.core.common.constants import MIN_THRESHOLD
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
-
-
-def power_of_two_max(max_tensor: torch.Tensor) -> torch.Tensor:
-    """
-    Compute the power of two threshold for a tensor.
-    """
-    return torch.pow(2, ste_ceil(torch.log2(torch.clip(max_tensor, min=MIN_THRESHOLD, max=torch.inf))))
-
-
-def calculate_delta(max_tensor: torch.Tensor,
-                    num_bits: int,
-                    signed: bool) -> torch.Tensor:
-    """
-    Compute the step size for the quantization.
-    """
-    return max_tensor / (2 ** (num_bits - int(signed)))
-
-
-def ste_ceil(x: torch.Tensor) -> torch.Tensor:
-    """
-    Return the ceil values of a tensor.
-    """
-    return (torch.ceil(x) - x).detach() + x
 
 
 def ste_round(x: torch.Tensor) -> torch.Tensor:
     """
     Calculate the rounded values of a tensor
     Args:
         x: input variable
@@ -62,40 +36,14 @@
         max_val: maximum value for clipping
     Returns:
         clipped variable
     """
     return (torch.clip(x, min=min_val, max=max_val) - x).detach() + x
 
 
-def symmetric_quantizer(input_tensor: torch.Tensor,
-                        max_tensor: torch.Tensor,
-                        num_bits: int,
-                        signed: bool,
-                        power_of_two: bool = False) -> torch.Tensor:
-    """
-    Quantize a tensor symmetrically.
-    Args:
-        input_tensor: Tensor to quantize. values of this tensor are not changed during gptq.
-        max_tensor: Tensor with max values to compute the threshold.
-        num_bits: Num of bits to use.
-        signed: Signedness of the quantization range.
-        power_of_two: Whether the threshold should be constrained or not.
-    Returns:
-        A quantized tensor.
-    """
-
-    if power_of_two:
-        max_tensor = power_of_two_max(max_tensor)
-    delta_tensor = calculate_delta(max_tensor, num_bits, signed)
-    tensor_q = ste_round(input_tensor / delta_tensor)
-    min_int = -int(signed) * (2 ** (num_bits - int(signed)))
-    max_int = (2 ** (num_bits - int(signed))) - 1
-    return delta_tensor * ste_clip(tensor_q, min_val=min_int, max_val=max_int)
-
-
 def fix_range_to_include_zero(range_min: torch.Tensor,
                               range_max: torch.Tensor,
                               n_bits: int) -> Tuple[torch.Tensor, torch.Tensor]:
     """
     Adjusting the quantization range to include representation of 0.0 in the quantization grid.
     If quantization per-channel, then range_min and range_max should be tensors in the specific shape that allows
     quantization along the channel_axis.
@@ -117,36 +65,72 @@
     max_range_adj = range_max - range_min + min_range_adj
 
     min_range_adj = min_range_adj * mid_range + max_negative * range_min
     max_range_adj = max_range_adj * mid_range + min_positive * range_max
     return min_range_adj, max_range_adj
 
 
+def symmetric_quantizer(tensor_data: torch.Tensor,
+                        threshold: torch.Tensor,
+                        n_bits: int,
+                        sign: bool = False) -> torch.Tensor:
+    """
+    Quantize a tensor according to the number of bits and threshold.
+    Symmetric quantization.
+    Args:
+        tensor_data: Tensor values to quantize.
+        threshold: threshold for quantization.
+        n_bits: Number of bits to quantize the tensor.
+        sign: sign of tensor_data
+    Returns:
+        Quantized data.
+    """
+
+    # Compute the step size of quantized values.
+    n_pos = 2 ** (n_bits - int(sign))
+    delta_tensor = threshold / n_pos
+
+    # Compute min/max int value
+    min_val = -int(sign) * n_pos
+    max_val = n_pos - 1
+
+    # Apply rounding
+    input_tensor_int = ste_round(tensor_data / delta_tensor)
+
+    # Clip data in range
+    clipped_tensor = ste_clip(input_tensor_int, min_val=min_val, max_val=max_val)
+
+    # Quantize the data between -threshold/threshold
+    q = delta_tensor * clipped_tensor
+    return q
+
+
 def uniform_quantizer(tensor_data: torch.Tensor,
-                       range_min: torch.Tensor,
-                       range_max: torch.Tensor,
-                       n_bits: int) -> torch.Tensor:
+                      range_min: torch.Tensor,
+                      range_max: torch.Tensor,
+                      n_bits: int) -> torch.Tensor:
     """
     Quantize a tensor according to given range (min, max) and number of bits.
+    Uniform quantization.
     Args:
         tensor_data: Tensor values to quantize.
         range_min: minimum bound of the range for quantization (or array of min values per channel).
         range_max: maximum bound of the range for quantization (or array of max values per channel).
         n_bits: Number of bits to quantize the tensor.
     Returns:
         Quantized data.
     """
-    # adjusts the quantization rage so the quantization grid include zero.
+    # adjusts the quantization range so the quantization grid includes zero.
     a, b = fix_range_to_include_zero(range_min, range_max, n_bits)
 
     # Compute the step size of quantized values.
     delta_tensor = (b - a) / (2 ** n_bits - 1)
 
     # Apply rounding
     input_tensor_int = ste_round((tensor_data - a) / delta_tensor)
 
     # Clip data in range
     clipped_tensor = ste_clip(input_tensor_int, min_val=0, max_val=2 ** n_bits - 1)
 
     # Quantize the data between min/max of quantization range.
     q = delta_tensor * clipped_tensor + a
-    return q
+    return q
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/quantizer_wrapper.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/pytorch/builder/fully_quantized_model_builder.py`

 * *Files 20% similar despite different names*

```diff
@@ -8,82 +8,78 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import torch
-import torch.nn as nn
-from model_compression_toolkit.core.common import BaseNode, Logger
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig, RoundingType
-from model_compression_toolkit.gptq.pytorch.quantizer.gptq_quantizer import BaseWeightQuantizer
-from model_compression_toolkit.gptq.pytorch.quantizer.ste_rounding.ste_weights_quantizer import STEWeightQuantizer
-from model_compression_toolkit.core.pytorch.back2framework.instance_builder import node_builder
-from model_compression_toolkit.core.pytorch.constants import KERNEL
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
-from model_compression_toolkit.core.common.target_platform.op_quantization_config import QuantizationMethod
 
+from typing import Union, Callable
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common import Graph
+from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.core.common import BaseNode
+
+if FOUND_TORCH:
+    import torch
+    from mct_quantizers import PytorchQuantizationWrapper, PytorchActivationQuantizationHolder
+    from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
+    from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizers import \
+        get_quantization_quantizers
 
-class WeightQuantizerWrapper(nn.Module):
 
-    def __init__(self, node: BaseNode, gptq_config: GradientPTQConfig, weight_quantizer: BaseWeightQuantizer):
+    def fully_quantized_wrapper(node: common.BaseNode,
+                                module: torch.nn.Module) -> Union[torch.nn.Module,PytorchQuantizationWrapper]:
         """
-        Construct a Pytorch model that constitutes as a wrapper for a Pytorch layer, built from a given graph node.
-        Args:
-            node: Node to build its Pytorch quantizer wrapper.
-            gptq_config: GradientPTQConfig object with parameters about the tuning process.
-            weight_quantizer: BaseWeightQuantizer object for gradient based weight quantizer
-        """
-        super().__init__()
-
-        # loading operation
-        self.op = node.type(**node.framework_attr)
+        A function which takes a computational graph node and a pytorch module and
+        perform the quantization wrapping
 
-        # loading the weights from the graph node (weights of the trained model)
-        self.op.load_state_dict({k: torch.Tensor(v) for k, v in node.weights.items()}, strict=False)
-        self.float_weight = to_torch_tensor(getattr(self.op, KERNEL)).detach()
-
-        # replace non-gradient needed nn.Parameter with gradient needed torch.tensor
-        delattr(self.op, KERNEL)
-        setattr(self.op, KERNEL, self.float_weight)
-        setattr(getattr(self.op, KERNEL), 'requires_grad', True)
+        Args:
+            node: A node of mct graph.
+            module: A Pytorch module
+        Returns: Wrapped layer
 
-        # quantizer
-        self.weight_quantizer = weight_quantizer(node.final_weights_quantization_cfg, gptq_config, self.float_weight)
+        """
+        weight_quantizers, _ = get_quantization_quantizers(node)
+        if len(weight_quantizers) > 0:
+            return PytorchQuantizationWrapper(module, weight_quantizers)
+        return module
 
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
+    def get_activation_quantizer_holder(node: BaseNode) -> Callable:
         """
-        Weight fake quantizer wrapper
+        Retrieve a PytorchActivationQuantizationHolder layer to use for activation quantization of a node.
+        If the layer is not supposed to be wrapped with an activation quantizer - return None.
         Args:
-            x: input to layer.
+            node: Node to attach a PytorchActivationQuantizationHolder to its output.
         Returns:
-            Output of layer after using operation with fake quantized weights
+            A PytorchActivationQuantizationHolder module for the node's activation quantization.
         """
-        # Run weight quantizer
-        setattr(self.op, KERNEL, self.weight_quantizer(self.float_weight))
-        # Do computation
-        return self.op(x)
-
-
-def quantizer_wrapper(node: BaseNode, gptq_config: GradientPTQConfig) -> nn.Module:
-    """
-    Construct a Pytorch model that constitutes as a wrapper for a Pytorch layer, built from a given graph node.
-    Args:
-        node: Node to build its Pytorch layer.
-        gptq_config: GradientPTQConfig with parameters about the tuning process.
-    """
-    if node.is_weights_quantization_enabled():
-        quantization_method = node.final_weights_quantization_cfg.weights_quantization_method
-        if quantization_method in [QuantizationMethod.SYMMETRIC, QuantizationMethod.POWER_OF_TWO]:
-            # STE quantizer
-            # ---------------
-            if gptq_config.rounding_type == RoundingType.STE:
-                node_instance = WeightQuantizerWrapper(node, gptq_config, STEWeightQuantizer)
-
-        else:
-            Logger.error(f"For quantization method {quantization_method}, GPTQ Rounding type {gptq_config.rounding_type} is not supported")
-    else:
-        # No quantization
-        node_instance = node_builder(node)
+        _, activation_quantizers = get_quantization_quantizers(node)
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node we no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return PytorchActivationQuantizationHolder(activation_quantizers[0])
+        Logger.error(
+            f'PytorchActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
+            f'were found for node {node}')
 
-    return node_instance
+    def get_exportable_pytorch_model(graph: Graph):
+        """
+        Convert graph to fully quantized PyTorch model.
+
+        Args:
+            graph: Graph to convert to a PyTorch model.
+
+        Returns:
+            Fully quantized PyTorch model.
+        """
+        return PyTorchModelBuilder(graph=graph,
+                                   wrapper=fully_quantized_wrapper,
+                                   get_activation_quantizer_holder_fn=get_activation_quantizer_holder).build_model()
+
+else:
+    def get_exportable_pytorch_model(*args, **kwargs):  # pragma: no cover
+        Logger.error('Installing torch is mandatory '
+                     'when using get_exportable_pytorch_model. '
+                     'Could not find PyTorch package.')
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py`

 * *Ordering differences only*

 * *Files 0% similar despite different names*

```diff
@@ -7,8 +7,8 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-# ==============================================================================
+# ==============================================================================
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/ste_weights_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py`

 * *Files 25% similar despite different names*

```diff
@@ -8,96 +8,87 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-import torch
-import torch.nn as nn
-from typing import List, Union
-from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfig
-from model_compression_toolkit.gptq.pytorch.quantizer.gptq_quantizer import BaseWeightQuantizer
-from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
-from model_compression_toolkit.gptq.pytorch.quantizer.quant_utils import ste_round, ste_clip
-from model_compression_toolkit.gptq.common.gptq_constants import AUXVAR
-from model_compression_toolkit.core.common.quantization.node_quantization_config import NodeWeightsQuantizationConfig
-from model_compression_toolkit.core.common.constants import THRESHOLD
-
-
-class STEWeightQuantizer(BaseWeightQuantizer):
-    """
-    Class that implements a quantizer with trainable parameters to be used for GPTQ training.
-    """
-
-    def __init__(self,
-                 weights_quantization_cfg: NodeWeightsQuantizationConfig,
-                 gptq_config: GradientPTQConfig,
-                 weight: torch.nn.Parameter):
+
+from typing import Tuple, Callable
+from model_compression_toolkit.core import common
+from model_compression_toolkit.core.common import Graph
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.core.common.user_info import UserInformation
+from model_compression_toolkit.logger import Logger
+from mct_quantizers import KerasActivationQuantizationHolder
+
+if FOUND_TF:
+    import tensorflow as tf
+    from tensorflow.keras.layers import Layer
+    from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
+    from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizers import get_quantization_quantizers
+    from mct_quantizers import KerasQuantizationWrapper
+
+    def _get_wrapper(node: common.BaseNode,
+                     layer: Layer) -> Layer:
         """
-        Construct a Pytorch model that utilize a fake weight quantizer of STE (Straight Through Estimator) for symmetric quantizer.
+        A function which takes a computational graph node and a keras layer and perform the quantization wrapping
         Args:
-            weights_quantization_cfg: Configuration of weight quantization
-            gptq_config: GradientPTQConfig object with parameters about the tuning process.
-            weight: weight for auxiliary tensor creation.
-        """
-        super().__init__()
+            node: A node of mct graph.
+            layer: A keras layer
 
-        self.signed = True
-        self.num_bits = weights_quantization_cfg.weights_n_bits
-        self.min_int = -int(self.signed) * (2 ** (self.num_bits - int(self.signed)))
-        self.max_int = (2 ** (self.num_bits - int(self.signed))) - 1
-        self.weight_shape = weight.shape
-        self.threshold_values = weights_quantization_cfg.weights_quantization_params.get(THRESHOLD)
-        self.delta_tensor = self.threshold_values / (2 ** (self.num_bits-int(self.signed)))
-        self.max_delta_change = gptq_config.lsb_change_per_bit_width.get(self.num_bits)
+        Returns: Wrapped layer with weights quantizers and activation quantizers
 
-        # Set trainable tensors
-        self.set_trainable_params()
+        """
+        weights_quantizers, _ = get_quantization_quantizers(node)
+        if len(weights_quantizers) > 0:
+            return KerasQuantizationWrapper(layer,
+                                            weights_quantizers)
+        return layer
 
-        # Create tensors
-        self.delta_tensor = to_torch_tensor(self.delta_tensor)
-        self.max_tensor_change = self.delta_tensor * self.max_delta_change
 
-    def set_trainable_params(self):
-        """
-        A function to set a list of trainable parameters of the quantizer for GPTQ retraining
+    def get_activation_quantizer_holder(node: common.BaseNode) -> Callable:
         """
-        self.aux_tensor = nn.Parameter(to_torch_tensor(torch.zeros(self.weight_shape)), requires_grad=True)
-        self.trainable_params.update({AUXVAR: self.aux_tensor})
+        Retrieve a ActivationQuantizationHolder layer to use for activation quantization for a node.
 
-    def get_aux_variable(self) -> torch.Tensor:
-        """
-        Returns auxiliary trainable variables
-        """
-        return self.trainable_params.get(AUXVAR)
+        Args:
+            node: Node to get ActivationQuantizationHolder to attach in its output.
 
-    def get_quantization_variable(self) -> Union[torch.Tensor, List]:
-        """
-        Returns quantization trainable variables
+        Returns:
+            A ActivationQuantizationHolder layer for the node activation quantization.
         """
-        return []
+        _, activation_quantizers = get_quantization_quantizers(node)
+
+        # Holder by definition uses a single quantizer for the activation quantization
+        # thus we make sure this is the only possible case (unless it's a node with no activation
+        # quantization, which in this case has an empty list).
+        if len(activation_quantizers) == 1:
+            return KerasActivationQuantizationHolder(activation_quantizers[0])
+
+        Logger.error(
+            f'ActivationQuantizationHolder supports a single quantizer but {len(activation_quantizers)} quantizers '
+            f'were found for node {node}')
 
-    def get_weight_quantization_params(self) -> dict:
-        """
-        Returns weight quantization dictionary params
-        """
-        return {THRESHOLD: self.threshold_values}
 
-    def forward(self, w: nn.Parameter, training: bool = True) -> nn.Parameter:
+
+    def get_exportable_keras_model(graph: Graph) -> Tuple[tf.keras.models.Model, UserInformation]:
         """
-        Weight fake quantizer
+        Convert graph to an exportable Keras model (model with all quantization parameters).
+        An exportable model can then be exported using model_exporter, to retrieve the
+        final exported model.
+
         Args:
-            w: weights to quantize.
-            training: whether in training mode or not
+            graph: Graph to convert to an exportable Keras model.
+
         Returns:
-            quantized weights
+            Exportable Keras model and user information.
         """
-        v0 = ste_clip(self.aux_tensor, min_val=-self.max_tensor_change, max_val=self.max_tensor_change)
-        v1 = v0 / self.delta_tensor
-        w0 = torch.round(w / self.delta_tensor).detach()
-        w1 = w0 + v1
-        w2 = ste_round(w1)
-        w3 = ste_clip(w2, min_val=self.min_int, max_val=self.max_int)
-        w_q = self.delta_tensor * w3
-        return w_q
-
+        exportable_model, user_info = KerasModelBuilder(graph=graph,
+                                                        wrapper=_get_wrapper,
+                                                        get_activation_quantizer_holder_fn=get_activation_quantizer_holder).build_model()
+        exportable_model.trainable = False
+        return exportable_model, user_info
+else:
+    def get_exportable_keras_model(*args, **kwargs):  # pragma: no cover
+        Logger.error('Installing tensorflow and tensorflow_model_optimization is mandatory '
+                     'when using get_exportable_keras_model. '
+                     'Could not find Tensorflow package.')
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/gptq/runner.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/runner.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,27 +11,28 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.statistics_correction.statistics_correction import \
     apply_statistics_correction
 from model_compression_toolkit.gptq.common.gptq_config import GradientPTQConfigV2
 from model_compression_toolkit.core.common.framework_implementation import FrameworkImplementation
 from model_compression_toolkit.core.common import FrameworkInfo
 from model_compression_toolkit.core.common.graph.base_graph import Graph
 from model_compression_toolkit.gptq.common.gptq_training import gptq_training
 
 from model_compression_toolkit.core.common.visualization.tensorboard_writer import TensorboardWriter
 from model_compression_toolkit.core.common.statistics_correction.apply_bias_correction_to_graph import \
     apply_bias_correction_to_graph
+from model_compression_toolkit.logger import Logger
 
 
 def _apply_gptq(gptq_config: GradientPTQConfigV2,
                 representative_data_gen: Callable,
                 tb_w: TensorboardWriter,
                 tg: Graph,
                 tg_bias: Graph,
@@ -51,15 +52,15 @@
         tg_bias: Graph of quantized model.
         fw_info: Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).
         fw_impl: Framework implementation per framework
     Returns:
 
     """
     if gptq_config is not None and gptq_config.n_epochs > 0:
-        common.Logger.info("Using experimental Gradient Based PTQ: If you encounter an issue "
+        Logger.info("Using experimental Gradient Based PTQ: If you encounter an issue "
                            "please file a bug. To disable it, do not pass a gptq configuration.")
 
         tg_bias = gptq_training(tg,
                                 tg_bias,
                                 gptq_config,
                                 representative_data_gen,
                                 fw_impl,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/keras/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-# ==============================================================================
+# ==============================================================================
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/keras/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/keras/quantization_facade.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,45 +11,44 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 
-from model_compression_toolkit import CoreConfig
-from model_compression_toolkit.core import common
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import TENSORFLOW, FOUND_TF
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import TENSORFLOW, FOUND_TF
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.ptq.runner import ptq_runner
 
 if FOUND_TF:
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
     from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
     from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
     from tensorflow.keras.models import Model
-    from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.exporter.model_wrapper import get_exportable_keras_model
 
     from model_compression_toolkit import get_target_platform_capabilities
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
     def keras_post_training_quantization_experimental(in_model: Model,
                                                       representative_data_gen: Callable,
                                                       target_kpi: KPI = None,
                                                       core_config: CoreConfig = CoreConfig(),
                                                       target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_KERAS_TPC,
-                                                      new_experimental_exporter: bool = False):
+                                                      new_experimental_exporter: bool = True):
         """
          Quantize a trained Keras model using post-training quantization. The model is quantized using a
          symmetric constraint quantization thresholds (power of two).
          The model is first optimized using several transformations (e.g. BatchNormalization folding to
          preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
          being collected for each layer's output (and input, depends on the quantization configuration).
          For each possible bit width (per layer) a threshold is then being calculated using the collected
@@ -61,15 +60,15 @@
 
          Args:
              in_model (Model): Keras model to quantize.
              representative_data_gen (Callable): Dataset used for calibration.
              target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
              core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
              target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the Keras model according to.
-             new_experimental_exporter (bool): Whether exporting the quantized model using new exporter or not (in progress. Avoiding it for now is recommended).
+             new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
          Returns:
 
              A quantized model and information the user may need to handle the quantized model.
 
          Examples:
 
@@ -89,50 +88,50 @@
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 224, 224, 3))]
 
             Create a MCT core config, containing the quantization configuration:
 
-            >>> config = mct.CoreConfig()
+            >>> config = mct.core.CoreConfig()
 
             If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
             The candidates bitwidth for quantization should be defined in the target platform model.
             In this example we use 1 image to search mixed-precision configuration:
 
-            >>> config = mct.CoreConfig(mixed_precision_config=mct.MixedPrecisionQuantizationConfigV2(num_of_images=1))
+            >>> config = mct.core.CoreConfig(mixed_precision_config=mct.core.MixedPrecisionQuantizationConfigV2(num_of_images=1))
 
             For mixed-precision set a target KPI object:
             Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
             that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
             while the bias will not):
 
-            >>> kpi = mct.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+            >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
             Pass the model, the representative dataset generator, the configuration and the target KPI to get a
             quantized model:
 
-            >>> quantized_model, quantization_info = mct.keras_post_training_quantization_experimental(model, repr_datagen, kpi, core_config=config)
+            >>> quantized_model, quantization_info = mct.ptq.keras_post_training_quantization_experimental(model, repr_datagen, kpi, core_config=config)
 
             For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
 
          """
 
         fw_info = DEFAULT_KERAS_INFO
 
         KerasModelValidation(model=in_model,
                              fw_info=fw_info).validate()
 
         if core_config.mixed_precision_enable:
             if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+                Logger.error("Given quantization config to mixed-precision facade is not of type "
                                     "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization "
                                     "API, or pass a valid mixed precision configuration.")  # pragma: no cover
 
-            common.Logger.info("Using experimental mixed-precision quantization. "
+            Logger.info("Using experimental mixed-precision quantization. "
                                "If you encounter an issue please file a bug.")
 
         tb_w = _init_tensorboard_writer(fw_info)
 
         fw_impl = KerasImplementation()
 
         tg, bit_widths_config = core_runner(in_model=in_model,
@@ -149,16 +148,19 @@
         if core_config.debug_config.analyze_similarity:
             analyzer_model_quantization(representative_data_gen,
                                         tb_w, tg,
                                         fw_impl,
                                         fw_info)
 
         if new_experimental_exporter:
-            Logger.warning('Using new experimental exported models. '
-                           'Please do not use unless you are familiar with what you are doing')
+            Logger.warning('Using new experimental wrapped and ready for export models. To '
+                           'disable it, please set new_experimental_exporter to False when '
+                           'calling keras_post_training_quantization_experimental. '
+                           'If you encounter an issue please file a bug.')
+
             return get_exportable_keras_model(tg)
 
         return export_model(tg,
                             fw_info,
                             fw_impl,
                             tb_w,
                             bit_widths_config)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/pytorch/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-# ==============================================================================
+# ==============================================================================
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/pytorch/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/pytorch/quantization_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -11,43 +11,43 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Callable
 
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import PYTORCH, FOUND_TORCH
-from model_compression_toolkit.core.common.target_platform import TargetPlatformCapabilities
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import PYTORCH, FOUND_TORCH
+from model_compression_toolkit.target_platform_capabilities.target_platform import TargetPlatformCapabilities
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfigV2
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.ptq.runner import ptq_runner
 from model_compression_toolkit.core.exporter import export_model
 from model_compression_toolkit.core.analyzer import analyzer_model_quantization
 
 
 if FOUND_TORCH:
     from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
     from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.core.pytorch.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from torch.nn import Module
     from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.fully_quantized_model_builder import get_exportable_pytorch_model
     from model_compression_toolkit import get_target_platform_capabilities
 
     DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
     def pytorch_post_training_quantization_experimental(in_module: Module,
                                                         representative_data_gen: Callable,
                                                         target_kpi: KPI = None,
                                                         core_config: CoreConfig = CoreConfig(),
                                                         target_platform_capabilities: TargetPlatformCapabilities = DEFAULT_PYTORCH_TPC,
-                                                        new_experimental_exporter: bool = False):
+                                                        new_experimental_exporter: bool = True):
         """
         Quantize a trained Pytorch module using post-training quantization.
         By default, the module is quantized using a symmetric constraint quantization thresholds
         (power of two) as defined in the default TargetPlatformCapabilities.
         The module is first optimized using several transformations (e.g. BatchNormalization folding to
         preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
         being collected for each layer's output (and input, depends on the quantization configuration).
@@ -59,15 +59,15 @@
 
         Args:
             in_module (Module): Pytorch module to quantize.
             representative_data_gen (Callable): Dataset used for calibration.
             target_kpi (KPI): KPI object to limit the search of the mixed-precision configuration as desired.
             core_config (CoreConfig): Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.
             target_platform_capabilities (TargetPlatformCapabilities): TargetPlatformCapabilities to optimize the PyTorch model according to.
-            new_experimental_exporter (bool): Whether exporting the quantized model using new exporter or not (in progress. Avoiding it for now is recommended).
+            new_experimental_exporter (bool): Whether to wrap the quantized model using quantization information or not. Enabled by default. Experimental and subject to future changes.
 
         Returns:
             A quantized module and information the user may need to handle the quantized module.
 
         Examples:
 
             Import a Pytorch module:
@@ -84,26 +84,26 @@
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 3, 224, 224))]
 
             Import MCT and pass the module with the representative dataset generator to get a quantized module
             Set number of clibration iterations to 1:
 
             >>> import model_compression_toolkit as mct
-            >>> quantized_module, quantization_info = mct.pytorch_post_training_quantization_experimental(module, repr_datagen)
+            >>> quantized_module, quantization_info = mct.ptq.pytorch_post_training_quantization_experimental(module, repr_datagen)
 
         """
 
         if core_config.mixed_precision_enable:
             if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+                Logger.error("Given quantization config to mixed-precision facade is not of type "
                                     "MixedPrecisionQuantizationConfigV2. Please use "
                                     "pytorch_post_training_quantization API, or pass a valid mixed precision "
                                     "configuration.")  # pragma: no cover
 
-            common.Logger.info("Using experimental mixed-precision quantization. "
+            Logger.info("Using experimental mixed-precision quantization. "
                                "If you encounter an issue please file a bug.")
 
         tb_w = _init_tensorboard_writer(DEFAULT_PYTORCH_INFO)
 
         fw_impl = PytorchImplementation()
 
         tg, bit_widths_config = core_runner(in_model=in_module,
@@ -121,16 +121,18 @@
             analyzer_model_quantization(representative_data_gen,
                                         tb_w,
                                         tg,
                                         fw_impl,
                                         DEFAULT_PYTORCH_INFO)
 
         if new_experimental_exporter:
-            Logger.warning('Using new experimental exported models. '
-                           'Please do not use unless you are familiar with what you are doing')
+            Logger.warning('Using new experimental wrapped and ready for export models. To '
+                           'disable it, please set new_experimental_exporter to False when '
+                           'calling pytorch_post_training_quantization_experimental. '
+                           'If you encounter an issue please file a bug.')
 
             return get_exportable_pytorch_model(tg)
 
         quantized_model, user_info = export_model(tg,
                                                   DEFAULT_PYTORCH_INFO,
                                                   fw_impl,
                                                   tb_w,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/ptq/runner.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/runner.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-# ==============================================================================
+# ==============================================================================
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/common/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,9 +8,9 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-from model_compression_toolkit.qat.common.constants import THRESHOLD_TENSOR, WEIGHTS_QUANTIZATION_PARAMS
+from model_compression_toolkit.trainable_infrastructure.common.constants import THRESHOLD_TENSOR, \
+    WEIGHTS_QUANTIZATION_PARAMS
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/constants.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -9,11 +9,9 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-FQ_MIN = "min"
-FQ_MAX = "max"
-THRESHOLD_TENSOR = "ptq_threshold_tensor"
-WEIGHTS_QUANTIZATION_PARAMS = 'weights_quantization_params'
+import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.symmetric_ste
+import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.uniform_ste
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/qat_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/common/qat_config.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,39 +13,45 @@
 # limitations under the License.
 # ==============================================================================
 
 from typing import Dict
 from enum import Enum
 from model_compression_toolkit.core import common
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.logger import Logger
 
-def _is_qat_applicable(node: common.BaseNode,
-                       fw_info: FrameworkInfo) -> bool:
+
+def is_qat_applicable(node: common.BaseNode,
+                      fw_info: FrameworkInfo) -> bool:
     """
     A function for deciding if a layer should be fine-tuned during QAT
     Args:
         node (BaseNode): Node for quantization decision
         fw_info (FrameworkInfo): Pytorch quantization information
 
     Returns:
         A boolean whether the layer is to be wrapped with a QuantizeWrapper
     """
 
     if node.is_weights_quantization_enabled() and not fw_info.is_kernel_op(node.type):
-        common.Logger.error("QAT Error: Quantizing a node without a kernel isn't supported")
+        Logger.error("QAT Error: Quantizing a node without a kernel isn't supported")
     return node.is_weights_quantization_enabled() or node.is_activation_quantization_enabled()
 
 
 class TrainingMethod(Enum):
     """
     An enum for selecting a QAT training method
 
     STE - Standard straight-through estimator. Includes PowerOfTwo, symmetric & uniform quantizers
+
+    DQA -  DNN Quantization with Attention. Includes a smooth quantization introduces by DQA method
+
     """
     STE = "STE",
+    DQA = "DQA"
 
 
 class QATConfig:
     """
     QAT configuration class.
     """
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/common/qat_get_quantizer_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,59 +9,64 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List
-from model_compression_toolkit.core.common import BaseNode, Logger
-from model_compression_toolkit.quantizers_infrastructure.common.trainable_quantizer_config import \
+from model_compression_toolkit.core.common import BaseNode
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig, TrainableQuantizerCandidateConfig
 
 
 def get_trainable_quantizer_weights_config(
         n: BaseNode,
-        weights_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None) -> TrainableQuantizerWeightsConfig:
+        weights_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None
+) -> TrainableQuantizerWeightsConfig:
     """
     Returns the relevant configuration for weights trainable quantizer
 
     Args:
-        n: BaseNode - the node to build a trainable quantizer from
+        n: BaseNode - the node to build a trainable quantizer from.
+        weights_quantization_candidates: A list of weights quantizer config candidates.
 
     Returns:
          TrainableQuantizerWeightsConfig: an object that contains the quantizer configuration
     """
     if n.final_weights_quantization_cfg is None:
-        Logger.error(f'Node must have final_weights_quantization_cfg in order to build quantizer configuration')
+        Logger.error(f'Node must have final_weights_quantization_cfg in order to build quantizer configuration')  # pragma: no cover
 
     final_cfg = n.final_weights_quantization_cfg
     return TrainableQuantizerWeightsConfig(final_cfg.weights_quantization_method,
                                            final_cfg.weights_n_bits,
                                            final_cfg.weights_quantization_params,
                                            final_cfg.enable_weights_quantization,
                                            final_cfg.weights_channels_axis,
                                            final_cfg.weights_per_channel_threshold,
                                            final_cfg.min_threshold,
                                            weights_quantization_candidates)
 
 
 def get_trainable_quantizer_activation_config(
         n: BaseNode,
-        activation_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None) -> TrainableQuantizerActivationConfig:
+        activation_quantization_candidates: List[TrainableQuantizerCandidateConfig] = None
+) -> TrainableQuantizerActivationConfig:
     """
     Returns configuration for activation trainable quantizer
 
     Args:
-        n: BaseNode - the node to build a trainable quantizer from
+        n: BaseNode - the node to build a trainable quantizer from.
+        activation_quantization_candidates: A list of activation quantizer candidates config.
 
     Returns:
          TrainableQuantizerActivationConfig - an object that contains the quantizer configuration
     """
     if n.final_activation_quantization_cfg is None:
-        Logger.error(f'Node must have final_activation_quantization_cfg in order to build quantizer configuration')
+        Logger.error(f'Node must have final_activation_quantization_cfg in order to build quantizer configuration')  # pragma: no cover
 
     final_cfg = n.final_activation_quantization_cfg
     return TrainableQuantizerActivationConfig(final_cfg.activation_quantization_method,
                                               final_cfg.activation_n_bits,
                                               final_cfg.activation_quantization_params,
                                               final_cfg.enable_activation_quantization,
                                               final_cfg.min_threshold,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -8,7 +8,10 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+
+import model_compression_toolkit.qat.keras.quantizer.ste_rounding.symmetric_ste
+import model_compression_toolkit.qat.keras.quantizer.ste_rounding.uniform_ste
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantization_facade.py`

 * *Files 8% similar despite different names*

```diff
@@ -12,71 +12,74 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
 from typing import Callable
 from functools import partial
 
-from model_compression_toolkit import CoreConfig
-from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import TENSORFLOW, FOUND_TF
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
+from model_compression_toolkit.core import CoreConfig
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TF
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from mct_quantizers import KerasActivationQuantizationHolder, KerasQuantizationWrapper
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import TargetPlatformCapabilities
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.ptq.runner import ptq_runner
 
 if FOUND_TF:
     import tensorflow as tf
     from tensorflow.keras.layers import Layer
     from tensorflow.keras.models import Model
 
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
     from model_compression_toolkit.core.keras.keras_implementation import KerasImplementation
     from model_compression_toolkit.core.keras.keras_model_validation import KerasModelValidation
-    from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
 
     from model_compression_toolkit.core.keras.back2framework.keras_model_builder import KerasModelBuilder
 
     from model_compression_toolkit import get_target_platform_capabilities
-    from model_compression_toolkit import quantizers_infrastructure as qi
 
     from model_compression_toolkit import get_target_platform_capabilities
     from model_compression_toolkit.core import common
     from model_compression_toolkit.core.common import BaseNode
-    from model_compression_toolkit.core.common.constants import TENSORFLOW
+    from model_compression_toolkit.constants import TENSORFLOW
     from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-    from model_compression_toolkit.qat.common.qat_config import _is_qat_applicable
-    from model_compression_toolkit.core.keras.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
-    from model_compression_toolkit.qat.keras.quantizer.quantization_builder import quantization_builder
+    from model_compression_toolkit.qat.keras.quantizer.quantization_builder import quantization_builder, \
+    get_activation_quantizer_holder
     from model_compression_toolkit.qat.common.qat_config import QATConfig
-    from model_compression_toolkit import quantizers_infrastructure as qi
 
     DEFAULT_KERAS_TPC = get_target_platform_capabilities(TENSORFLOW, DEFAULT_TP_MODEL)
 
 
-    def qat_wrapper(n: common.BaseNode, layer: Layer, qat_config):
+    def qat_wrapper(n: common.BaseNode,
+                    layer: Layer,
+                    qat_config: QATConfig):
         """
         A function which takes a computational graph node and a keras layer and perform the quantization wrapping
         Args:
+            qat_config: Configuration of QAT (such as training methods for example).
             n: A node of mct graph.
-            layer: A keras layer
+            layer: A keras layer.
 
         Returns: Wrapped layer
 
         """
-        if _is_qat_applicable(n, DEFAULT_KERAS_INFO):
-            weights_quantizers, activation_quantizers = quantization_builder(n, qat_config, DEFAULT_KERAS_INFO)
-            return qi.KerasQuantizationWrapper(layer, weights_quantizers, activation_quantizers)
-        else:
-            return layer
+        if is_qat_applicable(n, DEFAULT_KERAS_INFO):
+            weights_quantizers, _ = quantization_builder(n,
+                                                         qat_config,
+                                                         DEFAULT_KERAS_INFO)
+            if len(weights_quantizers) > 0:
+                return KerasQuantizationWrapper(layer, weights_quantizers)
+        return layer
 
 
     def keras_quantization_aware_training_init(in_model: Model,
                                                representative_data_gen: Callable,
                                                target_kpi: KPI = None,
                                                core_config: CoreConfig = CoreConfig(),
                                                qat_config: QATConfig = QATConfig(),
@@ -130,50 +133,50 @@
              >>> num_calibration_batches = 10
              >>> def repr_datagen():
              >>>     for _ in range(num_calibration_batches):
              >>>         yield [np.random.random((4, 224, 224, 3))]
 
              Create a MCT core config, containing the quantization configuration:
 
-             >>> config = mct.CoreConfig()
+             >>> config = mct.core.CoreConfig()
 
              If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
              The candidates bitwidth for quantization should be defined in the target platform model:
 
-             >>> config = mct.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfigV2())
+             >>> config = mct.core.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfigV2())
 
              For mixed-precision set a target KPI object:
              Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
              that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
              while the bias will not):
 
-             >>> kpi = mct.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+             >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
              Pass the model, the representative dataset generator, the configuration and the target KPI to get a
              quantized model:
 
-             >>> quantized_model, quantization_info, custom_objects = mct.keras_quantization_aware_training_init(model, repr_datagen, kpi, core_config=config)
+             >>> quantized_model, quantization_info, custom_objects = mct.qat.keras_quantization_aware_training_init(model, repr_datagen, kpi, core_config=config)
 
              Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:
 
              >>> quantized_model = tf.keras.models.load_model(model_file, custom_objects=custom_objects)
 
              For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
 
          """
         KerasModelValidation(model=in_model,
                              fw_info=fw_info).validate()
 
         if core_config.mixed_precision_enable:
             if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                common.Logger.error("Given quantization config to mixed-precision facade is not of type "
+                Logger.error("Given quantization config to mixed-precision facade is not of type "
                                     "MixedPrecisionQuantizationConfigV2. Please use keras_post_training_quantization API,"
                                     "or pass a valid mixed precision configuration.")
 
-            common.Logger.info("Using experimental mixed-precision quantization. "
+            Logger.info("Using experimental mixed-precision quantization. "
                                "If you encounter an issue please file a bug.")
 
         tb_w = _init_tensorboard_writer(fw_info)
 
         fw_impl = KerasImplementation()
 
         tg, bit_widths_config = core_runner(in_model=in_model,
@@ -184,15 +187,19 @@
                                             tpc=target_platform_capabilities,
                                             target_kpi=target_kpi,
                                             tb_w=tb_w)
 
         tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
 
         _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
-        qat_model, user_info = KerasModelBuilder(graph=tg, fw_info=fw_info, wrapper=_qat_wrapper).build_model()
+        qat_model, user_info = KerasModelBuilder(graph=tg,
+                                                 fw_info=fw_info,
+                                                 wrapper=_qat_wrapper,
+                                                 get_activation_quantizer_holder_fn=partial(get_activation_quantizer_holder,
+                                                                                            qat_config=qat_config)).build_model()
 
         user_info.mixed_precision_cfg = bit_widths_config
         #TODO: remove the last output after updating documentation.
         return qat_model, user_info, {}
 
 
     def keras_quantization_aware_training_finalize(in_model: Model) -> Model:
@@ -219,42 +226,51 @@
              Create a random dataset generator:
 
              >>> import numpy as np
              >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
 
              Create a MCT core config, containing the quantization configuration:
 
-             >>> config = mct.CoreConfig()
+             >>> config = mct.core.CoreConfig()
 
              If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
              The candidates bitwidth for quantization should be defined in the target platform model:
 
-             >>> config = mct.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfigV2())
+             >>> config = mct.core.CoreConfig(mixed_precision_config=MixedPrecisionQuantizationConfigV2())
 
              For mixed-precision set a target KPI object:
              Create a KPI object to limit our returned model's size. Note that this value affects only coefficients
              that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
              while the bias will not):
 
-             >>> kpi = mct.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
+             >>> kpi = mct.core.KPI(model.count_params() * 0.75)  # About 0.75 of the model size when quantized with 8 bits.
 
              Pass the model, the representative dataset generator, the configuration and the target KPI to get a
              quantized model:
 
-             >>> quantized_model, quantization_info, custom_objects = mct.keras_quantization_aware_training_init(model, repr_datagen, kpi, core_config=config)
+             >>> quantized_model, quantization_info, custom_objects = mct.qat.keras_quantization_aware_training_init(model, repr_datagen, kpi, core_config=config)
 
              Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:
 
              >>> quantized_model = tf.keras.models.load_model(model_file, custom_objects=custom_objects)
-             >>> quantized_model = mct.keras_quantization_aware_training_finalize(quantized_model)
+             >>> quantized_model = mct.qat.keras_quantization_aware_training_finalize(quantized_model)
 
          """
         def _export(layer):
-            if isinstance(layer, qi.KerasQuantizationWrapper):
-                layer.convert_to_inferable_quantizers()
+            if isinstance(layer, KerasQuantizationWrapper):
+                layer = layer.convert_to_inferable_quantizers()
+            # In the KerasActivationQuantizationHolder case - converting the quantizers only
+            # is not enough. We need to create a new layer with inferable quantizers. The reason for that
+            # is that if we only convert the quantizers, the layer will have some weights (such as min, max,
+            # threshold) that do not match the configuration, thus loading such a model will fail.
+            # To overcome this, the convert_to_inferable_quantizers of KerasActivationQuantizationHolder
+            # creates a new layer from its new configuration after converting the trainable quantizer
+            # to an inferable quantizer.
+            elif isinstance(layer, KerasActivationQuantizationHolder):
+                layer = layer.convert_to_inferable_quantizers()
             return layer
 
         # clone each layer in the model and apply _export to layers with TrainableQuantizeWrappers
         exported_model = tf.keras.models.clone_model(in_model, input_tensors=None, clone_function=_export)
 
         return exported_model
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -9,9 +9,10 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-import model_compression_toolkit.qat.keras.quantizer.ste_rounding.symmetric_ste
-import model_compression_toolkit.qat.keras.quantizer.ste_rounding.uniform_ste
+import model_compression_toolkit.gptq.keras.quantizer.ste_rounding.symmetric_ste
+import model_compression_toolkit.gptq.keras.quantizer.soft_rounding.symmetric_soft_quantizer
+import model_compression_toolkit.gptq.keras.quantizer.soft_rounding.uniform_soft_quantizer
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,19 +10,18 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import Union
 
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import FOUND_TF
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TF
 
-from model_compression_toolkit.quantizers_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure import TrainableQuantizerWeightsConfig, \
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
     TrainableQuantizerActivationConfig, BaseKerasTrainableQuantizer
 
 if FOUND_TF:
 
     class BaseKerasQATTrainableQuantizer(BaseKerasTrainableQuantizer):
         """
         A base class for trainable Keras quantizer for QAT.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/quant_utils.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/quant_utils.py`

 * *Files identical despite different names*

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/quantization_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,76 +1,76 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Tuple, Dict, List
+from typing import Dict, List, Tuple
 
+from model_compression_toolkit.gptq import GradientPTQConfigV2
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.qat.common.qat_get_quantizer import get_quantizer_class
-from model_compression_toolkit.qat.common.qat_get_quantizer_config import get_trainable_quantizer_weights_config, \
-    get_trainable_quantizer_activation_config, get_trainable_quantizer_quantization_candidates
-from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
-from model_compression_toolkit.qat.common.qat_config import QATConfig
-from model_compression_toolkit.quantizers_infrastructure import QuantizationTarget
+from model_compression_toolkit.core.keras.default_framework_info import DEFAULT_KERAS_INFO
+from model_compression_toolkit.exporter.model_wrapper.keras.builder.node_to_quantizer import \
+    get_inferable_quantizer_kwargs
+from model_compression_toolkit.gptq.common.gptq_graph import get_kernel_attribute_name_for_gptq
+from model_compression_toolkit.gptq.keras.quantizer.base_keras_gptq_quantizer import BaseKerasGPTQTrainableQuantizer
+from mct_quantizers import QuantizationTarget
+from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
+from mct_quantizers.keras.quantizers import BaseKerasInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
+    get_trainable_quantizer_weights_config
+from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
+    get_trainable_quantizer_class
 
 
 def quantization_builder(n: common.BaseNode,
-                         qat_config: QATConfig,
-                         fw_info: FrameworkInfo,
-                         ) -> Tuple[Dict[str, BaseKerasQATTrainableQuantizer], List[BaseKerasQATTrainableQuantizer]]:
+                         gptq_config: GradientPTQConfigV2
+                         ) -> Tuple[Dict[str, BaseKerasGPTQTrainableQuantizer], List[BaseKerasInferableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration and
     a global NoOpQuantizeConfig object.
 
     Args:
         n: Node to build its QuantizeConfig.
-        qat_config (QATConfig): QAT configuration
-        fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
+        gptq_config (GradientPTQConfigV2): GradientPTQConfigV2 configuration.
 
     Returns:
-        weights_quantizers: A dictionary between a weight's name to its quantizer.
-        activation_quantizers: A list of activations quantization, one for each layer output.
+        A dictionary which maps the weights kernel attribute to a quantizer for GPTQ training.
+        Note that we return a dictionary although there is only a single attribute that is being mapped to a quantizer,
+        to be compatible with the quantization infrastructure template.
     """
-    if len(n.candidates_quantization_cfg) > 1:
-        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n)
-    else:
-        wq_cand, aq_cand = None, None
 
-    weight_quantizers = {}
+    weights_quantizers = {}
     if n.is_weights_quantization_enabled():
         quant_method = n.final_weights_quantization_cfg.weights_quantization_method
 
-        quantizer_class = get_quantizer_class(QuantizationTarget.Weights,
-                                              qat_config.weight_training_method,
-                                              quant_method,
-                                              BaseKerasQATTrainableQuantizer)
-        attributes = fw_info.get_kernel_op_attributes(n.type)
-        for attr in attributes:
-            weight_quantizers.update({attr: quantizer_class(get_trainable_quantizer_weights_config(n, wq_cand),
-                                                            **qat_config.weight_quantizer_params_override)})
+        quantizer_class = get_trainable_quantizer_class(quant_target=QuantizationTarget.Weights,
+                                                        quantizer_type=gptq_config.rounding_type,
+                                                        quant_method=quant_method,
+                                                        quantizer_base_class=BaseKerasGPTQTrainableQuantizer)
+        kernel_attribute = get_kernel_attribute_name_for_gptq(layer_type=n.type,
+                                                              fw_info=DEFAULT_KERAS_INFO)
+
+        weights_quantizers.update({kernel_attribute: quantizer_class(get_trainable_quantizer_weights_config(n),
+                                                                     **gptq_config.gptq_quantizer_params_override)})
 
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
-        # single output -> normalize to list of output_shapes
-        output_shapes = n.output_shape if isinstance(n.output_shape[0], (list, tuple)) else [n.output_shape]
 
-        quantizer_class = get_quantizer_class(QuantizationTarget.Activation,
-                                              qat_config.activation_training_method,
-                                              quant_method,
-                                              BaseKerasQATTrainableQuantizer)
+        quantizer_class = get_inferable_quantizer_class(quant_target=QuantizationTarget.Activation,
+                                                        quant_method=quant_method,
+                                                        quantizer_base_class=BaseKerasInferableQuantizer)
+
+        kwargs = get_inferable_quantizer_kwargs(n, QuantizationTarget.Activation)
 
-        activation_quantizers = [quantizer_class(get_trainable_quantizer_activation_config(n, aq_cand),
-                                                 **qat_config.activation_quantizer_params_override)] * len(output_shapes)
+        activation_quantizers.append(quantizer_class(**kwargs))
 
-    return weight_quantizers, activation_quantizers
+    return weights_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 8% similar despite different names*

```diff
@@ -9,56 +9,60 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-from typing import Dict, Union
+from typing import Union
 
 import numpy as np
 import tensorflow as tf
 from tensorflow.python.framework.tensor_shape import TensorShape
-from model_compression_toolkit.core.common.constants import SIGNED
+from model_compression_toolkit.constants import SIGNED
+from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
 
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.qat import TrainingMethod
+
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget, mark_quantizer, KerasQuantizationWrapper
 from model_compression_toolkit.qat.common import THRESHOLD_TENSOR
-from model_compression_toolkit.qat.common.constants import FQ_MIN, FQ_MAX
-from model_compression_toolkit import quantizers_infrastructure as qi, TrainingMethod
-from model_compression_toolkit.core.common import constants as C
-import model_compression_toolkit.quantizers_infrastructure.keras.inferable_quantizers as iq
+from model_compression_toolkit import constants as C
+
 from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure import TrainableQuantizerWeightsConfig, \
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
     TrainableQuantizerActivationConfig
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import mark_quantizer
+from mct_quantizers.keras.quantizers import WeightsPOTInferableQuantizer, WeightsSymmetricInferableQuantizer, \
+    ActivationPOTInferableQuantizer, ActivationSymmetricInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Weights,
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
                 quantizer_type=TrainingMethod.STE)
-class STEWeightQuantizer(BaseKerasQATTrainableQuantizer):
+class STEWeightQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer inputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
         Initialize a TrainableWeightQuantizer object with parameters to use
         for the quantization.
 
         Args:
             quantization_config: trainable quantizer config class
         """
         super().__init__(quantization_config)
         self.power_of_two = quantization_config.weights_quantization_method == QuantizationMethod.POWER_OF_TWO
-        self.threshold_values = quantization_config.weights_quantization_params[C.THRESHOLD]
+        self.threshold_values = np.array(quantization_config.weights_quantization_params[C.THRESHOLD])
         self.threshold_shape = np.asarray(self.threshold_values).shape
         self.per_channel = self.quantization_config.weights_per_channel_threshold
         self.channel_axis = self.quantization_config.weights_channels_axis
-        self.np_threshold_values = np.reshape(np.asarray(self.threshold_values),[-1]) if self.channel_axis else float(self.threshold_values)
+        self.np_threshold_values = np.reshape(np.asarray(self.threshold_values),[-1]) if self.per_channel else float(self.threshold_values)
 
         if self.per_channel and self.channel_axis not in [-1, len(self.threshold_shape) - 1]:
             # Tensorflow's fake_quant_with_min_max_vars_per_channel only works on last axis, so
             # need to move the quantization axis to the last axis
             self.perm_vec = list(np.arange(len(self.threshold_shape)))
             self.perm_vec[self.channel_axis] = len(self.threshold_shape) - 1
             self.perm_vec[len(self.threshold_shape) - 1] = self.channel_axis
@@ -70,56 +74,53 @@
 
         self.num_bits = self.quantization_config.weights_n_bits
         delta = self.np_threshold_values / np.power(2.0, self.num_bits - int(C.WEIGHTS_SIGNED))
         min_int = -int(C.WEIGHTS_SIGNED) * (2 ** (self.num_bits - int(C.WEIGHTS_SIGNED)))
         max_int = (2 ** (self.num_bits - int(C.WEIGHTS_SIGNED))) - 1
         self.min = delta * min_int
         self.max = delta * max_int
-        self.quantizer_parameters = {}
+
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: qi.KerasQuantizationWrapper) -> Dict[str, tf.Variable]:
+                                layer: KerasQuantizationWrapper):
         """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            Dictionary of new variables.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
         ptq_threshold_tensor = layer.add_weight(
             name + THRESHOLD_TENSOR,
-            shape=len(self.np_threshold_values) if self.channel_axis else (),
+            shape=len(self.np_threshold_values) if self.per_channel else (),
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         ptq_threshold_tensor.assign(self.np_threshold_values)
 
         fq_min = layer.add_weight(
             name + FQ_MIN,
-            shape=len(self.min) if self.channel_axis else (),
+            shape=len(self.min) if self.per_channel else (),
             initializer=tf.keras.initializers.Constant(-1.0),
             trainable=False)
         fq_min.assign(self.min)
 
         fq_max = layer.add_weight(
             name + FQ_MAX,
-            shape=len(self.max) if self.channel_axis else (),
+            shape=len(self.max) if self.per_channel else (),
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         fq_max.assign(self.max)
 
         # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {THRESHOLD_TENSOR: ptq_threshold_tensor,
-                                     FQ_MIN: fq_min, FQ_MAX: fq_max}
-        return self.quantizer_parameters
+        self.add_quantizer_variable(THRESHOLD_TENSOR, ptq_threshold_tensor, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MIN, fq_min, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, fq_max, VariableGroup.QPARAMS)
 
     def __call__(self,
                  inputs: tf.Tensor,
                  training: bool):
         """
         Quantize a tensor.
         Args:
@@ -128,62 +129,62 @@
             weights: Dictionary of weights the quantizer can use to quantize the tensor.
             **kwargs: Additional variables the quantizer may receive.
 
         Returns:
             The quantized tensor.
         """
 
-        _min = self.quantizer_parameters[FQ_MIN]
-        _max = self.quantizer_parameters[FQ_MAX]
-        if self.channel_axis:
+        _min = self.get_quantizer_variable(FQ_MIN)
+        _max = self.get_quantizer_variable(FQ_MAX)
+        if self.per_channel:
             if self.perm_vec:
                 inputs = tf.transpose(inputs, perm=self.perm_vec)
             q_tensor = tf.quantization.fake_quant_with_min_max_vars_per_channel(inputs, _min, _max,
                                                                                 num_bits=self.num_bits)
             if self.perm_vec:
                 q_tensor = tf.transpose(q_tensor, perm=self.perm_vec)
         else:
             q_tensor = tf.quantization.fake_quant_with_min_max_vars(inputs, _min, _max,
                                                                     num_bits=self.num_bits)
 
         return q_tensor
 
-    def convert2inferable(self) -> Union[iq.WeightsPOTInferableQuantizer, iq.WeightsSymmetricInferableQuantizer]:
+    def convert2inferable(self) -> Union[WeightsPOTInferableQuantizer, WeightsSymmetricInferableQuantizer]:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseKerasInferableQuantizer object.
         """
         if self.power_of_two:
-            pot_threshold = 2 ** np.ceil(np.log2(self.quantizer_parameters[THRESHOLD_TENSOR]))
-            return iq.WeightsPOTInferableQuantizer(num_bits=self.num_bits,
-                                                   threshold=list(pot_threshold.flatten()),
-                                                   per_channel=self.per_channel,
-                                                   channel_axis=self.channel_axis,
-                                                   input_rank=len(self.threshold_shape))
+            pot_threshold = 2 ** np.ceil(np.log2(self.get_quantizer_variable(THRESHOLD_TENSOR)))
+            return WeightsPOTInferableQuantizer(num_bits=self.num_bits,
+                                                threshold=list(pot_threshold.flatten()),
+                                                per_channel=self.per_channel,
+                                                channel_axis=self.channel_axis,
+                                                input_rank=len(self.threshold_shape))
         else:
-            return iq.WeightsSymmetricInferableQuantizer(num_bits=self.num_bits,
-                                                         threshold=list(self.quantizer_parameters[THRESHOLD_TENSOR].numpy().flatten()),
-                                                         per_channel=self.per_channel,
-                                                         channel_axis=self.channel_axis,
-                                                         input_rank=len(self.threshold_shape))
+            return WeightsSymmetricInferableQuantizer(num_bits=self.num_bits,
+                                                      threshold=list(self.get_quantizer_variable(THRESHOLD_TENSOR).numpy().flatten()),
+                                                      per_channel=self.per_channel,
+                                                      channel_axis=self.channel_axis,
+                                                      input_rank=len(self.threshold_shape))
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Activation,
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
                 quantizer_type=TrainingMethod.STE)
-class STEActivationQuantizer(BaseKerasQATTrainableQuantizer):
+class STEActivationQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer outputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
-        Initialize a STEActivationQuantizer object with parameters to use
+        Initialize a STEActivationQATQuantizer object with parameters to use
         for the quantization.
 
         Args:
             quantization_config: trainable quantizer config class
         """
         super().__init__(quantization_config)
         self.power_of_two = quantization_config.activation_quantization_method == QuantizationMethod.POWER_OF_TWO
@@ -196,30 +197,26 @@
                                                 np.ceil(np.log2(np.maximum(self.np_threshold_values, C.MIN_THRESHOLD))))
         self.num_bits = quantization_config.activation_n_bits
         delta = self.np_threshold_values / np.power(2.0, self.num_bits - int(self.signed))
         min_int = -int(self.signed) * (2 ** (self.num_bits - int(self.signed)))
         max_int = (2 ** (self.num_bits - int(self.signed))) - 1
         self.min = delta * min_int
         self.max = delta * max_int
-        self.quantizer_parameters = {}
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: qi.KerasQuantizationWrapper) -> Dict[str, tf.Variable]:
+                                layer: KerasQuantizationWrapper):
         """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            Dictionary of new variables.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
         ptq_threshold_tensor = layer.add_weight(
             name + THRESHOLD_TENSOR,
             shape=(),
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         ptq_threshold_tensor.assign(self.np_threshold_values)
@@ -235,52 +232,53 @@
             name + FQ_MAX,
             shape=(),
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         fq_max.assign(self.max)
 
         # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {THRESHOLD_TENSOR: ptq_threshold_tensor,
-                                     FQ_MIN: fq_min, FQ_MAX: fq_max}
-        return self.quantizer_parameters
+        self.add_quantizer_variable(THRESHOLD_TENSOR, ptq_threshold_tensor, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MIN, fq_min, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, fq_max, VariableGroup.QPARAMS)
+
 
     def __call__(self,
                  inputs: tf.Tensor,
                  training: bool):
         """
         Quantize a tensor.
         Args:
             inputs: Input tensor to quantize.
             training: Whether the graph is in training mode.
 
         Returns:
             The quantized tensor.
         """
 
-        _min = self.quantizer_parameters[FQ_MIN]
-        _max = self.quantizer_parameters[FQ_MAX]
+        _min = self.get_quantizer_variable(FQ_MIN)
+        _max = self.get_quantizer_variable(FQ_MAX)
         q_tensor = tf.quantization.fake_quant_with_min_max_vars(inputs, _min, _max,
                                                                 num_bits=self.num_bits)
 
         return q_tensor
 
-    def convert2inferable(self) -> Union[iq.ActivationPOTInferableQuantizer, iq.ActivationSymmetricInferableQuantizer]:
+    def convert2inferable(self) -> Union[ActivationPOTInferableQuantizer, ActivationSymmetricInferableQuantizer]:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseKerasInferableQuantizer object.
         """
 
         if self.power_of_two:
-            pot_threshold = 2 ** np.ceil(np.log2(self.quantizer_parameters[THRESHOLD_TENSOR]))
-            return iq.ActivationPOTInferableQuantizer(num_bits=self.num_bits,
+            pot_threshold = 2 ** np.ceil(np.log2(self.get_quantizer_variable(THRESHOLD_TENSOR)))
+            return ActivationPOTInferableQuantizer(num_bits=self.num_bits,
                                                       # In activation quantization is per-tensor only - thus we pass
                                                       # the threshold as a list with a len of 1
                                                       threshold=[pot_threshold],
                                                       signed=self.signed)
         else:
-            return iq.ActivationSymmetricInferableQuantizer(num_bits=self.num_bits,
-                                                            # In activation quantization is per-tensor only - thus we
-                                                            # pass the threshold as a list with a len of 1
-                                                            threshold=[self.quantizer_parameters[THRESHOLD_TENSOR].numpy()],
-                                                            signed=self.signed)
+            return ActivationSymmetricInferableQuantizer(num_bits=self.num_bits,
+                                                         # In activation quantization is per-tensor only - thus we
+                                                         # pass the threshold as a list with a len of 1
+                                                         threshold=[self.get_quantizer_variable(THRESHOLD_TENSOR).numpy()],
+                                                         signed=self.signed)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/keras/quantizer/ste_rounding/uniform_ste.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,54 +8,55 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-from typing import Dict
-
 import numpy as np
 import tensorflow as tf
 from tensorflow.python.framework.tensor_shape import TensorShape
-from model_compression_toolkit.core.common.constants import RANGE_MIN, RANGE_MAX
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.qat.common.constants import FQ_MIN, FQ_MAX
+from model_compression_toolkit.constants import RANGE_MIN, RANGE_MAX
+from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
+from model_compression_toolkit.qat import TrainingMethod
+
+from mct_quantizers import mark_quantizer, QuantizationMethod, QuantizationTarget, KerasQuantizationWrapper
+from mct_quantizers.keras.quantizers import \
+    BaseKerasInferableQuantizer, WeightsUniformInferableQuantizer, ActivationUniformInferableQuantizer
+
 from model_compression_toolkit.qat.keras.quantizer.quant_utils import adjust_range_to_include_zero
 from model_compression_toolkit.core.common.quantization.quantizers.quantizers_helpers import fix_range_to_include_zero
-from model_compression_toolkit import quantizers_infrastructure as qi, TrainingMethod
-from model_compression_toolkit.core.common import constants as C
-import model_compression_toolkit.quantizers_infrastructure.keras.inferable_quantizers as iq
+from model_compression_toolkit import constants as C
+
 from model_compression_toolkit.qat.keras.quantizer.base_keras_qat_quantizer import BaseKerasQATTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure import TrainableQuantizerWeightsConfig, \
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
     TrainableQuantizerActivationConfig
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import mark_quantizer
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Weights,
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.UNIFORM],
                 quantizer_type=TrainingMethod.STE)
-class STEUniformWeightQuantizer(BaseKerasQATTrainableQuantizer):
+class STEUniformWeightQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer inputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
         Initialize a TrainableWeightQuantizer object with parameters to use
         for the quantization.
 
         Args:
             quantization_config: a trainable quantizer config class with attributes for the quantization.
 
         """
         super().__init__(quantization_config)
-        self.max_values = quantization_config.weights_quantization_params[RANGE_MAX]
-        self.min_values = quantization_config.weights_quantization_params[RANGE_MIN]
+        self.max_values = np.array(quantization_config.weights_quantization_params[RANGE_MAX])
+        self.min_values = np.array(quantization_config.weights_quantization_params[RANGE_MIN])
         self.num_bits = self.quantization_config.weights_n_bits
         self.per_channel = self.quantization_config.weights_per_channel_threshold
         self.channel_axis = self.quantization_config.weights_channels_axis
         self.min_max_shape = np.asarray(self.max_values).shape
         self.max = np.reshape(self.max_values, [-1]) if self.per_channel else float(self.max_values)
         self.min = np.reshape(self.min_values, [-1]) if self.per_channel else float(self.min_values)
 
@@ -64,30 +65,25 @@
             # need to move the quantization axis to the last axis
             self.perm_vec = list(np.arange(len(self.min_max_shape)))
             self.perm_vec[self.channel_axis] = len(self.min_max_shape) - 1
             self.perm_vec[len(self.min_max_shape) - 1] = self.channel_axis
         else:
             self.perm_vec = None
 
-        self.quantizer_parameters = {}
-
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: qi.KerasQuantizationWrapper) -> Dict[str, tf.Variable]:
+                                layer: KerasQuantizationWrapper):
         """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            Dictionary of new variables.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
         fq_min = layer.add_weight(
             name + FQ_MIN,
             shape=len(self.min) if self.per_channel else (),
             initializer=tf.keras.initializers.Constant(-1.0),
             trainable=False)
         fq_min.assign(self.min)
@@ -96,31 +92,31 @@
             name + FQ_MAX,
             shape=len(self.max) if self.per_channel else (),
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         fq_max.assign(self.max)
 
         # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {FQ_MIN: fq_min, FQ_MAX: fq_max}
-        return self.quantizer_parameters
+        self.add_quantizer_variable(FQ_MIN, fq_min, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, fq_max, VariableGroup.QPARAMS)
 
     def __call__(self, inputs: tf.Tensor,
                  training: bool):
         """
         Quantize a tensor.
         Args:
             inputs: Input tensor to quantize.
             training: Whether the graph is in training mode.
 
         Returns:
             The quantized tensor.
         """
 
-        _min = self.quantizer_parameters[FQ_MIN]
-        _max = self.quantizer_parameters[FQ_MAX]
+        _min = self.get_quantizer_variable(FQ_MIN)
+        _max = self.get_quantizer_variable(FQ_MAX)
         _min, _max = adjust_range_to_include_zero(_min, _max, self.num_bits)
 
         if self.per_channel:
             if self.perm_vec:
                 inputs = tf.transpose(inputs, perm=self.perm_vec)
 
             q_tensor = tf.quantization.fake_quant_with_min_max_vars_per_channel(inputs, _min, _max,
@@ -129,69 +125,65 @@
                 q_tensor = tf.transpose(q_tensor, perm=self.perm_vec)
         else:
             q_tensor = tf.quantization.fake_quant_with_min_max_vars(inputs, _min, _max,
                                                                     num_bits=self.num_bits)
 
         return q_tensor
 
-    def convert2inferable(self) -> qi.BaseKerasInferableQuantizer:
+    def convert2inferable(self) -> BaseKerasInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseKerasInferableQuantizer object.
         """
-        min_range, max_range = fix_range_to_include_zero(self.quantizer_parameters[FQ_MIN].numpy(),
-                                                         self.quantizer_parameters[FQ_MAX].numpy(),
+        min_range, max_range = fix_range_to_include_zero(self.get_quantizer_variable(FQ_MIN).numpy(),
+                                                         self.get_quantizer_variable(FQ_MAX).numpy(),
                                                          self.num_bits)
-        return iq.WeightsUniformInferableQuantizer(num_bits=self.num_bits,
-                                                   min_range=list(min_range.flatten()),
-                                                   max_range=list(max_range.flatten()),
-                                                   per_channel=self.per_channel,
-                                                   channel_axis=self.channel_axis,
-                                                   input_rank=len(self.min_max_shape))
+        return WeightsUniformInferableQuantizer(num_bits=self.num_bits,
+                                                min_range=list(min_range.flatten()),
+                                                max_range=list(max_range.flatten()),
+                                                per_channel=self.per_channel,
+                                                channel_axis=self.channel_axis,
+                                                input_rank=len(self.min_max_shape))
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Activation,
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.UNIFORM],
                 quantizer_type=TrainingMethod.STE)
-class STEUniformActivationQuantizer(BaseKerasQATTrainableQuantizer):
+class STEUniformActivationQATQuantizer(BaseKerasQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer outputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
-        Initialize a STEUniformActivationQuantizer object with parameters to use
+        Initialize a STEUniformActivationQATQuantizer object with parameters to use
         for the quantization.
 
         Args:
             quantization_config: trainable quantizer config class
         """
         super().__init__(quantization_config)
 
         self.num_bits = quantization_config.activation_n_bits
         self.min_range = quantization_config.activation_quantization_params[C.RANGE_MIN]
         self.max_range = quantization_config.activation_quantization_params[C.RANGE_MAX]
-        self.quantizer_parameters = {}
 
     def initialize_quantization(self,
                                 tensor_shape: TensorShape,
                                 name: str,
-                                layer: qi.KerasQuantizationWrapper) -> Dict[str, tf.Variable]:
+                                layer: KerasQuantizationWrapper):
         """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            Dictionary of new variables.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
         fq_min = layer.add_weight(
             name + FQ_MIN,
             shape=(),
             initializer=tf.keras.initializers.Constant(-1.0),
             trainable=False)
         fq_min.assign(self.min_range)
@@ -200,46 +192,46 @@
             name + FQ_MAX,
             shape=(),
             initializer=tf.keras.initializers.Constant(1.0),
             trainable=False)
         fq_max.assign(self.max_range)
 
         # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {FQ_MIN: fq_min, FQ_MAX: fq_max}
-        return self.quantizer_parameters
+        self.add_quantizer_variable(FQ_MIN, fq_min, VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, fq_max, VariableGroup.QPARAMS)
 
     def __call__(self,
                  inputs: tf.Tensor,
                  training: bool):
         """
         Quantize a tensor.
         Args:
             inputs: Input tensor to quantize.
             training: Whether the graph is in training mode.
 
         Returns:
             The quantized tensor.
         """
 
-        _min = self.quantizer_parameters[FQ_MIN]
-        _max = self.quantizer_parameters[FQ_MAX]
+        _min = self.get_quantizer_variable(FQ_MIN)
+        _max = self.get_quantizer_variable(FQ_MAX)
         _min, _max = adjust_range_to_include_zero(_min, _max, self.num_bits)
         q_tensor = tf.quantization.fake_quant_with_min_max_vars(inputs, _min, _max,
                                                                 num_bits=self.num_bits)
 
         return q_tensor
 
-    def convert2inferable(self) -> qi.BaseKerasInferableQuantizer:
+    def convert2inferable(self) -> BaseKerasInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseKerasInferableQuantizer object.
         """
-        min_range, max_range = fix_range_to_include_zero(self.quantizer_parameters[FQ_MIN].numpy(),
-                                                         self.quantizer_parameters[FQ_MAX].numpy(),
+        min_range, max_range = fix_range_to_include_zero(self.get_quantizer_variable(FQ_MIN).numpy(),
+                                                         self.get_quantizer_variable(FQ_MAX).numpy(),
                                                          self.num_bits)
-        return iq.ActivationUniformInferableQuantizer(num_bits=self.num_bits,
-                                                      # In activation quantization is per-tensor only - thus we pass
-                                                      # the min/max as lists with a len of 1
-                                                      min_range=[min_range],
-                                                      max_range=[max_range])
+        return ActivationUniformInferableQuantizer(num_bits=self.num_bits,
+                                                   # In activation quantization is per-tensor only - thus we pass
+                                                   # the min/max as lists with a len of 1
+                                                   min_range=[min_range],
+                                                   max_range=[max_range])
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantization_facade.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantization_facade.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,59 +12,63 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 import copy
 from typing import Callable
 from functools import partial
 
-from model_compression_toolkit.core.common.constants import FOUND_TORCH, PYTORCH
+from model_compression_toolkit.constants import FOUND_TORCH, PYTORCH
 
-from model_compression_toolkit import CoreConfig
+from model_compression_toolkit.core import CoreConfig
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 from model_compression_toolkit.core.common.framework_info import FrameworkInfo
 from model_compression_toolkit.core.common.mixed_precision.kpi_tools.kpi import KPI
 from model_compression_toolkit.core.common.mixed_precision.mixed_precision_quantization_config import \
     MixedPrecisionQuantizationConfigV2
-from model_compression_toolkit.core.common.target_platform.targetplatform2framework import TargetPlatformCapabilities
+from model_compression_toolkit.target_platform_capabilities.target_platform.targetplatform2framework import \
+    TargetPlatformCapabilities
 from model_compression_toolkit.core.runner import core_runner, _init_tensorboard_writer
 from model_compression_toolkit.ptq.runner import ptq_runner
 
-
 if FOUND_TORCH:
     import torch.nn as nn
     from torch.nn import Module
+    from mct_quantizers import PytorchActivationQuantizationHolder
     from model_compression_toolkit.core.pytorch.default_framework_info import DEFAULT_PYTORCH_INFO
-    from model_compression_toolkit.core.pytorch.constants import DEFAULT_TP_MODEL
+    from model_compression_toolkit.target_platform_capabilities.constants import DEFAULT_TP_MODEL
     from model_compression_toolkit.core.pytorch.pytorch_implementation import PytorchImplementation
-    from model_compression_toolkit.qat.common.qat_config import _is_qat_applicable
+    from model_compression_toolkit.qat.common.qat_config import is_qat_applicable
     from model_compression_toolkit.core.pytorch.back2framework.pytorch_model_builder import PyTorchModelBuilder
-    from model_compression_toolkit.quantizers_infrastructure import PytorchQuantizationWrapper
-    from model_compression_toolkit import quantizers_infrastructure as qi
+    from mct_quantizers import PytorchQuantizationWrapper
     from model_compression_toolkit import get_target_platform_capabilities
     from model_compression_toolkit.qat.common.qat_config import QATConfig
+    from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import get_activation_quantizer_holder
     from model_compression_toolkit.qat.pytorch.quantizer.quantization_builder import quantization_builder
+
     DEFAULT_PYTORCH_TPC = get_target_platform_capabilities(PYTORCH, DEFAULT_TP_MODEL)
 
 
-    def qat_wrapper(n: common.BaseNode, module: nn.Module, qat_config: QATConfig):
+    def qat_wrapper(n: common.BaseNode,
+                    module: nn.Module,
+                    qat_config: QATConfig):
         """
         A function which takes a computational graph node and a pytorch module and perform the quantization wrapping
         Args:
             n: A node of mct graph.
             module: A Pytorch module
             qat_config (QATConfig): QAT configuration
         Returns: Wrapped layer
 
         """
-        if _is_qat_applicable(n, DEFAULT_PYTORCH_INFO):
-            weights_quantizers, activation_quantizers = quantization_builder(n, qat_config, DEFAULT_PYTORCH_INFO)
-            return qi.PytorchQuantizationWrapper(module, weights_quantizers, activation_quantizers)
-        else:
-            return module
+        if is_qat_applicable(n, DEFAULT_PYTORCH_INFO):
+            weights_quantizers, _ = quantization_builder(n, qat_config, DEFAULT_PYTORCH_INFO)
+            if len(weights_quantizers) > 0:
+                return PytorchQuantizationWrapper(module, weights_quantizers)
+        return module
 
 
     def pytorch_quantization_aware_training_init(in_model: Module,
                                                  representative_data_gen: Callable,
                                                  target_kpi: KPI = None,
                                                  core_config: CoreConfig = CoreConfig(),
                                                  qat_config: QATConfig = QATConfig(),
@@ -117,33 +121,33 @@
             >>> num_calibration_batches = 10
             >>> def repr_datagen():
             >>>     for _ in range(num_calibration_batches):
             >>>         yield [np.random.random((4, 3, 224, 224))]
 
              Create a MCT core config, containing the quantization configuration:
 
-             >>> config = mct.CoreConfig()
+             >>> config = mct.core.CoreConfig()
 
              Pass the model, the representative dataset generator, the configuration and the target KPI to get a
              quantized model. Now the model contains quantizer wrappers for fine tunning the weights:
 
              >>> quantized_model, quantization_info = pytorch_quantization_aware_training_init(model, repr_datagen, core_config=config)
 
              For more configuration options, please take a look at our `API documentation <https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html>`_.
 
          """
 
         if core_config.mixed_precision_enable:
             if not isinstance(core_config.mixed_precision_config, MixedPrecisionQuantizationConfigV2):
-                common.Logger.error("Given quantization config to mixed-precision facade is not of type "
-                                    "MixedPrecisionQuantizationConfigV2. Please use pytorch_post_training_quantization API,"
-                                    "or pass a valid mixed precision configuration.")
+                Logger.error("Given quantization config to mixed-precision facade is not of type "
+                             "MixedPrecisionQuantizationConfigV2. Please use pytorch_post_training_quantization API,"
+                             "or pass a valid mixed precision configuration.")
 
-            common.Logger.info("Using experimental mixed-precision quantization. "
-                               "If you encounter an issue please file a bug.")
+            Logger.info("Using experimental mixed-precision quantization. "
+                        "If you encounter an issue please file a bug.")
 
         tb_w = _init_tensorboard_writer(fw_info)
 
         fw_impl = PytorchImplementation()
 
         tg, bit_widths_config = core_runner(in_model=in_model,
                                             representative_data_gen=representative_data_gen,
@@ -154,20 +158,29 @@
                                             target_kpi=target_kpi,
                                             tb_w=tb_w)
 
         tg = ptq_runner(tg, representative_data_gen, core_config, fw_info, fw_impl, tb_w)
 
         _qat_wrapper = partial(qat_wrapper, qat_config=qat_config)
 
-        qat_model, user_info = PyTorchModelBuilder(graph=tg, fw_info=fw_info, wrapper=_qat_wrapper).build_model()
+        qat_model, user_info = PyTorchModelBuilder(graph=tg,
+                                                   fw_info=fw_info,
+                                                   wrapper=_qat_wrapper,
+                                                   get_activation_quantizer_holder_fn=partial(
+                                                       get_activation_quantizer_holder,
+                                                       qat_config=qat_config)).build_model()
 
         user_info.mixed_precision_cfg = bit_widths_config
 
+        # Remove fw_info from graph to enable saving the pytorch model (fw_info can not be pickled)
+        delattr(qat_model.graph, 'fw_info')
+
         return qat_model, user_info
 
+
     def pytorch_quantization_aware_training_finalize(in_model: Module):
         """
          Convert a model fine-tuned by the user to a network with QuantizeWrappers containing
          InferableQuantizers, that quantizes both the layers weights and outputs
 
          Args:
              in_model (Model): Pytorch model to remove QuantizeWrappers.
@@ -189,39 +202,39 @@
              Create a random dataset generator:
 
              >>> import numpy as np
              >>> def repr_datagen(): yield [np.random.random((1, 224, 224, 3))]
 
              Create a MCT core config, containing the quantization configuration:
 
-             >>> config = mct.CoreConfig()
+             >>> config = mct.core.CoreConfig()
 
              Pass the model, the representative dataset generator, the configuration and the target KPI to get a
              quantized model:
 
              >>> quantized_model, quantization_info = pytorch_quantization_aware_training_init(model, repr_datagen, core_config=config)
 
              Use the quantized model for fine-tuning. Finally, remove the quantizer wrappers and keep a quantize model ready for inference.
 
              >>> quantized_model = mct.pytorch_quantization_aware_training_finalize(quantized_model)
 
          """
-        exported_model = copy.deepcopy(in_model)
-        for _, layer in exported_model.named_children():
-            if isinstance(layer, PytorchQuantizationWrapper):
+        for _, layer in in_model.named_children():
+            if isinstance(layer, (PytorchQuantizationWrapper, PytorchActivationQuantizationHolder)):
                 layer.convert_to_inferable_quantizers()
 
-        return exported_model
+        return in_model
 
 
 else:
     # If torch is not installed,
     # we raise an exception when trying to use these functions.
     def pytorch_quantization_aware_training_init(*args, **kwargs):
         Logger.critical('Installing Pytorch is mandatory '
                         'when using pytorch_quantization_aware_training_init. '
                         'Could not find the torch package.')  # pragma: no cover
 
+
     def pytorch_quantization_aware_training_finalize(*args, **kwargs):
         Logger.critical('Installing Pytorch is mandatory '
                         'when using pytorch_quantization_aware_training_finalize. '
                         'Could not find the torch package.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/__init__.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/ptq/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -9,9 +9,9 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.symmetric_ste
-import model_compression_toolkit.qat.pytorch.quantizer.ste_rounding.uniform_ste
+from model_compression_toolkit.ptq.pytorch.quantization_facade import pytorch_post_training_quantization_experimental
+from model_compression_toolkit.ptq.keras.quantization_facade import keras_post_training_quantization_experimental
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,48 +1,65 @@
-# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Union
+from typing import Union, List
 
-from model_compression_toolkit.core.common.logger import Logger
-from model_compression_toolkit.core.common.constants import FOUND_TORCH
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TORCH
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer, VAR, GROUP
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
+    TrainableQuantizerActivationConfig
 
-from model_compression_toolkit.quantizers_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure import TrainableQuantizerWeightsConfig, \
-    TrainableQuantizerActivationConfig, BasePytorchTrainableQuantizer
 
 if FOUND_TORCH:
 
-    class BasePytorchQATTrainableQuantizer(BasePytorchTrainableQuantizer):
-        """
-        A base class for trainable Keras quantizer for QAT.
-        """
+    import torch
 
+    class BasePytorchTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             """
-            Initializes BasePytorchQATTrainableQuantizer object.
+            This class is a base Pytorch quantizer which validates the provided quantization config and defines an
+            abstract function which any quantizer needs to implement.
 
             Args:
-                quantization_config: quantizer config class contains all the information about a quantizer configuration.
+                quantization_config: quantizer config class contains all the information about the quantizer configuration.
             """
             super().__init__(quantization_config)
 
+        def get_trainable_variables(self, group: VariableGroup) -> List[torch.Tensor]:
+            """
+            Get trainable parameters with specific group from quantizer
+
+            Args:
+                group: Enum of variable group
+
+            Returns:
+                List of trainable variables
+            """
+            quantizer_trainable = []
+            for name, parameter_dict in self.quantizer_parameters.items():
+                quantizer_parameter, parameter_group = parameter_dict[VAR], parameter_dict[GROUP]
+                if quantizer_parameter.requires_grad and parameter_group == group:
+                    quantizer_trainable.append(quantizer_parameter)
+            return quantizer_trainable
+
 else:
-    class BasePytorchTrainableQuantizer(BasePytorchTrainableQuantizer):
+    class BasePytorchTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             super().__init__(quantization_config)
             Logger.critical('Installing Pytorch is mandatory '
-                            'when using BasePytorchQATTrainableQuantizer. '
+                            'when using BasePytorchTrainableQuantizer. '
                             'Could not find torch package.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,65 +10,64 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from typing import List, Dict, Tuple
 
+from model_compression_toolkit.gptq import GradientPTQConfigV2
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common.framework_info import FrameworkInfo
-from model_compression_toolkit.qat.common.qat_config import QATConfig
-from model_compression_toolkit.qat.common.qat_get_quantizer import get_quantizer_class
-from model_compression_toolkit.qat.common.qat_get_quantizer_config import \
-    get_trainable_quantizer_quantization_candidates, get_trainable_quantizer_weights_config, \
-    get_trainable_quantizer_activation_config
+from model_compression_toolkit.core.pytorch.constants import KERNEL
+from model_compression_toolkit.exporter.model_wrapper.pytorch.builder.node_to_quantizer import \
+    get_activation_inferable_quantizer_kwargs
+from model_compression_toolkit.gptq.pytorch.quantizer.base_pytorch_gptq_quantizer import \
+    BasePytorchGPTQTrainableQuantizer
+from mct_quantizers import QuantizationTarget
+from mct_quantizers.common.get_quantizers import get_inferable_quantizer_class
+from mct_quantizers.pytorch.quantizers import BasePyTorchInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.get_quantizer_config import \
+    get_trainable_quantizer_weights_config
 from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure import QuantizationTarget
+from model_compression_toolkit.trainable_infrastructure.common.get_quantizers import \
+    get_trainable_quantizer_class
 
 
 def quantization_builder(n: common.BaseNode,
-                         qat_config: QATConfig,
-                         fw_info: FrameworkInfo,
+                         gptq_config: GradientPTQConfigV2,
                          ) -> Tuple[Dict[str, BasePytorchQATTrainableQuantizer],
-                                    List[BasePytorchQATTrainableQuantizer]]:
+                                    List[BasePyTorchInferableQuantizer]]:
     """
     Build quantizers for a node according to its quantization configuration and
     a global NoOpQuantizeConfig object.
 
     Args:
         n: Node to build its QuantizeConfig.
-        qat_config (QATConfig): QAT configuration
-        fw_info: Framework information (e.g., mapping from layers to their attributes to quantize).
+        gptq_config (GradientPTQConfigV2): GradientPTQConfigV2 configuration.
 
     Returns:
-        weights_quantizers: A dictionary between a weight's name to its quantizer.
-        activation_quantizers: A list of activations quantization, one for each layer output.).
+        A dictionary which maps the weights kernel attribute to a quantizer for GPTQ training.
+        Note that we return a dictionary although there is only a single attribute that is being mapped to a quantizer,
+        to be compatible with the quantization infrastructure template.
     """
-    if len(n.candidates_quantization_cfg) > 1:
-        wq_cand, aq_cand = get_trainable_quantizer_quantization_candidates(n)
-    else:
-        wq_cand, aq_cand = None, None
 
-    weight_quantizers = {}
+    weights_quantizers = {}
     if n.is_weights_quantization_enabled():
         quant_method = n.final_weights_quantization_cfg.weights_quantization_method
-        quantizer_class = get_quantizer_class(QuantizationTarget.Weights,
-                                              qat_config.activation_training_method,
-                                              quant_method,
-                                              BasePytorchQATTrainableQuantizer)
-        attributes = fw_info.get_kernel_op_attributes(n.type)
-        for attr in attributes:
-            weight_quantizers.update({attr: quantizer_class(get_trainable_quantizer_weights_config(n, wq_cand),
-                                                           **qat_config.weight_quantizer_params_override)})
-
+        quantizer_class = get_trainable_quantizer_class(quant_target=QuantizationTarget.Weights,
+                                                        quantizer_type=gptq_config.rounding_type,
+                                                        quant_method=quant_method,
+                                                        quantizer_base_class=BasePytorchGPTQTrainableQuantizer)
+        weights_quantizers.update({KERNEL: quantizer_class(get_trainable_quantizer_weights_config(n),
+                                                           **gptq_config.gptq_quantizer_params_override)})
     activation_quantizers = []
     if n.is_activation_quantization_enabled():
         quant_method = n.final_activation_quantization_cfg.activation_quantization_method
-        quantizer_class = get_quantizer_class(QuantizationTarget.Activation,
-                                              qat_config.activation_training_method,
-                                              quant_method,
-                                              BasePytorchQATTrainableQuantizer)
 
-        activation_quantizers = [quantizer_class(get_trainable_quantizer_activation_config(n, aq_cand),
-                                                 **qat_config.activation_quantizer_params_override)]
+        quantizer_class = get_inferable_quantizer_class(quant_target=QuantizationTarget.Activation,
+                                                        quant_method=quant_method,
+                                                        quantizer_base_class=BasePyTorchInferableQuantizer)
+
+        kwargs = get_activation_inferable_quantizer_kwargs(n)
+
+        activation_quantizers.append(quantizer_class(**kwargs))
 
-    return weight_quantizers, activation_quantizers
+    return weights_quantizers, activation_quantizers
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,16 +8,56 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Tuple
+from typing import Union, Tuple
 import torch
+from model_compression_toolkit.constants import MIN_THRESHOLD
+
+
+def power_of_two_max(max_tensor: torch.Tensor) -> torch.Tensor:
+    """
+    Compute the power of two threshold for a tensor.
+    """
+    return torch.pow(2, ste_ceil(torch.log2(torch.clip(max_tensor, min=MIN_THRESHOLD, max=torch.inf))))
+
+
+def calculate_delta(max_tensor: torch.Tensor,
+                    num_bits: int,
+                    signed: bool) -> torch.Tensor:
+    """
+    Compute the step size for the symmetric quantization.
+    """
+    return max_tensor / (2 ** (num_bits - int(signed)))
+
+
+def calculate_delta_uniform(min_tensor: torch.Tensor,
+                            max_tensor: torch.Tensor,
+                            num_bits: int) -> torch.Tensor:
+    """
+    Compute the step size for the uniform quantization.
+    """
+    return (max_tensor-min_tensor) / (2 ** num_bits - 1)
+
+
+def ste_ceil(x: torch.Tensor) -> torch.Tensor:
+    """
+    Return the ceil values of a tensor.
+    """
+    return (torch.ceil(x) - x).detach() + x
+
+
+def ste_floor(x: torch.Tensor) -> torch.Tensor:
+    """
+    Return the floor values of a tensor.
+    """
+    return (torch.floor(x) - x).detach() + x
 
 
 def ste_round(x: torch.Tensor) -> torch.Tensor:
     """
     Calculate the rounded values of a tensor
     Args:
         x: input variable
@@ -63,74 +103,7 @@
     scale = (range_max - range_min) / (2 ** n_bits - 1)
     min_range_adj = scale * torch.round(range_min / scale)
     max_range_adj = range_max - range_min + min_range_adj
 
     min_range_adj = min_range_adj * mid_range + max_negative * range_min
     max_range_adj = max_range_adj * mid_range + min_positive * range_max
     return min_range_adj, max_range_adj
-
-
-def symmetric_quantizer(tensor_data: torch.Tensor,
-                        threshold: torch.Tensor,
-                        n_bits: int,
-                        sign: bool = False) -> torch.Tensor:
-    """
-    Quantize a tensor according to the number of bits and threshold.
-    Symmetric quantization.
-    Args:
-        tensor_data: Tensor values to quantize.
-        threshold: threshold for quantization.
-        n_bits: Number of bits to quantize the tensor.
-        sign: sign of tensor_data
-    Returns:
-        Quantized data.
-    """
-
-    # Compute the step size of quantized values.
-    n_pos = 2 ** (n_bits - int(sign))
-    delta_tensor = threshold / n_pos
-
-    # Compute min/max int value
-    min_val = -int(sign) * n_pos
-    max_val = n_pos - 1
-
-    # Apply rounding
-    input_tensor_int = ste_round(tensor_data / delta_tensor)
-
-    # Clip data in range
-    clipped_tensor = ste_clip(input_tensor_int, min_val=min_val, max_val=max_val)
-
-    # Quantize the data between -threshold/threshold
-    q = delta_tensor * clipped_tensor
-    return q
-
-
-def uniform_quantizer(tensor_data: torch.Tensor,
-                      range_min: torch.Tensor,
-                      range_max: torch.Tensor,
-                      n_bits: int) -> torch.Tensor:
-    """
-    Quantize a tensor according to given range (min, max) and number of bits.
-    Uniform quantization.
-    Args:
-        tensor_data: Tensor values to quantize.
-        range_min: minimum bound of the range for quantization (or array of min values per channel).
-        range_max: maximum bound of the range for quantization (or array of max values per channel).
-        n_bits: Number of bits to quantize the tensor.
-    Returns:
-        Quantized data.
-    """
-    # adjusts the quantization range so the quantization grid includes zero.
-    a, b = fix_range_to_include_zero(range_min, range_max, n_bits)
-
-    # Compute the step size of quantized values.
-    delta_tensor = (b - a) / (2 ** n_bits - 1)
-
-    # Apply rounding
-    input_tensor_int = ste_round((tensor_data - a) / delta_tensor)
-
-    # Clip data in range
-    clipped_tensor = ste_clip(input_tensor_int, min_val=0, max_val=2 ** n_bits - 1)
-
-    # Quantize the data between min/max of quantization range.
-    q = delta_tensor * clipped_tensor + a
-    return q
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py`

 * *Files 8% similar despite different names*

```diff
@@ -8,37 +8,42 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Dict, Union
+from typing import Union
 
 import numpy as np
 import torch
 import torch.nn as nn
 
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.qat import TrainingMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import PytorchQuantizationWrapper
 from model_compression_toolkit.qat.common import THRESHOLD_TENSOR
-from model_compression_toolkit import quantizers_infrastructure as qi, TrainingMethod
+from model_compression_toolkit import constants as C
 from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import mark_quantizer
-from model_compression_toolkit.quantizers_infrastructure.pytorch import inferable_quantizers as iq
-from model_compression_toolkit.core.common import constants as C
+from mct_quantizers.common.base_inferable_quantizer import mark_quantizer, QuantizationTarget
+
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from model_compression_toolkit.qat.pytorch.quantizer.quantizer_utils import ste_round, ste_clip, symmetric_quantizer
-from model_compression_toolkit.quantizers_infrastructure.common.trainable_quantizer_config import \
+from mct_quantizers.pytorch.quantizers import \
+    WeightsPOTInferableQuantizer, WeightsSymmetricInferableQuantizer, ActivationPOTInferableQuantizer, \
+    ActivationSymmetricInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Weights,
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
                 quantizer_type=TrainingMethod.STE)
-class STEWeightQuantizer(BasePytorchQATTrainableQuantizer):
+class STEWeightQATQuantizer(BasePytorchQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer weights.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
         Initialize a TrainableWeightQuantizer object with parameters to use
@@ -60,40 +65,36 @@
         n_pos_bits = self.num_bits - int(C.WEIGHTS_SIGNED)
         delta = self.np_threshold_values / np.power(2.0, n_pos_bits)
         self.delta_tensor = to_torch_tensor(delta)
         self.min_int = -int(C.WEIGHTS_SIGNED) * (2 ** n_pos_bits)
         self.max_int = (2 ** n_pos_bits) - 1
         self.min = delta * self.min_int
         self.max = delta * self.max_int
-        self.quantizer_parameters = {}
+
 
     def initialize_quantization(self,
                                 tensor_shape: torch.Size,
                                 name: str,
-                                layer: qi.PytorchQuantizationWrapper) -> Dict[str, nn.Parameter]:
+                                layer: PytorchQuantizationWrapper):
         """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            Dictionary of new variables.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
 
         # Add threshold variables to layer.
         layer.register_parameter(name + "_" + THRESHOLD_TENSOR, nn.Parameter(to_torch_tensor(self.np_threshold_values),
                                                                              requires_grad=False))
 
         # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {THRESHOLD_TENSOR: layer.get_parameter(name + "_" + THRESHOLD_TENSOR)}
+        self.add_quantizer_variable(THRESHOLD_TENSOR, layer.get_parameter(name + "_" + THRESHOLD_TENSOR), VariableGroup.QPARAMS)
 
-        return self.quantizer_parameters
 
     def __call__(self,
                  inputs: nn.Parameter,
                  training: bool) -> nn.Parameter:
         """
         Quantize a tensor
         Args:
@@ -103,100 +104,103 @@
             quantized tensor
         """
         w0 = ste_round(inputs / self.delta_tensor)
         w1 = ste_clip(w0, min_val=self.min_int, max_val=self.max_int)
         w_q = self.delta_tensor * w1
         return w_q
 
-    def convert2inferable(self) -> Union[iq.WeightsPOTInferableQuantizer, iq.WeightsSymmetricInferableQuantizer]:
+    def convert2inferable(self) -> Union[WeightsPOTInferableQuantizer, WeightsSymmetricInferableQuantizer]:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             A pytorch inferable quanizer object.
         """
-        np_threshold = self.quantizer_parameters[THRESHOLD_TENSOR].cpu().detach().numpy().flatten()
+        np_threshold = self.get_quantizer_variable(THRESHOLD_TENSOR).cpu().detach().numpy().flatten()
         if self.power_of_two:
             pot_threshold = 2 ** np.ceil(np.log2(np_threshold))
-            return iq.WeightsPOTInferableQuantizer(num_bits=self.num_bits,
-                                                   threshold=pot_threshold,
-                                                   per_channel=self.quantization_config.weights_per_channel_threshold,
-                                                   channel_axis=self.quantization_config.weights_channels_axis)
+            return WeightsPOTInferableQuantizer(num_bits=self.num_bits,
+                                                threshold=pot_threshold,
+                                                per_channel=self.quantization_config.weights_per_channel_threshold,
+                                                channel_axis=self.quantization_config.weights_channels_axis)
         else:
-            return iq.WeightsSymmetricInferableQuantizer(num_bits=self.num_bits,
-                                                         threshold=np_threshold,
-                                                         per_channel=self.quantization_config.weights_per_channel_threshold,
-                                                         channel_axis=self.quantization_config.weights_channels_axis)
+            return WeightsSymmetricInferableQuantizer(num_bits=self.num_bits,
+                                                      threshold=np_threshold,
+                                                      per_channel=self.quantization_config.weights_per_channel_threshold,
+                                                      channel_axis=self.quantization_config.weights_channels_axis)
 
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Activation,
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.POWER_OF_TWO, QuantizationMethod.SYMMETRIC],
                 quantizer_type=TrainingMethod.STE)
-class STEActivationQuantizer(BasePytorchQATTrainableQuantizer):
+class STEActivationQATQuantizer(BasePytorchQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer activations.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
-        Initialize a STEActivationQuantizer object with parameters to use
+        Initialize a STEActivationQATQuantizer object with parameters to use
         for symmetric or power of two quantization.
 
         Args:
             quantization_config: trainable quantizer config class
         """
         super().__init__(quantization_config)
         self.power_of_two = quantization_config.activation_quantization_method == QuantizationMethod.POWER_OF_TWO
         self.sign = quantization_config.activation_quantization_params['is_signed']
         np_threshold_values = quantization_config.activation_quantization_params[C.THRESHOLD]
         self.threshold_tensor = torch.Tensor([np_threshold_values])
         self.num_bits = quantization_config.activation_n_bits
-        self.quantizer_parameters = {}
 
     def initialize_quantization(self,
                                 tensor_shape: torch.Size,
                                 name: str,
-                                layer: qi.PytorchQuantizationWrapper) -> Dict[str, nn.Parameter]:
+                                layer: PytorchQuantizationWrapper):
         """
-        Add threshold variables to layer.
+        Add quantizer parameters to the quantizer parameters dictionary
+
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
         layer.register_parameter(name, nn.Parameter(to_torch_tensor(self.threshold_tensor), requires_grad=True))
 
         # save the quantizer added parameters for later calculations
-        self.quantizer_parameters = {THRESHOLD_TENSOR: layer.get_parameter(name)}
-        return self.quantizer_parameters
+        self.add_quantizer_variable(THRESHOLD_TENSOR, layer.get_parameter(name), VariableGroup.QPARAMS)
 
     def __call__(self,
                  inputs: torch.Tensor,
                  training: bool = True) -> torch.Tensor:
         """
         Quantize a tensor.
         Args:
             inputs: Input tensor to quantize.
             training: Whether the graph is in training mode.
 
         Returns:
             The quantized tensor.
         """
 
-        _t = self.quantizer_parameters[THRESHOLD_TENSOR]
+        _t = self.get_quantizer_variable(THRESHOLD_TENSOR)
         q_tensor = symmetric_quantizer(inputs, _t, self.num_bits, sign=self.sign)
         return q_tensor
 
-    def convert2inferable(self) -> Union[iq.ActivationPOTInferableQuantizer, iq.ActivationSymmetricInferableQuantizer]:
+    def convert2inferable(self) -> Union[ActivationPOTInferableQuantizer, ActivationSymmetricInferableQuantizer]:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             A pytorch inferable quanizer object.
         """
-        np_threshold = self.quantizer_parameters[THRESHOLD_TENSOR].cpu().detach().numpy()
+        np_threshold = self.get_quantizer_variable(THRESHOLD_TENSOR).cpu().detach().numpy()
         if self.power_of_two:
             pot_threshold = np.power(2.0, np.ceil(np.log2(np_threshold)))
-            return iq.ActivationPOTInferableQuantizer(num_bits=self.num_bits,
-                                                      threshold=pot_threshold,
-                                                      signed=self.sign)
+            return ActivationPOTInferableQuantizer(num_bits=self.num_bits,
+                                                   threshold=pot_threshold,
+                                                   signed=self.sign)
         else:
-            return iq.ActivationSymmetricInferableQuantizer(num_bits=self.num_bits,
-                                                            threshold=np_threshold,
-                                                            signed=self.sign)
+            return ActivationSymmetricInferableQuantizer(num_bits=self.num_bits,
+                                                         threshold=np_threshold,
+                                                         signed=self.sign)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,39 +8,42 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Dict
-
 import numpy as np
 import torch
 import torch.nn as nn
 from torch import Tensor
 
-from model_compression_toolkit.core.common.constants import RANGE_MAX, RANGE_MIN
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.qat.common.constants import FQ_MIN, FQ_MAX
-from model_compression_toolkit.core.common import constants as C
-from model_compression_toolkit import quantizers_infrastructure as qi, TrainingMethod
+from model_compression_toolkit.constants import RANGE_MAX, RANGE_MIN
+from model_compression_toolkit.trainable_infrastructure.common.constants import FQ_MIN, FQ_MAX
+
+from model_compression_toolkit.qat import TrainingMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from mct_quantizers import QuantizationTarget, PytorchQuantizationWrapper
+from model_compression_toolkit import constants as C
+
 from model_compression_toolkit.qat.pytorch.quantizer.base_pytorch_qat_quantizer import BasePytorchQATTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import mark_quantizer
-from model_compression_toolkit.quantizers_infrastructure.pytorch import inferable_quantizers as iq
+from mct_quantizers import mark_quantizer
 from model_compression_toolkit.core.pytorch.utils import to_torch_tensor
 from model_compression_toolkit.qat.pytorch.quantizer.quantizer_utils import uniform_quantizer
-from model_compression_toolkit.quantizers_infrastructure.common.trainable_quantizer_config import \
+from mct_quantizers.pytorch.quantizers import \
+    WeightsUniformInferableQuantizer, ActivationUniformInferableQuantizer
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Weights,
+@mark_quantizer(quantization_target=QuantizationTarget.Weights,
                 quantization_method=[QuantizationMethod.UNIFORM],
                 quantizer_type=TrainingMethod.STE)
-class STEUniformWeightQuantizer(BasePytorchQATTrainableQuantizer):
+class STEUniformWeightQATQuantizer(BasePytorchQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer inputs.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerWeightsConfig):
         """
         Initialize a TrainableWeightQuantizer object with parameters to use
@@ -59,132 +62,132 @@
         self.max = np.reshape(self.max_values,
                               [-1]) if self.quantization_config.weights_per_channel_threshold else float(
             self.max_values)
         self.min = np.reshape(self.min_values,
                               [-1]) if self.quantization_config.weights_per_channel_threshold else float(
             self.min_values)
 
-        self.quantizer_parameters = {}
 
     def initialize_quantization(self,
                                 tensor_shape: torch.Size,
                                 name: str,
-                                layer: qi.PytorchQuantizationWrapper) -> Dict[str, nn.Parameter]:
+                                layer: PytorchQuantizationWrapper):
         """
-        Add min and max variables to layer.
-        Args:
-            tensor_shape: Tensor shape the quantizer quantize.
-            name: Prefix of variables names.
-            layer: Layer to add the variables to. The variables are saved
-            in the layer's scope.
+        Add quantizer parameters to the quantizer parameters dictionary
 
-        Returns:
-            Dictionary of new variables.
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
 
         # Add min and max variables to layer.
         layer.register_parameter(name+"_"+FQ_MIN, nn.Parameter(to_torch_tensor(self.min_values), requires_grad=False))
         layer.register_parameter(name+"_"+FQ_MAX, nn.Parameter(to_torch_tensor(self.max_values), requires_grad=False))
 
         # Save the quantizer parameters for later calculations
-        self.quantizer_parameters = {FQ_MIN: layer.get_parameter(name+"_"+FQ_MIN), FQ_MAX: layer.get_parameter(name+"_"+FQ_MAX)}
+        self.add_quantizer_variable(FQ_MIN, layer.get_parameter(name+"_"+FQ_MIN), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, layer.get_parameter(name+"_"+FQ_MAX), VariableGroup.QPARAMS)
 
-        return self.quantizer_parameters
 
     def __call__(self,
                  inputs: nn.Parameter,
                  training: bool) -> Tensor:
         """
         Quantize a tensor
         Args:
             inputs: Input tensor to quantize.
             training: whether in training mode or not
         Returns:
             quantized tensor
         """
-        return uniform_quantizer(inputs, self.quantizer_parameters[FQ_MIN], self.quantizer_parameters[FQ_MAX], self.num_bits)
+        return uniform_quantizer(inputs, self.get_quantizer_variable(FQ_MIN), self.get_quantizer_variable(FQ_MAX), self.num_bits)
 
-    def convert2inferable(self) -> iq.WeightsUniformInferableQuantizer:
+    def convert2inferable(self) -> WeightsUniformInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             A pytorch inferable quanizer object.
         """
-        _min = self.quantizer_parameters[FQ_MIN].cpu().detach().numpy()
-        _max = self.quantizer_parameters[FQ_MAX].cpu().detach().numpy()
+        _min = self.get_quantizer_variable(FQ_MIN).cpu().detach().numpy()
+        _max = self.get_quantizer_variable(FQ_MAX).cpu().detach().numpy()
 
-        return iq.WeightsUniformInferableQuantizer(num_bits=self.num_bits,
-                                                   min_range=_min, max_range=_max,
-                                                   per_channel=self.quantization_config.weights_per_channel_threshold,
-                                                   channel_axis=self.quantization_config.weights_channels_axis)
+        return WeightsUniformInferableQuantizer(num_bits=self.num_bits,
+                                                min_range=_min, max_range=_max,
+                                                per_channel=self.quantization_config.weights_per_channel_threshold,
+                                                channel_axis=self.quantization_config.weights_channels_axis)
 
 
-@mark_quantizer(quantization_target=qi.QuantizationTarget.Activation,
+@mark_quantizer(quantization_target=QuantizationTarget.Activation,
                 quantization_method=[QuantizationMethod.UNIFORM],
                 quantizer_type=TrainingMethod.STE)
-class STEUniformActivationQuantizer(BasePytorchQATTrainableQuantizer):
+class STEUniformActivationQATQuantizer(BasePytorchQATTrainableQuantizer):
     """
     Trainable constrained quantizer to quantize a layer activations.
     """
 
     def __init__(self, quantization_config: TrainableQuantizerActivationConfig):
         """
-        Initialize a STEUniformActivationQuantizer object with parameters to use
+        Initialize a STEUniformActivationQATQuantizer object with parameters to use
         for uniform quantization.
 
         Args:
             quantization_config: trainable quantizer config class
         """
         super().__init__(quantization_config)
 
         np_min_range = quantization_config.activation_quantization_params[C.RANGE_MIN]
         np_max_range = quantization_config.activation_quantization_params[C.RANGE_MAX]
         self.min_range_tensor = torch.Tensor([np_min_range])
         self.max_range_tensor = torch.Tensor([np_max_range])
         self.num_bits = quantization_config.activation_n_bits
-        self.quantizer_parameters = {}
 
     def initialize_quantization(self,
                                 tensor_shape: torch.Size,
                                 name: str,
-                                layer: qi.PytorchQuantizationWrapper) -> Dict[str, nn.Parameter]:
+                                layer: PytorchQuantizationWrapper):
         """
-        Add min and max variables to layer.
+        Add quantizer parameters to the quantizer parameters dictionary
+
+        Args:
+            tensor_shape: tensor shape of the quantized tensor.
+            name: Tensor name.
+            layer: Layer to quantize.
         """
         layer.register_parameter(name+"_"+FQ_MIN, nn.Parameter(to_torch_tensor(self.min_range_tensor), requires_grad=True))
         layer.register_parameter(name+"_"+FQ_MAX, nn.Parameter(to_torch_tensor(self.max_range_tensor), requires_grad=True))
 
         # Save the quantizer parameters for later calculations
-        self.quantizer_parameters = {FQ_MIN: layer.get_parameter(name+"_"+FQ_MIN), FQ_MAX: layer.get_parameter(name+"_"+FQ_MAX)}
-        return self.quantizer_parameters
+        self.add_quantizer_variable(FQ_MIN, layer.get_parameter(name+"_"+FQ_MIN), VariableGroup.QPARAMS)
+        self.add_quantizer_variable(FQ_MAX, layer.get_parameter(name+"_"+FQ_MAX), VariableGroup.QPARAMS)
 
     def __call__(self,
                  inputs: torch.Tensor,
                  training: bool = True) -> torch.Tensor:
         """
         Quantize a tensor.
         Args:
             inputs: Input tensor to quantize.
             training: Whether the graph is in training mode.
 
         Returns:
             The quantized tensor.
         """
 
-        _min = self.quantizer_parameters[FQ_MIN]
-        _max = self.quantizer_parameters[FQ_MAX]
+        _min = self.get_quantizer_variable(FQ_MIN)
+        _max = self.get_quantizer_variable(FQ_MAX)
         q_tensor = uniform_quantizer(inputs, _min, _max, self.num_bits)
         return q_tensor
 
-    def convert2inferable(self) -> iq.ActivationUniformInferableQuantizer:
+    def convert2inferable(self) -> ActivationUniformInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             A pytorch inferable quanizer object.
         """
-        _min = self.quantizer_parameters[FQ_MIN].cpu().detach().numpy()
-        _max = self.quantizer_parameters[FQ_MAX].cpu().detach().numpy()
+        _min = self.get_quantizer_variable(FQ_MIN).cpu().detach().numpy()
+        _max = self.get_quantizer_variable(FQ_MAX).cpu().detach().numpy()
 
-        return iq.ActivationUniformInferableQuantizer(num_bits=self.num_bits,
-                                                      min_range=_min, max_range=_max)
+        return ActivationUniformInferableQuantizer(num_bits=self.num_bits,
+                                                   min_range=_min, max_range=_max)
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/base_trainable_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py`

 * *Files 14% similar despite different names*

```diff
@@ -8,46 +8,60 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-from typing import Union
+from abc import abstractmethod
+from enum import Enum
+from typing import Union, List, Any
 from inspect import signature
 
 from model_compression_toolkit.core import common
-from model_compression_toolkit.core.common import Logger
+from model_compression_toolkit.logger import Logger
 
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import BaseInferableQuantizer, \
+from mct_quantizers.common.base_inferable_quantizer import BaseInferableQuantizer, \
     QuantizationTarget
-from model_compression_toolkit.quantizers_infrastructure.common.trainable_quantizer_config import \
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerActivationConfig, TrainableQuantizerWeightsConfig
-from model_compression_toolkit.quantizers_infrastructure.common.constants import QUANTIZATION_METHOD, \
+from mct_quantizers.common.constants import QUANTIZATION_METHOD, \
     QUANTIZATION_TARGET
 
 
+VAR = 'var'
+GROUP = 'group'
+
+class VariableGroup(Enum):
+    """
+    An enum for choosing trainable variable group
+    0. WEIGHTS
+    1. QPARAMS
+    """
+    WEIGHTS = 0
+    QPARAMS = 1
+
+
 class BaseTrainableQuantizer(BaseInferableQuantizer):
     def __init__(self,
                  quantization_config: Union[TrainableQuantizerActivationConfig, TrainableQuantizerWeightsConfig]):
         """
         This class is a base quantizer which validates the provided quantization config and defines an abstract function which any quantizer needs to implment.
 
         Args:
             quantization_config: quantizer config class contains all the information about the quantizer configuration.
         """
 
         # verify the quantizer class that inherits this class only has a config argument and key-word arguments
         for i, (k, v) in enumerate(self.get_sig().parameters.items()):
             if i == 0:
                 if v.annotation not in [TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]:
-                    common.Logger.error(f"First parameter must be either TrainableQuantizerWeightsConfig or TrainableQuantizerActivationConfig")  # pragma: no cover
+                    Logger.error(f"First parameter must be either TrainableQuantizerWeightsConfig or TrainableQuantizerActivationConfig")  # pragma: no cover
             elif v.default is v.empty:
-                common.Logger.error(f"Parameter {k} doesn't have a default value")  # pragma: no cover
+                Logger.error(f"Parameter {k} doesn't have a default value")  # pragma: no cover
 
         super(BaseTrainableQuantizer, self).__init__()
         self.quantization_config = quantization_config
 
         # Inherited class should be decorated with @mark_quantizer decorator, and define the following static properties
         static_quantization_method = getattr(self, QUANTIZATION_METHOD, None)
         static_quantization_target = getattr(self, QUANTIZATION_TARGET, None)
@@ -55,25 +69,27 @@
         if static_quantization_method is None or static_quantization_target is None:
             Logger.error("A quantizer class that inherit from BaseTrainableQuantizer is not defined appropriately."
                          "Either it misses the @mark_quantizer decorator or the decorator is not used correctly.")
 
         if static_quantization_target == QuantizationTarget.Weights:
             self.validate_weights()
             if self.quantization_config.weights_quantization_method not in static_quantization_method:
-                common.Logger.error(
+                Logger.error(
                     f'Quantization method mismatch expected: {static_quantization_method} and got  {self.quantization_config.weights_quantization_method}')
         elif static_quantization_target == QuantizationTarget.Activation:
             self.validate_activation()
             if self.quantization_config.activation_quantization_method not in static_quantization_method:
-                common.Logger.error(
+                Logger.error(
                     f'Quantization method mismatch expected: {static_quantization_method} and got  {self.quantization_config.activation_quantization_method}')
         else:
-            common.Logger.error(
+            Logger.error(
                 f'Unknown Quantization Part:{static_quantization_target}')  # pragma: no cover
 
+        self.quantizer_parameters = {}
+
     @classmethod
     def get_sig(cls):
         return signature(cls)
 
     def initialize_quantization(self,
                                 tensor_shape,
                                 name: str,
@@ -125,25 +141,60 @@
     def validate_weights(self) -> None:
         """
         This function validates the quantization config compared with its parameters.
 
 
         """
         if self.activation_quantization() or not self.weights_quantization():
-            common.Logger.error(f'Expect weight quantization got activation')
+            Logger.error(f'Expect weight quantization got activation')
 
     def validate_activation(self) -> None:
         """
         This function validates the quantization config compared with its parameters.
 
         """
         if not self.activation_quantization() or self.weights_quantization():
-            common.Logger.error(f'Expect activation quantization got weight')
+            Logger.error(f'Expect activation quantization got weight')
 
     def convert2inferable(self) -> BaseInferableQuantizer:
         """
         Convert quantizer to inferable quantizer.
 
         Returns:
             BaseInferableQuantizer object.
         """
         raise NotImplemented  # pragma: no cover
+
+    def add_quantizer_variable(self, name: str, variable: Any, group: VariableGroup = VariableGroup.WEIGHTS):
+        """
+        Add a quantizer variable to quantizer_parameters dictionary
+        """
+        self.quantizer_parameters.update({name: {VAR: variable, GROUP: group}})
+
+    def get_quantizer_variable(self, name: str) -> Any:
+        """
+        Get a quantizer variable by name
+
+        Args:
+            name: variable name
+
+        Returns:
+            trainable variable
+        """
+        if name in self.quantizer_parameters:
+            return self.quantizer_parameters[name][VAR]
+        else:
+            Logger.error(f'Variable {name} is not exist in quantizers parameters!') # pragma: no cover
+
+
+    @abstractmethod
+    def get_trainable_variables(self, group: VariableGroup) -> List[Any]:
+        """
+        Get trainable parameters with specific group from quantizer
+
+        Args:
+            group: Enum of variable group
+
+        Returns:
+            List of trainable variables
+        """
+        raise NotImplemented  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/common/trainable_quantizer_config.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 from abc import ABC
 from typing import Dict, List
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
 
 
 class TrainableQuantizerCandidateConfig:
 
     def __init__(self,
                  n_bits: int,
                  quantization_params: Dict,
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/base_keras_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,35 +8,36 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from typing import Dict, Any, Union
+from typing import Dict, Any, Union, List
 
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import FOUND_TF
-
-from model_compression_toolkit.quantizers_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer
-from model_compression_toolkit.quantizers_infrastructure import TrainableQuantizerWeightsConfig, \
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import VariableGroup
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer, VAR, GROUP
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, \
     TrainableQuantizerActivationConfig
 
 if FOUND_TF:
     QUANTIZATION_CONFIG = 'quantization_config'
-    from model_compression_toolkit.quantizers_infrastructure.keras.config_serialization import config_serialization, \
+    from model_compression_toolkit.trainable_infrastructure.keras.config_serialization import config_serialization, \
         config_deserialization
-
+    import tensorflow as tf
 
     class BaseKerasTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             """
             This class is a base quantizer which validates provided quantization config and defines an abstract function which any quantizer needs to implement.
             This class adds to the base quantizer a get_config and from_config functions to enable loading and saving the keras model.
+
             Args:
                 quantization_config: quantizer config class contains all the information about a quantizer configuration.
             """
             super().__init__(quantization_config)
 
         def get_config(self) -> Dict[str, Any]:
             """
@@ -57,14 +58,32 @@
 
             """
             config = config.copy()
             quantization_config = config_deserialization(config[QUANTIZATION_CONFIG])
             # Note that a quantizer only receive quantization config and the rest of define hardcoded inside the speficie quantizer.
             return cls(quantization_config=quantization_config)
 
+        def get_trainable_variables(self, group: VariableGroup) -> List[tf.Tensor]:
+            """
+            Get trainable parameters with specific group from quantizer
+
+            Args:
+                group: Enum of variable group
+
+            Returns:
+                List of trainable variables
+            """
+            quantizer_trainable = []
+            for name, parameter_dict in self.quantizer_parameters.items():
+                quantizer_parameter, parameter_group = parameter_dict[VAR], parameter_dict[GROUP]
+                if quantizer_parameter.trainable and parameter_group == group:
+                    quantizer_trainable.append(quantizer_parameter)
+            return quantizer_trainable
+
+
 else:
     class BaseKerasTrainableQuantizer(BaseTrainableQuantizer):
         def __init__(self,
                      quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
 
             super().__init__(quantization_config)
             Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/config_serialization.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py`

 * *Files 2% similar despite different names*

```diff
@@ -13,18 +13,18 @@
 # limitations under the License.
 # ==============================================================================
 import copy
 
 from typing import Any, Union
 from enum import Enum
 
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.quantizers_infrastructure.common.trainable_quantizer_config import \
+from model_compression_toolkit.target_platform_capabilities.target_platform import QuantizationMethod
+from model_compression_toolkit.trainable_infrastructure.common.trainable_quantizer_config import \
     TrainableQuantizerActivationConfig, TrainableQuantizerWeightsConfig
-from model_compression_toolkit.quantizers_infrastructure.common import constants as C
+from mct_quantizers.common import constants as C
 
 
 def transform_enum(v: Any):
     """
     If an enum is received it value is return otherwise the input is returned.
     Args:
         v: Any type
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/constants.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/target_platform_capabilities/constants.py`

 * *Files 15% similar despite different names*

```diff
@@ -9,16 +9,19 @@
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
 
-# Inferable keras quantizer signature parameters:
-NUM_BITS = 'num_bits'
-SIGNED = 'signed'
-THRESHOLD = 'threshold'
-PER_CHANNEL = 'per_channel'
-MIN_RANGE = 'min_range'
-MAX_RANGE = 'max_range'
-CHANNEL_AXIS = 'channel_axis'
-INPUT_RANK = 'input_rank'
+# TP Model constants
+OPS_SET_LIST = 'ops_set_list'
+
+# Version
+LATEST = 'latest'
+
+
+# Supported TP models names:
+DEFAULT_TP_MODEL = 'default'
+IMX500_TP_MODEL = 'imx500'
+TFLITE_TP_MODEL = 'tflite'
+QNNPACK_TP_MODEL = 'qnnpack'
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/keras/load_model.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/trainable_infrastructure/keras/load_model.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,61 +1,64 @@
-# Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.
+# Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-from model_compression_toolkit.core.common import Logger
-from model_compression_toolkit.core.common.constants import FOUND_TF
-from model_compression_toolkit.quantizers_infrastructure.common.get_all_subclasses import get_all_subclasses
+from typing import Any
+
+import mct_quantizers
+from mct_quantizers.common.get_all_subclasses import get_all_subclasses
+
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.logger import Logger
 
 if FOUND_TF:
     import tensorflow as tf
-    from model_compression_toolkit import quantizers_infrastructure as qi
-    from model_compression_toolkit.quantizers_infrastructure import BaseKerasTrainableQuantizer
-    from model_compression_toolkit.quantizers_infrastructure.keras.inferable_quantizers.base_keras_inferable_quantizer \
-        import \
-        BaseKerasInferableQuantizer
+    from tensorflow.python.saved_model.load_options import LoadOptions
+    from model_compression_toolkit.trainable_infrastructure import BaseKerasTrainableQuantizer
     keras = tf.keras
 
-    def keras_load_quantized_model(filepath, custom_objects=None, compile=True, options=None):
+    def keras_load_quantized_model(filepath: str, custom_objects: Any = None, compile: bool = True,
+                                   options: LoadOptions = None):
         """
-        This function wraps the keras load model and MCT quantization custom class to it.
+        This function wraps the keras load model and adds trainable quantizers classes to its custom objects.
 
         Args:
             filepath: the model file path.
             custom_objects: Additional custom objects
             compile: Boolean, whether to compile the model after loading.
             options: Optional `tf.saved_model.LoadOptions` object that specifies options for loading from SavedModel.
 
         Returns: A keras Model
 
         """
-        qi_inferable_custom_objects = {subclass.__name__: subclass for subclass in get_all_subclasses(BaseKerasInferableQuantizer)}
+
         qi_trainable_custom_objects = {subclass.__name__: subclass for subclass in
                                        get_all_subclasses(BaseKerasTrainableQuantizer)}
+        all_trainable_names = list(qi_trainable_custom_objects.keys())
+        if len(set(all_trainable_names)) < len(all_trainable_names):
+            Logger.error(f"Found multiple quantizers with the same name that inherit from BaseKerasTrainableQuantizer"
+                         f"while trying to load a model.")
 
-        # Merge dictionaries into one dict
-        qi_custom_objects = {**qi_inferable_custom_objects, **qi_trainable_custom_objects}
+        qi_custom_objects = {**qi_trainable_custom_objects}
 
-        # Add non-quantizers custom objects
-        qi_custom_objects.update({qi.KerasQuantizationWrapper.__name__: qi.KerasQuantizationWrapper})
         if custom_objects is not None:
             qi_custom_objects.update(custom_objects)
-        return tf.keras.models.load_model(filepath,
-                                          custom_objects=qi_custom_objects, compile=compile,
-                                          options=options)
+        return mct_quantizers.keras_load_quantized_model(filepath,
+                                                         custom_objects=qi_custom_objects, compile=compile,
+                                                         options=options)
 else:
     def keras_load_quantized_model(filepath, custom_objects=None, compile=True, options=None):
         """
         This function wraps the keras load model and MCT quantization custom class to it.
 
         Args:
             filepath: the model file path.
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/constants.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,16 +8,22 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from enum import Enum
 
-# Inferable pytorch quantizer signature parameters:
-NUM_BITS = 'num_bits'
-SIGNED = 'signed'
-THRESHOLD = 'threshold'
-PER_CHANNEL = 'per_channel'
-MIN_RANGE = 'min_range'
-MAX_RANGE = 'max_range'
-CHANNEL_AXIS = 'channel_axis'
+
+class KerasExportSerializationFormat(Enum):
+    """
+    Specify which serialization format to use for exporting a quantized Keras model.
+
+    KERAS_H5 - .h5 file format
+
+    TFLITE - .tflite file format
+
+    """
+
+    KERAS_H5 = 0
+    TFLITE = 1
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py` & `model_compression_toolkit-1.9.0/model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py`

 * *Files 26% similar despite different names*

```diff
@@ -8,98 +8,104 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
+from abc import abstractmethod
+from typing import Union, Dict, List
 
-import numpy as np
+from model_compression_toolkit.logger import Logger
+from model_compression_toolkit.constants import FOUND_TF
+from model_compression_toolkit.gptq.common.gptq_constants import WEIGHTS_QUANTIZATION_PARAMS
 
-from model_compression_toolkit.core.common.constants import FOUND_TORCH
-from model_compression_toolkit.core.common.target_platform import QuantizationMethod
-from model_compression_toolkit.quantizers_infrastructure.common.base_inferable_quantizer import mark_quantizer, \
-    QuantizationTarget
-
-if FOUND_TORCH:
-    import torch
-    from model_compression_toolkit.quantizers_infrastructure.pytorch.quantizer_utils import to_torch_tensor, \
-        get_working_device
-    from model_compression_toolkit.quantizers_infrastructure.pytorch.inferable_quantizers \
-        .base_symmetric_inferable_quantizer import \
-        BaseSymmetricInferableQuantizer
-
-
-    @mark_quantizer(quantization_target=QuantizationTarget.Weights,
-                    quantization_method=[QuantizationMethod.SYMMETRIC],
-                    quantizer_type=None)
-    class WeightsSymmetricInferableQuantizer(BaseSymmetricInferableQuantizer):
+from model_compression_toolkit.trainable_infrastructure import TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig
+from model_compression_toolkit.trainable_infrastructure.common.base_trainable_quantizer import BaseTrainableQuantizer
+
+if FOUND_TF:
+    import tensorflow as tf
+
+    from model_compression_toolkit.trainable_infrastructure import BaseKerasTrainableQuantizer
+    from mct_quantizers import KerasQuantizationWrapper
+
+    class BaseKerasGPTQTrainableQuantizer(BaseKerasTrainableQuantizer):
         """
-        Class for quantizing weights using a symmetric quantizer
+        A base class for trainable Keras quantizer for GPTQ.
         """
 
         def __init__(self,
-                     num_bits: int,
-                     threshold: np.ndarray,
-                     per_channel: bool,
-                     channel_axis: int = None
-                     ):
+                     quantization_config: Union[TrainableQuantizerWeightsConfig, TrainableQuantizerActivationConfig]):
             """
-            Initialize the quantizer with the specified parameters.
+            Initializes BaseKerasGPTQTrainableQuantizer object.
 
             Args:
-                num_bits: number of bits to use for quantization
-                threshold: threshold for quantizing weights
-                per_channel: whether to use per-channel quantization
-                channel_axis: Axis of input to apply per-channel quantization on.
-            """
-
-            super(WeightsSymmetricInferableQuantizer, self).__init__(threshold=threshold,
-                                                                     num_bits=num_bits,
-                                                                     signed=True)
-
-            if per_channel:
-                assert channel_axis is not None, f'Channel axis is missing in per channel quantization'
-                assert len(
-                    threshold) >= 1, f'In per-channel quantization threshold should be of length >= 1 but is ' \
-                                     f'{len(threshold)}'
-            else:
-                assert len(
-                    threshold) == 1, f'In per-tensor quantization threshold should be of length 1 but is {len(threshold)}'
-
-            self.per_channel = per_channel
-            self.channel_axis = channel_axis
+                quantization_config: quantizer config class contains all the information about a quantizer configuration.
+            """
 
-            self.scales = to_torch_tensor(self.scales).to(get_working_device())
-            self.zero_points = torch.zeros(len(threshold), dtype=torch.int32).to(get_working_device())
+            super().__init__(quantization_config)
 
-        def __call__(self, inputs: torch.Tensor) -> torch.Tensor:
+
+        def update_layer_quantization_params(self, layer: KerasQuantizationWrapper
+                                             ) -> (Dict[str, tf.Tensor], Dict[str, Dict], Dict):
             """
-            Quantize the given inputs using the quantizer parameters.
+            A Function to calculate the needed change in attributes in NodeQuantizationConfig after retraining.
 
             Args:
-                inputs: input tensor to quantize
+                layer: A wrapped Keras layer.
 
             Returns:
-                quantized tensor.
+                3 dictionaries describing the change in layer's weights, weights config, activation config
+                that changed during GPTQ retraining.
+                Keys must match NodeQuantizationConfig attributes
+
+            """
+            weights = {}
+            for weight, quantizer_vars, quantizer in layer.get_weights_vars():
+                if not isinstance(quantizer, BaseTrainableQuantizer):
+                    Logger.error(f"Expecting a GPTQ trainable quantizer, "  # pragma: no cover
+                                 f"but got {type(quantizer)} which is not callable.")
+                weights.update({weight: quantizer(training=False, inputs=quantizer_vars)})
+
+            quant_config = {WEIGHTS_QUANTIZATION_PARAMS: self.get_quant_config()}
+
+            return weights, quant_config, {}
+
+        def get_aux_variable(self) -> List[tf.Tensor]:
+            """
+            This function return a list with the quantizer's quantization auxiliary variables.
+
+            Returns: A list with the quantization auxiliary variables.
+
             """
-            inputs.requires_grad = False
-            if self.per_channel:
-                return torch.fake_quantize_per_channel_affine(inputs,
-                                                              self.scales,
-                                                              self.zero_points,
-                                                              axis=self.channel_axis,
-                                                              quant_min=self.min_quantized_domain,
-                                                              quant_max=self.max_quantized_domain)
-            return torch.fake_quantize_per_tensor_affine(inputs,
-                                                         self.scales,
-                                                         self.zero_points,
-                                                         quant_min=self.min_quantized_domain,
-                                                         quant_max=self.max_quantized_domain)
 
+            return []  # pragma: no cover
+
+        def get_quantization_variable(self) -> List[tf.Tensor]:
+            """
+            This function return a list with the quantizer's quantization parameters variables.
+
+            Returns: A list with the quantization parameters.
+
+            """
+
+            return []  # pragma: no cover
+
+        @abstractmethod
+        def get_quant_config(self):
+            """
+            Returns the config used to edit NodeQuantizationConfig after GPTQ retraining.
+
+            Returns:
+                A dictionary of attributes the quantize_config retraining has changed during GPTQ retraining.
+                Keys must match NodeQuantizationConfig attributes.
+
+            """
+            raise NotImplemented(f'{self.__class__.__name__} have to implement the '  # pragma: no cover
+                                 f'quantizer\'s get_quant_config.')
 
 else:
-    class WeightsSymmetricInferableQuantizer:
+    class BaseKerasGPTQTrainableQuantizer:  # pragma: no cover
         def __init__(self, *args, **kwargs):
-            raise Exception('Installing torch is mandatory '
-                            'when using WeightsSymmetricInferableQuantizer. '
-                            'Could not find torch package.')
+            Logger.critical('Installing tensorflow and tensorflow_model_optimization is mandatory '
+                            'when using BaseKerasGPTQTrainableQuantizer. '
+                            'Could not find Tensorflow package.')  # pragma: no cover
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/PKG-INFO` & `model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/PKG-INFO`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: model-compression-toolkit
-Version: 1.8.0
+Version: 1.9.0
 Summary: A Model Compression Toolkit for neural networks
 Home-page: UNKNOWN
 License: UNKNOWN
 Description: # Model Compression Toolkit (MCT)
         
         Model Compression Toolkit (MCT) is an open-source project for neural network model optimization under efficient, constrained hardware.
         
@@ -16,128 +16,107 @@
         
         MCT is developed by researchers and engineers working at Sony Semiconductor Israel.
         
         
         
         ## Table of Contents
         
-        - [Supported features](#supported-features)
         - [Getting Started](#getting-started)
+        - [Supported features](#supported-features)
         - [Results](#results)
         - [Contributions](#contributions)
         - [License](#license)
         
-        ## Supported Features
-        
-        MCT supports different quantization methods:
-        * Post training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
-        * Gradient-based post training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
-        * Quantization aware training (QAT)[*](#experimental-features)
-        
-        
-        | Quantization Method | Complexity                                    | Computational Cost          |
-        |---------------------|-----------------------------------------------|-----------------------------|
-        | PTQ                 | Low                                           | Low (order of minutes)      |
-        | GPTQ                | Mild (parameters fine-tuning using gradients) | Mild (order of 2-3 hours)   |
-        | QAT                 | High                                          | High (order of 12-36 hours) |
-        
-        
-        In addition, MCT supports different quantization schemes for quantizing weights and activations:
-        * Power-Of-Two (hardware-friendly quantization [1])
-        * Symmetric
-        * Uniform
-        
-        Core features:
-        * <ins>Graph optimizations:</ins> Transforming the model to an equivalent (yet, more efficient) model (for example, batch-normalization layer folding to its preceding linear layer).
-        * <ins>Quantization parameter search:</ins> Different methods can be used to minimize the expected added quantization-noise during thresholds search (by default, we use Mean-Square-Errorm but other metrics can be used such as No-Clipping, Mean-Average-Error, and more).
-        * <ins>Advanced quantization algorithms:</ins> To prevent a performance degradation some algorithms are applied such as: 
-          * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
-          * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
-        * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
-        * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
-        * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
-        
-        
-        #### Experimental features 
-        
-        Some features are experimental and subject to future changes. 
-         
-        For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
-        
         
         ## Getting Started
         
-        This section provides a quick starting guide. We begin with installation via source code or pip server. Then, we provide a short usage example.
+        This section provides an installation and a quick starting guide.
         
         ### Installation
-        See the MCT install guide for the pip package, and build from the source.
-        
         
-        #### From Source
-        ```
-        git clone https://github.com/sony/model_optimization.git
-        python setup.py install
-        ```
-        #### From PyPi - latest stable release
+        To install the latest stable release of MCT, run the following command:
         ```
         pip install model-compression-toolkit
         ```
         
-        A nightly package is also available (unstable):
-        ```
-        pip install mct-nightly
-        ```
+        For installing the nightly version or installing from source, refer to the [installation guide](INSTALLATION.md).
         
-        ### Requierments
         
-        To run MCT, one of the supported frameworks, Tenosflow/Pytorch, needs to be installed.
+        ### Quick start & tutorials 
         
-        For using with Tensorflow please install the packages: 
-        [tensorflow](https://www.tensorflow.org/install), 
-        [tensorflow-model-optimization](https://www.tensorflow.org/model_optimization/guide/install)
+        For an example of how to use MCT with TensorFlow or PyTorch on various models and tasks,
+        check out the [quick-start page](tutorials/quick_start/README.md) and
+        the [results CSV](tutorials/quick_start/results/model_quantization_results.csv).
         
-        For using with PyTorch please install the packages: 
-        [torch](https://pytorch.org/)
-        
-        Also, a [requirements](requirements.txt) file can be used to set up your environment.
+        In addition, a set of [notebooks](tutorials/notebooks) are provided for an easy start. For example:
+        * [MobileNet with Tensorflow](tutorials/notebooks/example_keras_mobilenet.py).
+        * [MobileNetV2 with PyTorch](tutorials/notebooks/example_pytorch_mobilenet_v2.py).
         
         
         ### Supported Python Versions
         
         Currently, MCT is being tested on various Python versions:
         
-        
-        
         | Python Version                                                                                                                                                                                                                                                            |
         |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
         | [![Run Tests - Python 3.10](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python310.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python310.yml) |
         | [![Run Tests - Python 3.9](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python39.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python39.yml)   |
         | [![Run Tests - Python 3.8](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python38.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python38.yml)   |
         | [![Run Tests - Python 3.7](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python37.yml/badge.svg)](https://github.com/sony/model_optimization/actions/workflows/run_tests_suite_python37.yml)   |
         
         
         ### Supported NN-Frameworks Versions
         
-        Currently, MCT supports compressing models of TensorFlow and PyTorch, and
-        is tested on various versions:
+        MCT supports compressing models built with the TensorFlow or PyTorch frameworks, and is tested on various python versions:
         
         | TensorFlow Version                                                                                   | PyTorch Version                                                                                          |
         |------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------|
         | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf211.yml/badge.svg) | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_13.yml/badge.svg) |
         | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf210.yml/badge.svg) | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_12.yml/badge.svg) |
         | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_tf29.yml/badge.svg)  | ![tests](https://github.com/sony/model_optimization/actions/workflows/run_tests_torch1_11.yml/badge.svg) |
         
         
-        ### Usage Example 
-        For an example of how to use the post-training quantization, using Keras,
-        please use this [link](tutorials/example_keras_mobilenet.py).
+        ## Supported Features
+        
+        MCT supports different quantization methods:
+        * Post-training quantization (PTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_post_training_quantization_experimental.html#ug-keras-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_post_training_quantization_experimental.html#ug-pytorch-post-training-quantization-experimental)
+        * Gradient-based post-training quantization (GPTQ): [Keras API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/keras_gradient_post_training_quantization_experimental.html#ug-keras-gradient-post-training-quantization-experimental), [PyTorch API](https://sony.github.io/model_optimization/docs/api/experimental_api_docs/methods/pytorch_gradient_post_training_quantization_experimental.html#ug-pytorch-gradient-post-training-quantization-experimental)
+        * Quantization-aware training (QAT)[*](#experimental-features)
+        
+        
+        | Quantization Method                           | Complexity | Computational Cost          |
+        |-----------------------------------------------|------------|-----------------------------|
+        | PTQ                                           | Low        | Low (order of minutes)      |
+        | GPTQ (parameters fine-tuning using gradients) | Mild       | Mild (order of 2-3 hours)   |
+        | QAT                                           | High       | High (order of 12-36 hours) |
         
-        For an example using PyTorch, please use this [link](tutorials/example_pytorch_mobilenet_v2.py).
         
-        For more examples please see the [tutorials' directory](https://github.com/sony/model_optimization/tree/main/tutorials).
+        In addition, MCT supports different quantization schemes for quantizing weights and activations:
+        
+        * Power-Of-Two (hardware-friendly quantization [1])
+        * Symmetric
+        * Uniform
+        
+        Main features:
+        * <ins>Graph optimizations:</ins> Transforming the model to an equivalent (yet, more efficient) model (for example, batch-normalization layer folding to its preceding linear layer).
+        * <ins>Quantization parameter search:</ins> Different methods can be used to minimize the expected added quantization-noise during thresholds search (by default, we use Mean-Square-Error, but other metrics can be used such as No-Clipping, Mean-Average-Error, and more).
+        * <ins>Advanced quantization algorithms:</ins> To prevent a performance degradation some algorithms are applied such as: 
+          * <ins>Shift negative correction:</ins> Symmetric activation quantization can hurt the model's performance when some layers output both negative and positive activations, but their range is asymmetric. For more details please visit [1].
+          * <ins>Outliers filtering:</ins> Computing z-score for activation statistics to detect and remove outliers.
+        * <ins>Clustering:</ins> Using non-uniform quantization grid to quantize the weights and activations to match their distributions.[*](#experimental-features)
+        * <ins>Mixed-precision search:</ins> Assigning quantization bit-width per layer (for weights/activations), based on the layer's sensitivity to different bit-widths.
+        * <ins>Visualization:</ins> You can use TensorBoard to observe useful information for troubleshooting the quantized model's performance (for example, the model in different phases of the quantization, collected statistics, similarity between layers of the float and quantized model and bit-width configuration for mixed-precision quantization). For more details, please read the [visualization documentation](https://sony.github.io/model_optimization/docs/guidelines/visualization.html).   
+        * <ins>Target Platform Capabilities:</ins> The Target Platform Capabilities (TPC) describes the target platform (an edge device with dedicated hardware). For more details, please read the [TPC README](model_compression_toolkit/target_platform_capabilities/README.md).   
+        
+        
+        #### Experimental features 
+        
+        Some features are experimental and subject to future changes. 
+         
+        For more details, we highly recommend visiting our project website where experimental features are mentioned as experimental.
         
         
         ## Results
         ### Keras
         Graph of [MobileNetV2](https://keras.io/api/applications/mobilenet/) accuracy on ImageNet vs average bit-width of weights, using 
         single-precision quantization, mixed-precision quantization, and mixed-precision quantization with GPTQ.
         
@@ -151,15 +130,15 @@
         
         | Network Name              | Float Accuracy  | 8Bit Accuracy   | 
         | --------------------------| ---------------:| ---------------:| 
         | MobileNet V2 [3]          | 71.886          | 71.444           |                                      
         | ResNet-18 [3]             | 69.86           | 69.63           |                                      
         | SqueezeNet 1.1 [3]        | 58.128          | 57.678          |                                      
         
-        
+        For more results, please refer to [quick start](https://github.com/sony/model_optimization/tree/main/tutorials/quick_start).
         
         ## Contributions
         MCT aims at keeping a more up-to-date fork and welcomes contributions from anyone.
         
         *You will find more information about contributions in the [Contribution guide](CONTRIBUTING.md).
```

### Comparing `model_compression_toolkit-1.8.0/model_compression_toolkit.egg-info/SOURCES.txt` & `model_compression_toolkit-1.9.0/model_compression_toolkit.egg-info/SOURCES.txt`

 * *Files 18% similar despite different names*

```diff
@@ -1,30 +1,29 @@
 LICENSE.md
 README.md
 setup.cfg
 setup.py
 model_compression_toolkit/__init__.py
+model_compression_toolkit/constants.py
+model_compression_toolkit/logger.py
 model_compression_toolkit.egg-info/PKG-INFO
 model_compression_toolkit.egg-info/SOURCES.txt
 model_compression_toolkit.egg-info/dependency_links.txt
 model_compression_toolkit.egg-info/requires.txt
 model_compression_toolkit.egg-info/top_level.txt
 model_compression_toolkit/core/__init__.py
 model_compression_toolkit/core/analyzer.py
 model_compression_toolkit/core/exporter.py
 model_compression_toolkit/core/runner.py
 model_compression_toolkit/core/common/__init__.py
 model_compression_toolkit/core/common/base_substitutions.py
-model_compression_toolkit/core/common/constants.py
 model_compression_toolkit/core/common/data_loader.py
 model_compression_toolkit/core/common/defaultdict.py
 model_compression_toolkit/core/common/framework_implementation.py
 model_compression_toolkit/core/common/framework_info.py
-model_compression_toolkit/core/common/immutable.py
-model_compression_toolkit/core/common/logger.py
 model_compression_toolkit/core/common/memory_computation.py
 model_compression_toolkit/core/common/model_builder_mode.py
 model_compression_toolkit/core/common/model_collector.py
 model_compression_toolkit/core/common/model_validation.py
 model_compression_toolkit/core/common/node_prior_info.py
 model_compression_toolkit/core/common/similarity_analyzer.py
 model_compression_toolkit/core/common/user_info.py
@@ -125,40 +124,25 @@
 model_compression_toolkit/core/common/substitutions/linear_collapsing_substitution.py
 model_compression_toolkit/core/common/substitutions/residual_collapsing.py
 model_compression_toolkit/core/common/substitutions/scale_equalization.py
 model_compression_toolkit/core/common/substitutions/shift_negative_activation.py
 model_compression_toolkit/core/common/substitutions/softmax_shift.py
 model_compression_toolkit/core/common/substitutions/virtual_activation_weights_composition.py
 model_compression_toolkit/core/common/substitutions/weights_activation_split.py
-model_compression_toolkit/core/common/target_platform/__init__.py
-model_compression_toolkit/core/common/target_platform/current_tp_model.py
-model_compression_toolkit/core/common/target_platform/fusing.py
-model_compression_toolkit/core/common/target_platform/op_quantization_config.py
-model_compression_toolkit/core/common/target_platform/operators.py
-model_compression_toolkit/core/common/target_platform/target_platform_model.py
-model_compression_toolkit/core/common/target_platform/target_platform_model_component.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/__init__.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/attribute_filter.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/current_tpc.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/layer_filter_params.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/operations_to_layers.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/target_platform_capabilities.py
-model_compression_toolkit/core/common/target_platform/targetplatform2framework/target_platform_capabilities_component.py
 model_compression_toolkit/core/common/visualization/__init__.py
 model_compression_toolkit/core/common/visualization/final_config_visualizer.py
 model_compression_toolkit/core/common/visualization/nn_visualizer.py
 model_compression_toolkit/core/common/visualization/tensorboard_writer.py
 model_compression_toolkit/core/keras/__init__.py
 model_compression_toolkit/core/keras/constants.py
 model_compression_toolkit/core/keras/default_framework_info.py
 model_compression_toolkit/core/keras/keras_implementation.py
 model_compression_toolkit/core/keras/keras_model_validation.py
 model_compression_toolkit/core/keras/keras_node_prior_info.py
 model_compression_toolkit/core/keras/kpi_data_facade.py
-model_compression_toolkit/core/keras/quantization_facade.py
 model_compression_toolkit/core/keras/tf_tensor_numpy.py
 model_compression_toolkit/core/keras/back2framework/__init__.py
 model_compression_toolkit/core/keras/back2framework/factory_model_builder.py
 model_compression_toolkit/core/keras/back2framework/float_model_builder.py
 model_compression_toolkit/core/keras/back2framework/instance_builder.py
 model_compression_toolkit/core/keras/back2framework/keras_model_builder.py
 model_compression_toolkit/core/keras/back2framework/mixed_precision_model_builder.py
@@ -209,15 +193,14 @@
 model_compression_toolkit/core/keras/visualization/__init__.py
 model_compression_toolkit/core/pytorch/__init__.py
 model_compression_toolkit/core/pytorch/constants.py
 model_compression_toolkit/core/pytorch/default_framework_info.py
 model_compression_toolkit/core/pytorch/kpi_data_facade.py
 model_compression_toolkit/core/pytorch/pytorch_implementation.py
 model_compression_toolkit/core/pytorch/pytorch_node_prior_info.py
-model_compression_toolkit/core/pytorch/quantization_facade.py
 model_compression_toolkit/core/pytorch/utils.py
 model_compression_toolkit/core/pytorch/back2framework/__init__.py
 model_compression_toolkit/core/pytorch/back2framework/factory_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/float_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/instance_builder.py
 model_compression_toolkit/core/pytorch/back2framework/mixed_precision_model_builder.py
 model_compression_toolkit/core/pytorch/back2framework/model_gradients.py
@@ -251,88 +234,32 @@
 model_compression_toolkit/core/pytorch/quantizer/lut_fake_quant.py
 model_compression_toolkit/core/pytorch/reader/__init__.py
 model_compression_toolkit/core/pytorch/reader/graph_builders.py
 model_compression_toolkit/core/pytorch/reader/node_holders.py
 model_compression_toolkit/core/pytorch/reader/reader.py
 model_compression_toolkit/core/pytorch/statistics_correction/__init__.py
 model_compression_toolkit/core/pytorch/statistics_correction/apply_second_moment_correction.py
-model_compression_toolkit/core/tpc_models/__init__.py
-model_compression_toolkit/core/tpc_models/get_target_platform_capabilities.py
-model_compression_toolkit/core/tpc_models/default_tpc/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/target_platform_capabilities.py
-model_compression_toolkit/core/tpc_models/default_tpc/latest/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v1/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v1/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v1/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v1/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/default_tpc/v2/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v2/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v2/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v2/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v3_lut/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v4_lut/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/default_tpc/v5/__init__.py
-model_compression_toolkit/core/tpc_models/default_tpc/v5/tp_model.py
-model_compression_toolkit/core/tpc_models/default_tpc/v5/tpc_keras.py
-model_compression_toolkit/core/tpc_models/default_tpc/v5/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/__init__.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/target_platform_capabilities.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/latest/__init__.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/v1/__init__.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tp_model.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tpc_keras.py
-model_compression_toolkit/core/tpc_models/imx500_tpc/v1/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/__init__.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/target_platform_capabilities.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/latest/__init__.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/__init__.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tp_model.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tpc_keras.py
-model_compression_toolkit/core/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/__init__.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/target_platform_capabilities.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/latest/__init__.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/v1/__init__.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tp_model.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tpc_keras.py
-model_compression_toolkit/core/tpc_models/tflite_tpc/v1/tpc_pytorch.py
 model_compression_toolkit/exporter/__init__.py
 model_compression_toolkit/exporter/model_exporter/__init__.py
 model_compression_toolkit/exporter/model_exporter/fw_agonstic/__init__.py
 model_compression_toolkit/exporter/model_exporter/fw_agonstic/exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/__init__.py
 model_compression_toolkit/exporter/model_exporter/keras/base_keras_exporter.py
+model_compression_toolkit/exporter/model_exporter/keras/export_serialization_format.py
 model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_keras_exporter.py
+model_compression_toolkit/exporter/model_exporter/keras/fakely_quant_tflite_exporter.py
+model_compression_toolkit/exporter/model_exporter/keras/int8_tflite_exporter.py
 model_compression_toolkit/exporter/model_exporter/keras/keras_export_facade.py
 model_compression_toolkit/exporter/model_exporter/pytorch/__init__.py
 model_compression_toolkit/exporter/model_exporter/pytorch/base_pytorch_exporter.py
+model_compression_toolkit/exporter/model_exporter/pytorch/export_serialization_format.py
 model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_onnx_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/fakely_quant_torchscript_pytorch_exporter.py
 model_compression_toolkit/exporter/model_exporter/pytorch/pytorch_export_facade.py
-model_compression_toolkit/exporter/model_exporter/tflite/__init__.py
-model_compression_toolkit/exporter/model_exporter/tflite/fakely_quant_tflite_exporter.py
-model_compression_toolkit/exporter/model_exporter/tflite/int8_tflite_exporter.py
-model_compression_toolkit/exporter/model_exporter/tflite/tflite_export_facade.py
 model_compression_toolkit/exporter/model_wrapper/__init__.py
-model_compression_toolkit/exporter/model_wrapper/common/__init__.py
-model_compression_toolkit/exporter/model_wrapper/common/exporter_get_quantizer.py
 model_compression_toolkit/exporter/model_wrapper/keras/__init__.py
 model_compression_toolkit/exporter/model_wrapper/keras/validate_layer.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/__init__.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/fully_quantized_model_builder.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizer.py
 model_compression_toolkit/exporter/model_wrapper/keras/builder/node_to_quantizers.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/__init__.py
@@ -342,59 +269,63 @@
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizer.py
 model_compression_toolkit/exporter/model_wrapper/pytorch/builder/node_to_quantizers.py
 model_compression_toolkit/gptq/__init__.py
 model_compression_toolkit/gptq/runner.py
 model_compression_toolkit/gptq/common/__init__.py
 model_compression_toolkit/gptq/common/gptq_config.py
 model_compression_toolkit/gptq/common/gptq_constants.py
+model_compression_toolkit/gptq/common/gptq_framework_implementation.py
 model_compression_toolkit/gptq/common/gptq_graph.py
-model_compression_toolkit/gptq/common/gptq_quantizer_config.py
 model_compression_toolkit/gptq/common/gptq_training.py
 model_compression_toolkit/gptq/keras/__init__.py
+model_compression_toolkit/gptq/keras/gptq_keras_implementation.py
 model_compression_toolkit/gptq/keras/gptq_loss.py
-model_compression_toolkit/gptq/keras/gptq_model_builder.py
 model_compression_toolkit/gptq/keras/gptq_training.py
 model_compression_toolkit/gptq/keras/graph_info.py
 model_compression_toolkit/gptq/keras/quantization_facade.py
 model_compression_toolkit/gptq/keras/quantizer/__init__.py
-model_compression_toolkit/gptq/keras/quantizer/config_factory.py
-model_compression_toolkit/gptq/keras/quantizer/kernel_functions.py
+model_compression_toolkit/gptq/keras/quantizer/base_keras_gptq_quantizer.py
 model_compression_toolkit/gptq/keras/quantizer/quant_utils.py
-model_compression_toolkit/gptq/keras/quantizer/configs/__init__.py
-model_compression_toolkit/gptq/keras/quantizer/configs/base_quantizer_gptq_config.py
-model_compression_toolkit/gptq/keras/quantizer/configs/weight_quantizer_gptq_config.py
+model_compression_toolkit/gptq/keras/quantizer/quantization_builder.py
+model_compression_toolkit/gptq/keras/quantizer/regularization_factory.py
 model_compression_toolkit/gptq/keras/quantizer/soft_rounding/__init__.py
+model_compression_toolkit/gptq/keras/quantizer/soft_rounding/soft_quantizer_reg.py
 model_compression_toolkit/gptq/keras/quantizer/soft_rounding/symmetric_soft_quantizer.py
+model_compression_toolkit/gptq/keras/quantizer/soft_rounding/uniform_soft_quantizer.py
 model_compression_toolkit/gptq/keras/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/gptq/keras/quantizer/ste_rounding/symmetric_ste.py
-model_compression_toolkit/gptq/keras/quantizer/ste_rounding/uniform_ste.py
 model_compression_toolkit/gptq/pytorch/__init__.py
-model_compression_toolkit/gptq/pytorch/gptq_graph_info.py
 model_compression_toolkit/gptq/pytorch/gptq_loss.py
-model_compression_toolkit/gptq/pytorch/gptq_model_builder.py
+model_compression_toolkit/gptq/pytorch/gptq_pytorch_implementation.py
 model_compression_toolkit/gptq/pytorch/gptq_training.py
+model_compression_toolkit/gptq/pytorch/graph_info.py
 model_compression_toolkit/gptq/pytorch/quantization_facade.py
 model_compression_toolkit/gptq/pytorch/quantizer/__init__.py
-model_compression_toolkit/gptq/pytorch/quantizer/gptq_quantizer.py
+model_compression_toolkit/gptq/pytorch/quantizer/base_pytorch_gptq_quantizer.py
 model_compression_toolkit/gptq/pytorch/quantizer/quant_utils.py
-model_compression_toolkit/gptq/pytorch/quantizer/quantizer_wrapper.py
+model_compression_toolkit/gptq/pytorch/quantizer/quantization_builder.py
+model_compression_toolkit/gptq/pytorch/quantizer/regularization_factory.py
+model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/__init__.py
+model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/soft_quantizer_reg.py
+model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/symmetric_soft_quantizer.py
+model_compression_toolkit/gptq/pytorch/quantizer/soft_rounding/uniform_soft_quantizer.py
 model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/__init__.py
-model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/ste_weights_quantizer.py
+model_compression_toolkit/gptq/pytorch/quantizer/ste_rounding/symmetric_ste.py
+model_compression_toolkit/legacy/__init__.py
+model_compression_toolkit/legacy/keras_quantization_facade.py
+model_compression_toolkit/legacy/pytorch_quantization_facade.py
 model_compression_toolkit/ptq/__init__.py
 model_compression_toolkit/ptq/runner.py
 model_compression_toolkit/ptq/keras/__init__.py
 model_compression_toolkit/ptq/keras/quantization_facade.py
 model_compression_toolkit/ptq/pytorch/__init__.py
 model_compression_toolkit/ptq/pytorch/quantization_facade.py
 model_compression_toolkit/qat/__init__.py
 model_compression_toolkit/qat/common/__init__.py
-model_compression_toolkit/qat/common/constants.py
 model_compression_toolkit/qat/common/qat_config.py
-model_compression_toolkit/qat/common/qat_get_quantizer.py
-model_compression_toolkit/qat/common/qat_get_quantizer_config.py
 model_compression_toolkit/qat/keras/__init__.py
 model_compression_toolkit/qat/keras/quantization_facade.py
 model_compression_toolkit/qat/keras/quantizer/__init__.py
 model_compression_toolkit/qat/keras/quantizer/base_keras_qat_quantizer.py
 model_compression_toolkit/qat/keras/quantizer/quant_utils.py
 model_compression_toolkit/qat/keras/quantizer/quantization_builder.py
 model_compression_toolkit/qat/keras/quantizer/ste_rounding/__init__.py
@@ -405,49 +336,94 @@
 model_compression_toolkit/qat/pytorch/quantizer/__init__.py
 model_compression_toolkit/qat/pytorch/quantizer/base_pytorch_qat_quantizer.py
 model_compression_toolkit/qat/pytorch/quantizer/quantization_builder.py
 model_compression_toolkit/qat/pytorch/quantizer/quantizer_utils.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/__init__.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/symmetric_ste.py
 model_compression_toolkit/qat/pytorch/quantizer/ste_rounding/uniform_ste.py
-model_compression_toolkit/quantizers_infrastructure/__init__.py
-model_compression_toolkit/quantizers_infrastructure/common/__init__.py
-model_compression_toolkit/quantizers_infrastructure/common/base_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/common/base_trainable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/common/constants.py
-model_compression_toolkit/quantizers_infrastructure/common/get_all_subclasses.py
-model_compression_toolkit/quantizers_infrastructure/common/quant_utils.py
-model_compression_toolkit/quantizers_infrastructure/common/trainable_quantizer_config.py
-model_compression_toolkit/quantizers_infrastructure/keras/__init__.py
-model_compression_toolkit/quantizers_infrastructure/keras/base_keras_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/config_serialization.py
-model_compression_toolkit/quantizers_infrastructure/keras/load_model.py
-model_compression_toolkit/quantizers_infrastructure/keras/quantize_wrapper.py
-model_compression_toolkit/quantizers_infrastructure/keras/validation_functions.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/__init__.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/base_keras_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/constants.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/__init__.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/activation_pot_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/activation_symmetric_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/activation_inferable_quantizers/activation_uniform_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/__init__.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/weights_pot_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/keras/inferable_quantizers/weights_inferable_quantizers/weights_uniform_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/__init__.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/base_pytorch_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/quantize_wrapper.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/quantizer_utils.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/__init__.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/base_pytorch_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/base_symmetric_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/base_uniform_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/constants.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/__init__.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/activation_pot_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/activation_symmetric_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/activation_inferable_quantizers/activation_uniform_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/__init__.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_pot_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py
-model_compression_toolkit/quantizers_infrastructure/pytorch/inferable_quantizers/weights_inferable_quantizers/weights_uniform_inferable_quantizer.py
+model_compression_toolkit/target_platform_capabilities/__init__.py
+model_compression_toolkit/target_platform_capabilities/constants.py
+model_compression_toolkit/target_platform_capabilities/immutable.py
+model_compression_toolkit/target_platform_capabilities/target_platform/__init__.py
+model_compression_toolkit/target_platform_capabilities/target_platform/current_tp_model.py
+model_compression_toolkit/target_platform_capabilities/target_platform/fusing.py
+model_compression_toolkit/target_platform_capabilities/target_platform/op_quantization_config.py
+model_compression_toolkit/target_platform_capabilities/target_platform/operators.py
+model_compression_toolkit/target_platform_capabilities/target_platform/quantization_format.py
+model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model.py
+model_compression_toolkit/target_platform_capabilities/target_platform/target_platform_model_component.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/__init__.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/attribute_filter.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/current_tpc.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/layer_filter_params.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/operations_to_layers.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities.py
+model_compression_toolkit/target_platform_capabilities/target_platform/targetplatform2framework/target_platform_capabilities_component.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/get_target_platform_capabilities.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/target_platform_capabilities.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/latest/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v1/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v2/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v3_lut/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v4_lut/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/default_tpc/v5/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/target_platform_capabilities.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/latest/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/imx500_tpc/v1/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/target_platform_capabilities.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/latest/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/qnnpack_tpc/v1/tpc_pytorch.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/target_platform_capabilities.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/latest/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/__init__.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tp_model.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_keras.py
+model_compression_toolkit/target_platform_capabilities/tpc_models/tflite_tpc/v1/tpc_pytorch.py
+model_compression_toolkit/trainable_infrastructure/__init__.py
+model_compression_toolkit/trainable_infrastructure/common/__init__.py
+model_compression_toolkit/trainable_infrastructure/common/base_trainable_quantizer.py
+model_compression_toolkit/trainable_infrastructure/common/constants.py
+model_compression_toolkit/trainable_infrastructure/common/get_quantizer_config.py
+model_compression_toolkit/trainable_infrastructure/common/get_quantizers.py
+model_compression_toolkit/trainable_infrastructure/common/quant_utils.py
+model_compression_toolkit/trainable_infrastructure/common/trainable_quantizer_config.py
+model_compression_toolkit/trainable_infrastructure/keras/__init__.py
+model_compression_toolkit/trainable_infrastructure/keras/base_keras_quantizer.py
+model_compression_toolkit/trainable_infrastructure/keras/config_serialization.py
+model_compression_toolkit/trainable_infrastructure/keras/load_model.py
+model_compression_toolkit/trainable_infrastructure/keras/quantizer_utils.py
+model_compression_toolkit/trainable_infrastructure/pytorch/__init__.py
+model_compression_toolkit/trainable_infrastructure/pytorch/base_pytorch_quantizer.py
```

### Comparing `model_compression_toolkit-1.8.0/setup.py` & `model_compression_toolkit-1.9.0/setup.py`

 * *Files identical despite different names*

